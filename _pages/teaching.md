---
title:  "Teaching"
layout: archive
permalink: /teaching/
author_profile: true
comments: true
---
I created the following lectures for a Bachelor level course about data mining and machine learning at TU/e. For every lecture, there are videos (to see them expand the corresponding lecture item), slides and a Proofs, Exercises and Literature (PELi) document. Some lectures have additionally a Python notebook.
<details>
  <summary>
    Linear Algebra - Recap <br>
    <i>vectors - matrices - matrix multiplication - vector and matrix norms - SVD</i>
  </summary>
  <p>Part 1: Vectors and Matrices</p>
  <ul>
    <li>Vector spaces</li> 
    <li>The transposed of a matrix</li>
    <li>Symmetric and diagonal matrices</li>
  </ul>
  <a href="http://www.youtube.com/watch?feature=player_embedded&v=yIWNOktZ3kQ
" target="_blank"><img src="http://img.youtube.com/vi/yIWNOktZ3kQ/0.jpg" 
alt="Video 1" width="240" height="180" border="10" /></a>
  <p>Part 2: Matrix Multiplication</p>
  <ul>
        <li>The inner and outer product of vectors</li>
        <li>Matrix multiplication: inner and outer product-wise</li>
        <li>Identity matrix and inverse matrices</li>
        <li>Transposed of a matrix product</li>
  </ul>
<a href="http://www.youtube.com/watch?feature=player_embedded&v=z-Hc0vtgQog
" target="_blank"><img src="http://img.youtube.com/vi/z-Hc0vtgQog/0.jpg" 
alt="Video 2" width="240" height="180" border="10" /></a>
  <p>Part 3: Vector Norms</p>
  <ul>
        <li>The Euclidean norm and the inner product</li>
        <li>Orthogonal vectors</li>
        <li>Vector Lp-norms</li>
  </ul>
<a href="http://www.youtube.com/watch?feature=player_embedded&v=AeYInW-Z63w
" target="_blank"><img src="http://img.youtube.com/vi/AeYInW-Z63w/0.jpg" 
alt="Video 3" width="240" height="180" border="10" /></a>
  <p>Part 4: Matrix Norms</p>
  <ul>
        <li>Matrix Lp-norms and the operator norm</li>
        <li>Orthogonal matrices</li>
        <li>Orthogonal invariance of matrix norms</li>
        <li>The trace</li>
        <li>Binomial formulas for norms</li>
        <li>Singular Value Decomposition and invertibility of a matrix</li>
  </ul>
<a href="http://www.youtube.com/watch?feature=player_embedded&v=mupvSmwKnf4
" target="_blank"><img src="http://img.youtube.com/vi/mupvSmwKnf4/0.jpg" 
alt="Video 4" width="240" height="180" border="10" /></a>
</details>

[slides]({{ site.url }}{{ site.baseurl }}/assets/documents/DMML_LinearAlgebra.pdf){: .btn .btn--primary .btn--small} [PELi](/ZeroShades/assets/documents/DMML_LinearAlgebra_PELi.pdf){: .btn .btn--success .btn--small} [PELi Solutions](/ZeroShades/assets/documents/DMML_LinearAlgebra_PELi_S.pdf){: .btn .btn--success .btn--small}

<details>
  <summary>
    Optimization <br>
    <i>FONC & SONC - numerical optimization - convexity - gradients</i>
  </summary>
  <p>Part 1: FONC & SONC</p>
  <ul>
        <li>Unconstrained optimization objectives</li>
        <li>First Order Necessary Condition (FONC) for minimizers</li>
        <li>Second Order Necessary Condition (SONC) for minimizers</li>
        <li>Finding stationary points of smooth functions</li>
    </ul>
  <a href="http://www.youtube.com/watch?feature=player_embedded&v=xcriyhPfEro
" target="_blank"><img src="http://img.youtube.com/vi/xcriyhPfEro/0.jpg" 
alt="Video 1" width="240" height="180" border="10" /></a>
  <p>Part 2: Numerical Optimization</p>
  <ul>
        <li>Constrained optimization objectives</li>
        <li>Gradient Descent</li>
        <li>Coordinate Descent</li>
  </ul>
<a href="http://www.youtube.com/watch?feature=player_embedded&v=eqUoLPsrCaE
" target="_blank"><img src="http://img.youtube.com/vi/eqUoLPsrCaE/0.jpg" 
alt="Video 2" width="240" height="180" border="10" /></a>
  <p>Part 3: Convexity</p>
  <ul>
        <li>Convex sets</li>
        <li>Convex functions</li>
        <li>Convex optimization problems</li>
  </ul>
<a href="http://www.youtube.com/watch?feature=player_embedded&v=j_zM7845nus
" target="_blank"><img src="http://img.youtube.com/vi/j_zM7845nus/0.jpg" 
alt="Video 3" width="240" height="180" border="10" /></a>
  <p>Part 4: Computing Gradients</p>
  <ul>
        <li>Partial derivatives, the gradient and the Jacobian</li>
        <li>Linearity of gradients</li>
        <li>Chain rule</li>
  </ul>
<a href="http://www.youtube.com/watch?feature=player_embedded&v=6Y1DA7vJ8XA
" target="_blank"><img src="http://img.youtube.com/vi/6Y1DA7vJ8XA/0.jpg" 
alt="Video 4" width="240" height="180" border="10" /></a>
</details>

[slides]({{ site.url }}{{ site.baseurl }}/assets/documents/DMML_Optimization.pdf){: .btn .btn--primary .btn--small} [PELi](/ZeroShades/assets/documents/DMML_Optimization_PELi.pdf){: .btn .btn--success .btn--small} [PELi Solutions](/ZeroShades/assets/documents/DMML_Optimization_PELi_S.pdf){: .btn .btn--success .btn--small}

<details>
  <summary>
    Regression <br>
    <i>regression with basis functions - bias-variance tradeoff - cross validation</i>
  </summary>
  <p>Part 1: The Regression Objective</p>
  <ul>
        <li>Formal regression task definition</li>
        <li>Affine regression functions</li>
        <li>Polynomial regression functions</li>
        <li>Radial Basis regression functions</li>
    </ul>
  <a href="http://www.youtube.com/watch?feature=player_embedded&v=mQ8x9D7uYsQ
" target="_blank"><img src="http://img.youtube.com/vi/mQ8x9D7uYsQ/0.jpg" 
alt="Video 1" width="240" height="180" border="10" /></a>
  <p>Part 2: Regression Optimization</p>
  <ul>
        <li>Residual Sum of Squares (RSS)</li>
        <li>Design matrix</li>
        <li>Solving the regression problem</li>
        <li>The set of global regression minimizers</li>
  </ul>
<a href="http://www.youtube.com/watch?feature=player_embedded&v=QAK1yRj8Tws
" target="_blank"><img src="http://img.youtube.com/vi/QAK1yRj8Tws/0.jpg" 
alt="Video 2" width="240" height="180" border="10" /></a>
  <p>Part 3: The Bias-Variance Tradeoff in Regression</p>
  <ul>
        <li>Evaluating the regression model</li>
        <li>The Mean Squared Error (MSE)</li>
        <li>Splitting in test- and training dataset</li>
        <li>The Expected Prediction Error (EPE)</li>
        <li>Bias, variance and noise of a regression model and the bias-variance tradeoff</li>
        <li>Cross-validation</li>
  </ul>
<a href="http://www.youtube.com/watch?feature=player_embedded&v=is-Ovf8irpk
" target="_blank"><img src="http://img.youtube.com/vi/is-Ovf8irpk/0.jpg" 
alt="Video 3" width="240" height="180" border="10" /></a>
</details>

[slides]({{ site.url }}{{ site.baseurl }}/assets/documents/DMML_Regression.pdf){: .btn .btn--primary .btn--small} [PELi](/ZeroShades/assets/documents/DMML_Regression_PELi.pdf){: .btn .btn--success .btn--small} [PELi Solutions](/ZeroShades/assets/documents/DMML_Regression_PELi_S.pdf){: .btn .btn--success .btn--small}

<details>
  <summary>
    Regularization in Regression <br>
    <i>regression in high dimensional feature spaces -  ridge regression - Lasso</i>
  </summary>
  <p>Part 1: p larger n</p>
  <ul>
        <li>Determining the set of global minimizers by SVD</li>
        <li>Python implementation</li>
    </ul>
  <a href="http://www.youtube.com/watch?feature=player_embedded&v=ewZXca3WANM
" target="_blank"><img src="http://img.youtube.com/vi/ewZXca3WANM/0.jpg" 
alt="Video 1" width="240" height="180" border="10" /></a>
  <p>Part 2: Sparse Regression</p>
  <ul>
        <li>The sparse regression objective</li>
        <li>Relaxing the sparse regression objective</li>
        <li>Lp-norm regularization</li>
  </ul>
<a href="http://www.youtube.com/watch?feature=player_embedded&v=3EG58QLzT1s
" target="_blank"><img src="http://img.youtube.com/vi/3EG58QLzT1s/0.jpg" 
alt="Video 2" width="240" height="180" border="10" /></a>
  <p>Part 3: Ridge Regression</p>
  <ul>
        <li>The ridge regression objective</li>
        <li>The minimizer of ridge regression</li>
  </ul>
<a href="http://www.youtube.com/watch?feature=player_embedded&v=ahxEMgBe-RU
" target="_blank"><img src="http://img.youtube.com/vi/ahxEMgBe-RU/0.jpg" 
alt="Video 3" width="240" height="180" border="10" /></a>
  <p>Part 4: Lasso</p>
  <ul>
        <li>The Lasso objective</li>
        <li>The coordinate descent optimization of Lasso</li>
        <li> Comparison of L1 and L2 regularization </li>
  </ul>
<a href="http://www.youtube.com/watch?feature=player_embedded&v=ptdgweBCiHw
" target="_blank"><img src="http://img.youtube.com/vi/ptdgweBCiHw/0.jpg" 
alt="Video 4" width="240" height="180" border="10" /></a>
</details>

[slides]({{ site.url }}{{ site.baseurl }}/assets/documents/DMML_Regularization.pdf){: .btn .btn--primary .btn--small} [PELi](/ZeroShades/assets/documents/DMML_Regularization_PELi.pdf){: .btn .btn--success .btn--small} [PELi Solutions](/ZeroShades/assets/documents/DMML_Regularization_PELi_S.pdf){: .btn .btn--success .btn--small} [Notebook](/ZeroShades/assets/documents/DMML_Regularization.ipynb){: .btn .btn--warning .btn--small} 

<details>
  <summary>
    Recommender Systems and Dimensionality Reduction <br>
    <i>matrix factorization - matrix completion - PCA</i>
  </summary>
  <p>Part 1: The Rank-r Matrix Factorization Problem</p>
  <ul>
        <li>Summarizing user behavior via a matrix product</li>
        <li>The matrix factorization objective</li>
        <li>Truncated SVD as the solver for the rank-r MF problem</li>
        <li>Nonconvexity of the objective</li>
    </ul>
  <a href="http://www.youtube.com/watch?feature=player_embedded&v=3hEmvFTJGFw
" target="_blank"><img src="http://img.youtube.com/vi/3hEmvFTJGFw/0.jpg" 
alt="Video 1" width="240" height="180" border="10" /></a>
  <p>Part 2: Matrix Completion</p>
  <ul>
        <li>Handling missing values in low-rank MFs</li>
        <li>Interpretation of the factorization in the scope of movie recommendations</li>
        <li>A Netflix prize-winning approach for matrix completion</li>
  </ul>
<a href="http://www.youtube.com/watch?feature=player_embedded&v=j9hfOlIY1mg
" target="_blank"><img src="http://img.youtube.com/vi/j9hfOlIY1mg/0.jpg" 
alt="Video 2" width="240" height="180" border="10" /></a>
  <p>Part 3: Principal Components Analysis (PCA)</p>
  <ul>
        <li>Finding good low-dimensional representations of the data</li>
        <li>Finding the directions of maximum variance in the data</li>
        <li>Solving the objective of PCA by means of the truncated SVD</li>
  </ul>
<a href="http://www.youtube.com/watch?feature=player_embedded&v=qAMvT9Se36E
" target="_blank"><img src="http://img.youtube.com/vi/qAMvT9Se36E/0.jpg" 
alt="Video 3" width="240" height="180" border="10" /></a>
  <p>Part 4: Notebook</p>
  <ul>
        <li>Visualization of SVD</li>
        <li>Computing the variance of the data in a direction</li>
        <li>Visualization of PCA projections</li>
  </ul>
<a href="http://www.youtube.com/watch?feature=player_embedded&v=D0pZ8qgsx54
" target="_blank"><img src="http://img.youtube.com/vi/D0pZ8qgsx54/0.jpg" 
alt="Video 4" width="240" height="180" border="10" /></a>
</details>

[slides]({{ site.url }}{{ site.baseurl }}/assets/documents/DMML_RecSys_PCA.pdf){: .btn .btn--primary .btn--small} [PELi](/ZeroShades/assets/documents/DMML_RecSys_PCA_PELi.pdf){: .btn .btn--success .btn--small} [PELi Solutions](/ZeroShades/assets/documents/DMML_RecSys_PCA_PELi_S.pdf){: .btn .btn--success .btn--small} [Notebook](/ZeroShades/assets/documents/DMML_RecSys_PCA.ipynb){: .btn .btn--warning .btn--small} 

<details>
  <summary>
    k-means Clustering<br>
    <i>within-cluster-scatter - k-means as matrix factorization - alternating minimization</i>
  </summary>
  <p>Part 1: The k-means Objective</p>
  <ul>
        <li>The cluster model of k-means</li>
        <li>The k-means objective to minimize the within-cluster-scatter</li>
        <li>The k-means objective is equivalent to minimizing the distance of points to their closest centroid</li>
        <li>Lloyds' algorithm for the optimization of k-means</li>
    </ul>
  <a href="http://www.youtube.com/watch?feature=player_embedded&v=fB0tFX7k5T8
" target="_blank"><img src="http://img.youtube.com/vi/fB0tFX7k5T8/0.jpg" 
alt="Video 1" width="240" height="180" border="10" /></a>
  <p>Part 2: k-means as a Matrix Factorization</p>
  <ul>
        <li>Indicating clusters by a binary matrix</li>
        <li>Computing the centroids in matrix notation</li>
        <li>The k-means objective as a constrained matrix factorization problem</li>
  </ul>
<a href="http://www.youtube.com/watch?feature=player_embedded&v=dOh2hY23hK0
" target="_blank"><img src="http://img.youtube.com/vi/dOh2hY23hK0/0.jpg" 
alt="Video 2" width="240" height="180" border="10" /></a>
  <p>Part 3: k-means Optimization via Block-Coordinate Descent</p>
  <ul>
        <li>Centroids are the minimizes of the k-means objective when fixing the cluster assignments</li>
        <li>Assigning points to the clusters with closest centroid minimizes the k-means objective when we fix the centroids</li>
        <li>Lloyds' algorithm as block-coordinate descent</li>
  </ul>
<a href="http://www.youtube.com/watch?feature=player_embedded&v=eJxr55ESqx4
" target="_blank"><img src="http://img.youtube.com/vi/eJxr55ESqx4/0.jpg" 
alt="Video 3" width="240" height="180" border="10" /></a>
  <p>Part 4: Notebook</p>
  <ul>
        <li>Visualization of k-means' optimization</li>
        <li>Effect of initialization</li>
        <li>k-means as matrix factorization</li>
  </ul>
<a href="http://www.youtube.com/watch?feature=player_embedded&v=X-geBBKTgZI
" target="_blank"><img src="http://img.youtube.com/vi/X-geBBKTgZI/0.jpg" 
alt="Video 4" width="240" height="180" border="10" /></a>
</details>

[slides]({{ site.url }}{{ site.baseurl }}/assets/documents/DMML_kMeans.pdf){: .btn .btn--primary .btn--small} [PELi](/ZeroShades/assets/documents/DMML_kMeans_PELi.pdf){: .btn .btn--success .btn--small} [PELi Solutions](/ZeroShades/assets/documents/DMML_kMeans_PELi_S.pdf){: .btn .btn--success .btn--small} [Notebook](/ZeroShades/assets/documents/DMML_kmeans.ipynb){: .btn .btn--warning .btn--small} 


