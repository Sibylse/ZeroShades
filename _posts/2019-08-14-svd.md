---
layout: single
title:  "Matrix Decompositions: Why and How"
date:   2019-08-14
mathjax: true
---
When we want to talk about matrix factorizations, then we have to start at the beginning: the eigendecomposition and the singular value decomposition. There are so many aspects about these decompositions, I don't think that we will have time to explore them. Here, I just want to talk about the somewhat less known but nevertheless imposrtant optimization problems which these decompositions solve. Let's just take what we need from the vast theory about these decompositions and run with it.

# Singular Value Decomposition
Any real-valued matrix has a Singular Value Decomposition (SVD). Based on the SVD, we can derive characteristics of a matrix, in particular with respect to  invertibility. There are various representations of the SVD. Let us start with the full representation, sketched in the following picture.
![alt]({{ site.url }}{{ site.baseurl }}/assets/images/SVD_full.png "full SVD")
The $m\times n$ matrix $D$ is decomposed into three matrices $U,V$ and $\Sigma$. $U$ and $V$ are orthogonal, that means that 
$$\begin{align*}
  U^\top U = UU^\top = I_m && V^\top V =VV^\top = I_n,
\end{align*}$$
where $I_m$ and $I_n$ are the $m$- and $n$-dimensional identity matrices. The good thing about orhogonal matrices is that they are invertible, and we can easily compute the inverse: it's the transpose of the matrix. This allows us to play with the elements of SVD. We can bring the orthogonal matrices from one side of the equation to the other side, by multiplying with the inverse. Hence, if we multiply with $V$ from the right, we get:
![alt]({{ site.url }}{{ site.baseurl }}/assets/images/SVD_DV.png "SVD: multiplication with V from the right")
Or, if we multiply with $U^\top$ from the left, we get:
![alt]({{ site.url }}{{ site.baseurl }}/assets/images/SVD_UD.png "SVD: multiplication with U transposed from the left")
So, having these orthogonal matrices in SVD is convenient for solving equations. We can for example solve for the matrix $\Sigma$: 
![alt]({{ site.url }}{{ site.baseurl }}/assets/images/SVD_Sigma.png "SVD: solving for Sigma")
$\Sigma$ is a block-diagonal matrix, which has the singular values on its diagonal. Singular values are always nonnegative, that is $\Sigma$ is nonnegative.If $m\neq n$, like in our example, then $\Sigma$ is composed by a diagonal matrix (the upper part) and a zero matrix (the lower part). We can separate the diagonal part of $\Sigma$ from the zero part by the matrix multiplication rules. When multiplying $U$ with $\Sigma$, the blue part of $U$ is multiplied with the blue part of $\Sigma$, as visualized in the following picture:   
![alt]({{ site.url }}{{ site.baseurl }}/assets/images/SVD_rank_decomp.png "SVD: decomposition in nonzero- and zero part").
So, here we employ the notation of the set $\mathcal{N}=\{1,\ldots,n\}$ and $\mathcal{M}=\{1,\ldots,m\}$. The matrix $\Sigma_{\mathcal{M}\setminus\mathcal{N}\cdot}$ denotes the lower, blue part of $\Sigma$, which is entirely filled with zeros. Hence, the second part of the sum is a zero matrix and can be omitted from the sum. This leaves us with the following, shorter representation of SVD:  
![alt]({{ site.url }}{{ site.baseurl }}/assets/images/SVD_short.png "SVD: short representation")
Here, we have now a diagonal middle matrix, but the left matrix $U_{\cdot\mathcal{N}}$ is not orthogonal anymore. $U_{\cdot\mathcal{N}}$ does have orthogonal columns, but it is not an orthogonal matrix, because $U_{\cdot\mathcal{N}} U_{\cdot\mathcal{N}}^\top$ is not the identity matrix. At least, we still have $U_{\cdot\mathcal{N}}^\top U_{\cdot\mathcal{N}}=I_n$.

Now, let's have another look at the middle diagonal matrix. We can order the columns of $\Sigma_{\mathcal{N}\cdot}$ such that the singular values on the diagonal ar e ordered from highest to lowest: $\Sigma_{11}\geq\ldots\geq \Sigma_{nn}$. Now, if one, or more singular values are equal to zero, then we have again the case that the middle matrix is composed of a diagonal part and a zero part. The zero part is again visualized in blue here, and by applying the same trick as above, we can again omit the blue part from the matrix multiplication. Therewith, we get an even shorter representation of SVD:  
![alt]({{ site.url }}{{ site.baseurl }}/assets/images/SVD_rank.png "SVD: rank representation")
The number $r$ of the nonzero singular values is equal to the rank of the matrix $D$. 
# Eigendecompositions
Let us assume that we have a symmetric, real-valued $m\times m$ matrix $W$. Such a matrix could for example reflect similarities between data points, that is entry $W_{jl}$ denotes the similarity between data points $j$ and $l$.
![alt]({{ site.url }}{{ site.baseurl }}/assets/images/eigendecomp.png "eigendecomp 3")


