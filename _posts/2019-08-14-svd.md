---
layout: single
title:  "Matrix Decompositions: Why and How"
date:   2019-08-14
mathjax: true
---
When we want to talk about matrix factorizations, then we have to start at the beginning: the eigendecomposition and the singular value decomposition. There are so many aspects about these decompositions, I don't think that we will have time to explore them. Here, I just want to talk about the somewhat less known but nevertheless imposrtant optimization problems which these decompositions solve. Let's just take what we need from the vast theory about these decompositions and run with it.

# Singular Value Decomposition
Any real-valued matrix has a Singular Value Decomposition (SVD). Based on the SVD, we can derive characteristics of a matrix, in particular with respect to  invertibility. There are various representations of the SVD. Let us start with the full representation, sketched in the following picture.
![alt]({{ site.url }}{{ site.baseurl }}/assets/images/SVD_full.png "full SVD")
The $m\times n$ matrix $D$ is decomposed into three matrices $U,V$ and $\Sigma$. $U$ and $V$ are orthogonal, that means that 
$$\begin{align_}
  U^\top U = UU^\top = I_m && V^\top V =VV^\top = I_n,
\end{align_}$$
where $I_m$ and $I_n$ are the $m$- and $n$-dimensional identity matrices. The good thing about orhogonal matrices is that they are invertible, and we can easily compute the inverse by transposing the matrix.
![alt]({{ site.url }}{{ site.baseurl }}/assets/images/SVD_DV.png "SVD: multiplication with V from the right")
![alt]({{ site.url }}{{ site.baseurl }}/assets/images/SVD_UD.png "SVD: multiplication with U transposed from the left")
![alt]({{ site.url }}{{ site.baseurl }}/assets/images/SVD_rank_decomp.png "SVD: decomposition in nonzero- and zero part")
![alt]({{ site.url }}{{ site.baseurl }}/assets/images/SVD_short.png "SVD: short representation")
![alt]({{ site.url }}{{ site.baseurl }}/assets/images/SVD_rank.png "SVD: rank representation")
The $m\times n$ matrix $\Sigma$ is block-diagonal. That is, alle entries are zero, except for the entries on the diagonal. In the example above, we have $m>n$, hence we have $n$ elements on the diagonal. The area which is marked in cyan corresponds to a block full of zeros in $\Sigma$ and the corresponding columns in $U$, which are in the tripel matrix multiplication always multiplied with zero. 
# Eigendecompositions
Let us assume that we have a symmetric, real-valued $m\times m$ matrix $W$. Such a matrix could for example reflect similarities between data points, that is entry $W_{jl}$ denotes the similarity between data points $j$ and $l$.
![alt]({{ site.url }}{{ site.baseurl }}/assets/images/eigendecomp.png "eigendecomp 3")


