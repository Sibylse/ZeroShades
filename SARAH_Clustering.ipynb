{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SARAH_Clustering.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPZfvJRXFi3tvj69stg8afM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sibylse/ZeroShades/blob/master/SARAH_Clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qP-WgMgIEFuV",
        "colab_type": "text"
      },
      "source": [
        "Code from the [proximal SGD blogpost](http://pmelchior.net/blog/proximal-matrix-factorization-in-pytorch.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ky8-gaL_E4T",
        "colab_type": "text"
      },
      "source": [
        "From the [alternating minimization pytorch](https://medium.com/@rinabuoy13/explicit-recommender-system-matrix-factorization-in-pytorch-f3779bb55d74) blog:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORWXvgCNj07r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "45f103e2-31d1-40e5-9ca5-e767ce89f243"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.autograd import Variable\n",
        "import collections\n",
        "rng = np.random.default_rng()\n",
        "cuda = torch.cuda.is_available()\n",
        "dev = torch.device(\"cuda\") if cuda else torch.device(\"cpu\")\n",
        "dev"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDvXlyYIgaUB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generateBinaryFactor(n, r, q):\n",
        "  B=np.zeros((n,r))\n",
        "  l= int(np.ceil(n*0.01)) #lower bound of uniquely assigned ones per cluster\n",
        "  t = r*l # end of block-diagonal part\n",
        "  for s in range(r):\n",
        "    B[s*l:(s+1)*l,s]=1 #create block for diagonal\n",
        "    B[t:,s]= rng.binomial(1, q-0.01, n-t)\n",
        "  return B"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5etI81jivmO",
        "colab_type": "text"
      },
      "source": [
        "Generate a diagonal-dominant middle factor matrix C."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgxbHezFICT0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "cb97f49e-2460-45a2-f0ab-64cd4fbf2957"
      },
      "source": [
        "r=3\n",
        "m=300 \n",
        "n=200\n",
        "Y = generateBinaryFactor(m, r, 0.2)\n",
        "X = generateBinaryFactor(n, r, 0.2)\n",
        "C = np.round(rng.uniform(0,5,(r,r)),2) \n",
        "# Make diagonal-dominant matrix:\n",
        "# C= np.round(rng.uniform(0,0.5,(r,r)),2) + np.diag(rng.normal(1,0.1,r))\n",
        "C"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.64, 4.07, 0.47],\n",
              "       [0.55, 0.23, 4.34],\n",
              "       [3.33, 4.03, 4.98]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Msa2_SFGKHxL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "D = Y@C@X.T + np.round(rng.normal(0,0.1,(m,n)),2)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ReEPmVOIhtAZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7a35a7f5-5bd5-4207-d4f9-da07d57e98e8"
      },
      "source": [
        "np.mean((D-Y@C@X.T)**2)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.009987398333333335"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0jVMLmP3Rv4",
        "colab_type": "text"
      },
      "source": [
        "Custom dataset on the basis of [this](https://www.kaggle.com/pinocookie/pytorch-dataset-and-dataloader) code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jp6AZSgG28q-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DataMatrix(Dataset):\n",
        "        \n",
        "    def __init__(self, D):\n",
        "        self.data_ = torch.from_numpy(D).double()\n",
        "        self.m_ = D.shape[0]\n",
        "        self.n_ = D.shape[1]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.m_*self.n_\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        j = int(index/self.n_)\n",
        "        i = index%self.n_       \n",
        "        return torch.tensor([j,i]), self.data_[j,i]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qftmrGyQs-I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ac60492d-8322-49ac-cf59-40bfe5614c04"
      },
      "source": [
        "int(n*m*2.5/100)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1500"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01gENeceG05J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d754d993-d500-47bd-df7f-00af676bf1d5"
      },
      "source": [
        "bs = int(n*m*5/100)\n",
        "train_loader = DataLoader(DataMatrix(D), batch_size=bs, shuffle=True)\n",
        "test_loader = DataLoader(DataMatrix(Y@C@X.T), batch_size=1000)\n",
        "I_b=torch.eye(bs)\n",
        "print(bs, len(train_loader))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3000 20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NX3qb0-cVVEP",
        "colab_type": "text"
      },
      "source": [
        "Models: Matrix Factorization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WObi_SLvwQee",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GaussianNoise(nn.Module):\n",
        "    \"\"\"Gaussian noise regularizer.\n",
        "\n",
        "    Args:\n",
        "        sigma (float, optional): relative standard deviation used to generate the\n",
        "            noise. Relative means that it will be multiplied by the magnitude of\n",
        "            the value your are adding the noise to. This means that sigma can be\n",
        "            the same regardless of the scale of the vector.\n",
        "        is_relative_detach (bool, optional): whether to detach the variable before\n",
        "            computing the scale of the noise. If `False` then the scale of the noise\n",
        "            won't be seen as a constant but something to optimize: this will bias the\n",
        "            network to generate vectors with smaller values.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, sigma=0.1, is_relative_detach=True):\n",
        "        super().__init__()\n",
        "        self.sigma = sigma\n",
        "        self.is_relative_detach = is_relative_detach\n",
        "        self.noise = torch.tensor(0).to(dev)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.training and self.sigma != 0:\n",
        "            scale = self.sigma * x.detach() if self.is_relative_detach else self.sigma * x\n",
        "            sampled_noise = self.noise.repeat(*x.size()).float().normal_() * scale\n",
        "            x = x + sampled_noise\n",
        "        return x "
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7fn9nYy_TPl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MatrixFactorization(torch.nn.Module):\n",
        "    \n",
        "    def __init__(self, m, n, r=20,sigma=0):\n",
        "        super().__init__()\n",
        "        self.X = nn.Sequential(collections.OrderedDict([\n",
        "          ('fact', torch.nn.Embedding(n, r).double()),\n",
        "          ('noise', GaussianNoise(sigma))\n",
        "        ]))\n",
        "        self.Y = nn.Sequential(collections.OrderedDict([\n",
        "          ('fact', torch.nn.Embedding(m, r).double()),\n",
        "          ('noise', GaussianNoise(sigma))\n",
        "        ]))\n",
        "        self.C = nn.Sequential(collections.OrderedDict([\n",
        "          ('fact', torch.nn.Embedding(r, r).double()),\n",
        "          ('noise', GaussianNoise(sigma))\n",
        "        ]))\n",
        "        #self.Y = torch.nn.Embedding(m, r).double()\n",
        "        #self.X = torch.nn.Embedding(n, r).double()\n",
        "        #self.C = torch.nn.Embedding(r, r).double()\n",
        "        torch.nn.init.uniform_(self.Y.fact.weight)\n",
        "        torch.nn.init.uniform_(self.X.fact.weight)\n",
        "        torch.nn.init.uniform_(self.C.fact.weight)\n",
        "        \n",
        "    def forward(self, idx):\n",
        "      #j and i are torch tensors, denoting a set of indices i and j\n",
        "      YC =torch.matmul(self.Y(idx[:,0]),self.C.fact.weight) \n",
        "      YCX_batch = (YC * self.X(idx[:,1])).sum(1,keepdim=True)\n",
        "      return YCX_batch.squeeze() #reduces every 1x dimension for tensor"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRM1jHSta7-Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "outputId": "85e70660-3276-49f2-8233-2f13a7e17fef"
      },
      "source": [
        "model = MatrixFactorization(n, m, r=2)\n",
        "model.to(dev)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MatrixFactorization(\n",
              "  (X): Sequential(\n",
              "    (fact): Embedding(300, 2)\n",
              "    (noise): GaussianNoise()\n",
              "  )\n",
              "  (Y): Sequential(\n",
              "    (fact): Embedding(200, 2)\n",
              "    (noise): GaussianNoise()\n",
              "  )\n",
              "  (C): Sequential(\n",
              "    (fact): Embedding(2, 2)\n",
              "    (noise): GaussianNoise()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCWSS3UTt-I9",
        "colab_type": "text"
      },
      "source": [
        "Get batch-Lipschitz constants:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7nh_2K3Aquu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def L_X(batch_Y,batch_i):\n",
        "  with torch.no_grad():\n",
        "    unique_i, inverse_i = torch.unique(batch_i, return_inverse=True)\n",
        "    I1hot =I_b[inverse_i].to(dev)\n",
        "    W = torch.einsum('js,jt,ju->stu', batch_Y , batch_Y, I1hot)\n",
        "    L = torch.sqrt((W**2).sum([0,1]).max()).item()\n",
        "    L= L/bs*2\n",
        "    return max(L,0.001)\n",
        "def L_C(batch_X,batch_Y):\n",
        "  with torch.no_grad():\n",
        "    d = torch.einsum('js,jt->j', batch_Y , batch_X)\n",
        "    W = torch.matmul(torch.transpose(batch_X,0,1)*d, batch_Y)\n",
        "    L = torch.sqrt((W**2).sum()).item()\n",
        "    L = L/bs*2\n",
        "    return max(L,0.001)\n",
        "def stepsizeX(model,batch):\n",
        "  with torch.no_grad():\n",
        "    batch_Y = model.Y.fact(batch[:,0]).float().to(dev)\n",
        "    batch_Y = torch.matmul(batch_Y,model.C.fact.weight.float()).to(dev)\n",
        "    L= L_X(batch_Y,batch[:,1])\n",
        "    return 1/4.1/L\n",
        "def stepsizeY(model,batch):\n",
        "  with torch.no_grad():\n",
        "    batch_X = model.X.fact(batch[:,1]).float().to(dev)\n",
        "    batch_X = torch.matmul(batch_X,torch.transpose(model.C.fact.weight.float(),0,1)).to(dev) \n",
        "    L= L_X(batch_X,batch[:,0])\n",
        "    return 1/4.1/L\n",
        "def stepsizeC(model,batch):\n",
        "  with torch.no_grad():\n",
        "    batch_X = model.X.fact(batch[:,1]).float().to(dev)\n",
        "    batch_Y = model.Y.fact(batch[:,0]).float().to(dev)\n",
        "    L= L_C(batch_X,batch_Y)\n",
        "    return 1/4.1/L"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TB3THgGj4DER",
        "colab_type": "text"
      },
      "source": [
        "Prox operators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCL1QdbU6qCt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def phi(x):\n",
        "    return 1-torch.abs(1-2*x)\n",
        "def prox_binary_(x,lambdas,lr,alpha=-1e-8):\n",
        "  with torch.no_grad():\n",
        "    idx_up = x>0.5\n",
        "    idx_down = x<=0.5\n",
        "    x[idx_up] += 2*lr*lambdas[idx_up]\n",
        "    x[idx_down] -= 2*lr*lambdas[idx_down]\n",
        "    x[x>1] = 1 \n",
        "    x[x<0] = 0 \n",
        "    lambdas.add_(phi(x)-1,alpha=alpha)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QW4RelrWe4Y6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prox_pos_(X,weight,lr,alpha=0):\n",
        "  with torch.no_grad():\n",
        "    X[X<0] = 0"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yi-dOp83VYG6",
        "colab_type": "text"
      },
      "source": [
        "Optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "c5O3rHutuYYE",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.optim.optimizer import Optimizer,required\n",
        "\n",
        "class SAGA(Optimizer):\n",
        "    r\"\"\"Implements SAGA TODO.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, params, prox, L, reg_weight=0.1, saga=False):\n",
        "\n",
        "        defaults = dict(reg_weight=reg_weight,saga=saga)\n",
        "        super(Spring, self).__init__(params, defaults)\n",
        "        self.prox = prox\n",
        "        self.L=L\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(Spring, self).__setstate__(state)\n",
        "        for group in self.param_groups:\n",
        "            group.setdefault('saga', False)\n",
        "            group.setdefault('reg_weight', 0.1)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"Performs a single optimization step.\n",
        "        \"\"\"\n",
        "        loss = None\n",
        "\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            reg_weight = group['reg_weight'] #lambda\n",
        "            estimator = group['estimator'] #SAGA, SARAH or SGD?\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                # p.grad gradient has same shape like the parameter but \n",
        "                # zeros at the positions which are not updated.\n",
        "                # grad_p has same size as batch times r \n",
        "                if estimator == \"SAGA\": \n",
        "                    \n",
        "                    param_state = self.state[p]\n",
        "                    batch_idx = p.grad.sum(1) != 0\n",
        "                    grad_p = p.grad[batch_idx,:]\n",
        "                    if 'grad_buffer' not in param_state:\n",
        "                        G = grad_p\n",
        "                        param_state['grad_buffer'] = torch.clone(p.grad).detach() \n",
        "                    else:\n",
        "                        G = param_state['grad_buffer'][batch_i,:]\n",
        "                        G.mul_(-1).add_(grad_p, alpha=1) #G = grad_t - grad_t-1\n",
        "                        param_state['grad_buffer'][batch_i,:] = torch.clone(grad_p).detach() #memory?\n",
        "                    if 'avg_buffer' not in param_state:\n",
        "                        grad_p = grad_p.add(buf, alpha=momentum)\n",
        "                    else:\n",
        "                        param_state['avg_buffer']\n",
        "                p.add_(p.grad, alpha=-group['lr'])\n",
        "\n",
        "        return loss"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMryWfaHR74l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.optim.optimizer import Optimizer,required\n",
        "\n",
        "class SARAH(Optimizer):\n",
        "    r\"\"\"Implements SARAH\n",
        "    \"\"\"\n",
        "    def __init__(self, params, params_prev, lr=required, weight_decay=0):\n",
        "\n",
        "        defaults = dict(lr=lr, weight_decay=weight_decay)\n",
        "        super(SARAH, self).__init__(params, defaults)\n",
        "        self.grad_buff = None\n",
        "        self.params_prev = params_prev\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(SARAH, self).__setstate__(state)\n",
        "    \n",
        "    def zero_grad(self):\n",
        "        super(SARAH, self).zero_grad()\n",
        "        for p in self.params_prev:\n",
        "            if p.grad is not None:\n",
        "                p.grad.detach_()\n",
        "                p.grad.zero_()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"Performs a single optimization step.\n",
        "        \"\"\"\n",
        "        loss = None\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            weight_decay = group['weight_decay']\n",
        "            for x, prev_x in zip(group['params'],self.params_prev):\n",
        "                if x.grad is None:\n",
        "                    continue\n",
        "                if weight_decay != 0:\n",
        "                    x.grad.add_(x, alpha=weight_decay)\n",
        "                # x.grad gradient has same shape like the parameter but \n",
        "                # zeros at the positions which are not updated.\n",
        "                # grad_p has same size as batch times r \n",
        "                #param_state = self.state[x]\n",
        "                if self.grad_buff is None: #do full gradient update\n",
        "                    self.grad_buff = torch.clone(x.grad).detach() \n",
        "                else:\n",
        "                    self.grad_buff.add_(x.grad, alpha=1)\n",
        "                    self.grad_buff.add_(prev_x.grad, alpha=-1) #g_t = g_t-1  +  grad_t - grad_t-1\n",
        "                prev_x.mul_(0).add_(torch.clone(x),alpha=1)  \n",
        "                x.add_(x.grad, alpha=-group['lr'])\n",
        "\n",
        "        return loss"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFPTe8DbxluM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2665b62c-93ad-416e-9cff-c6eda17a0dc2"
      },
      "source": [
        "n*5"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NCBk6tYdQj28",
        "colab": {}
      },
      "source": [
        "def train(epoch,alpha):\n",
        "  model.train()\n",
        "  model_prev.train()\n",
        "  cum_loss = 0.\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "      lr_mean, lambda_mean =0,0\n",
        "      if cuda:\n",
        "        data, target = data.cuda(), target.cuda()\n",
        "      data, target = Variable(data), Variable(target.double())\n",
        "      \n",
        "      for group in param_list:\n",
        "        optimizer = group[\"optimizer\"]\n",
        "        optParam = optimizer.param_groups[0]\n",
        "        stepsize = group[\"step\"]\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        output_prev = model_prev(data)\n",
        "        loss = loss_func(output, target)\n",
        "        loss_prev = loss_func(output_prev, target)\n",
        "        loss.backward()\n",
        "        loss_prev.backward()\n",
        "        #print(\"grad:\",optParam['params'][0].grad)\n",
        "        #print(\"grad nonzero:\",(optParam['params'][0].grad !=0)*reg_weight*optParam['lr'])\n",
        "        optParam['lr'] =stepsize(model,data)/2\n",
        "        lr_mean += optParam['lr']\n",
        "        optimizer.step()\n",
        "        if \"prox\" in group:\n",
        "            prox = group[\"prox\"]\n",
        "            #The whole factor matrix is proxed for one batch? \n",
        "            #Most of the time, this is ok because not often there is no tupel for one row/column.\n",
        "            prox(optParam['params'][0].data,group[\"lambda\"],optParam['lr'], alpha=alpha) \n",
        "            lambda_mean += torch.mean(group[\"lambda\"])\n",
        "        cum_loss+=loss.item()/3\n",
        "        if batch_idx % 2 +1 == 0:\n",
        "            print('Train Epoch:\\t\\t\\t {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "  if epoch % 3 == 0:\n",
        "    print('==Train Epoch:\\t\\t\\t {} \\tLoss: {:.6f}\\t lambda: {:.3e}\\t lr: {:.3f}'.format(\n",
        "        epoch, cum_loss/len(train_loader),lambda_mean/2,lr_mean/3))\n",
        "\n",
        "#\n",
        "# Train full grad\n",
        "#\n",
        "def train_full_grad(epoch,alpha):\n",
        "  model.train()\n",
        "  cum_loss = 0.\n",
        "  lambda_mean, lr_mean=0,0\n",
        "  #for every factor matrix - optimizer\n",
        "  for group in param_list: \n",
        "      optimizer = group[\"optimizer\"]\n",
        "      optimizer.grad_buff =None #Sign that we do a full grad update\n",
        "      optimizer.zero_grad()\n",
        "      optParam = optimizer.param_groups[0]\n",
        "      stepsize = group[\"step\"]\n",
        "      for batch_idx, (data, target) in enumerate(train_loader):\n",
        "          if cuda:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "          data, target = Variable(data), Variable(target.double())\n",
        "            \n",
        "          output = model(data)\n",
        "          # loss is mean squared error over a batch \n",
        "          loss = loss_func(output, target)*bs/n/m\n",
        "          loss.backward()\n",
        "          cum_loss+=loss.item()\n",
        "      # gamma = 1/(2L), L is normalized with bs but this is a full grad update  \n",
        "      optParam['lr'] =stepsize(model,data)/2 #TODO stepsize is computed for one batch! \n",
        "      lr_mean+= optParam['lr']\n",
        "      optimizer.step()\n",
        "      if \"prox\" in group:\n",
        "          prox = group[\"prox\"]\n",
        "          prox(optParam['params'][0].data,group[\"lambda\"],optParam['lr'], alpha=alpha) \n",
        "          lambda_mean += torch.mean(group[\"lambda\"])\n",
        "      if batch_idx % 2 +1 == 0:\n",
        "          print('Train Full Grad Batch:\\t {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "  if epoch % 3 == 0:\n",
        "    print('==Train Full Grad Epoch:\\t {} \\tLoss: {:.6f}\\t lambda: {:.3e}\\t lr: {:.3f}'.format(\n",
        "        epoch, cum_loss/3,lambda_mean/2,lr_mean/3))\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3U0TuUT4MtZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test():\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    for data, target in test_loader:\n",
        "        if cuda:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "        data, target = Variable(data), Variable(target)\n",
        "\n",
        "        output = model(data)\n",
        "        # sum up batch loss\n",
        "        test_loss += loss_func(output, target).item()\n",
        "\n",
        "    test_loss /= len(test_loader)\n",
        "    print('\\nTest set: Average loss: {:.4f}\\n'.format(test_loss))"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRb6bddpbFiQ",
        "colab_type": "text"
      },
      "source": [
        "Can L be 0? If not, there might be an error! I've seen that L_C is zero"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wP2VSzKamWUg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def is_binary(A):\n",
        "  return ((A<1) *(A>0)).sum().item() == 0"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QlF9qB6xWJ2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7d2fbbd8-44fa-4523-b8ca-533859d514b8"
      },
      "source": [
        "model = MatrixFactorization(m, n, r=r,sigma=0)\n",
        "model.to(dev)\n",
        "model_prev = MatrixFactorization(m, n, r=r,sigma=0)\n",
        "model_prev.to(dev)\n",
        "optimizerY = SARAH([model.Y.fact.weight],[model_prev.Y.fact.weight], lr=0.1) # learning rate\n",
        "optimizerX = SARAH([model.X.fact.weight],[model_prev.X.fact.weight], lr=0.1) # learning rate\n",
        "optimizerC = SARAH([model.C.fact.weight],[model_prev.C.fact.weight], lr=0.1)\n",
        "lambdasY = torch.zeros_like(model.Y.fact.weight) #TODO is prev_model also proxed?\n",
        "lambdasX = torch.zeros_like(model.X.fact.weight)\n",
        "param_list = [{'optimizer': optimizerX, 'step': stepsizeX, 'prox':prox_binary_, 'lambda':lambdasX},\n",
        "              {'optimizer': optimizerY, 'step': stepsizeY, 'prox':prox_binary_, 'lambda':lambdasY},\n",
        "              {'optimizer': optimizerC, 'step': stepsizeC, 'prox':prox_pos_, 'lambda':torch.tensor([0.0])}]\n",
        "loss_func = torch.nn.MSELoss()\n",
        "epoch = 1\n",
        "test()\n",
        "alpha=-1e-8\n",
        "thresh=0.01\n",
        "full_grad_prob=0.3\n",
        "while not is_binary(model.Y.fact.weight.data) or not is_binary(model.X.fact.weight.data):\n",
        "    full_batch_grad = np.random.binomial(1,full_grad_prob)\n",
        "    if full_batch_grad:\n",
        "      train_full_grad(epoch,alpha)\n",
        "    else:\n",
        "      train(epoch,alpha)\n",
        "    if epoch % 6 == 0:\n",
        "      phiX, phiY = torch.mean(phi(model.X.fact.weight.data)), torch.mean(phi(model.Y.fact.weight.data))\n",
        "      print('--\\t\\t\\tphi(X):\\t {:.3f} \\tphi(Y): {:.3f}'.format(phiX,phiY))\n",
        "      if max(phiX,phiY) < thresh:\n",
        "        alpha*=2\n",
        "        thresh/=2\n",
        "    epoch+=1\n",
        "    if epoch % 50 == 0:\n",
        "      test()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 4.4215\n",
            "\n",
            "==Train Epoch:\t\t\t 3 \tLoss: 2.256917\t lambda: 1.229e-07\t lr: 1.643\n",
            "==Train Epoch:\t\t\t 6 \tLoss: 0.893256\t lambda: 4.191e-07\t lr: 1.060\n",
            "--\t\t\tphi(X):\t 0.253 \tphi(Y): 0.232\n",
            "==Train Full Grad Epoch:\t 9 \tLoss: 0.743900\t lambda: 5.873e-07\t lr: 0.878\n",
            "==Train Epoch:\t\t\t 12 \tLoss: 0.621190\t lambda: 1.052e-06\t lr: 0.740\n",
            "--\t\t\tphi(X):\t 0.260 \tphi(Y): 0.188\n",
            "==Train Epoch:\t\t\t 15 \tLoss: 0.534607\t lambda: 1.517e-06\t lr: 0.620\n",
            "==Train Epoch:\t\t\t 18 \tLoss: 0.481876\t lambda: 1.980e-06\t lr: 0.517\n",
            "--\t\t\tphi(X):\t 0.261 \tphi(Y): 0.198\n",
            "==Train Epoch:\t\t\t 21 \tLoss: 0.458119\t lambda: 2.295e-06\t lr: 0.516\n",
            "==Train Full Grad Epoch:\t 24 \tLoss: 0.437350\t lambda: 2.609e-06\t lr: 0.511\n",
            "--\t\t\tphi(X):\t 0.263 \tphi(Y): 0.207\n",
            "==Train Epoch:\t\t\t 27 \tLoss: 0.427172\t lambda: 2.922e-06\t lr: 0.473\n",
            "==Train Epoch:\t\t\t 30 \tLoss: 0.416427\t lambda: 3.234e-06\t lr: 0.479\n",
            "--\t\t\tphi(X):\t 0.269 \tphi(Y): 0.215\n",
            "==Train Epoch:\t\t\t 33 \tLoss: 0.404738\t lambda: 3.687e-06\t lr: 0.467\n",
            "==Train Full Grad Epoch:\t 36 \tLoss: 0.400342\t lambda: 3.853e-06\t lr: 0.456\n",
            "--\t\t\tphi(X):\t 0.273 \tphi(Y): 0.219\n",
            "==Train Epoch:\t\t\t 39 \tLoss: 0.396107\t lambda: 4.162e-06\t lr: 0.437\n",
            "==Train Epoch:\t\t\t 42 \tLoss: 0.391784\t lambda: 4.470e-06\t lr: 0.497\n",
            "--\t\t\tphi(X):\t 0.277 \tphi(Y): 0.223\n",
            "==Train Epoch:\t\t\t 45 \tLoss: 0.386500\t lambda: 4.919e-06\t lr: 0.349\n",
            "==Train Epoch:\t\t\t 48 \tLoss: 0.381662\t lambda: 5.368e-06\t lr: 0.417\n",
            "--\t\t\tphi(X):\t 0.282 \tphi(Y): 0.226\n",
            "\n",
            "Test set: Average loss: 0.3699\n",
            "\n",
            "==Train Epoch:\t\t\t 51 \tLoss: 0.379151\t lambda: 5.674e-06\t lr: 0.404\n",
            "==Train Epoch:\t\t\t 54 \tLoss: 0.375612\t lambda: 6.121e-06\t lr: 0.411\n",
            "--\t\t\tphi(X):\t 0.284 \tphi(Y): 0.226\n",
            "==Train Full Grad Epoch:\t 57 \tLoss: 0.373235\t lambda: 6.426e-06\t lr: 0.402\n",
            "==Train Epoch:\t\t\t 60 \tLoss: 0.372336\t lambda: 6.590e-06\t lr: 0.400\n",
            "--\t\t\tphi(X):\t 0.285 \tphi(Y): 0.227\n",
            "==Train Epoch:\t\t\t 63 \tLoss: 0.370628\t lambda: 6.894e-06\t lr: 0.339\n",
            "==Train Full Grad Epoch:\t 66 \tLoss: 0.369052\t lambda: 7.199e-06\t lr: 0.292\n",
            "--\t\t\tphi(X):\t 0.286 \tphi(Y): 0.228\n",
            "==Train Epoch:\t\t\t 69 \tLoss: 0.367313\t lambda: 7.645e-06\t lr: 0.380\n",
            "==Train Epoch:\t\t\t 72 \tLoss: 0.365808\t lambda: 7.949e-06\t lr: 0.369\n",
            "--\t\t\tphi(X):\t 0.286 \tphi(Y): 0.229\n",
            "==Train Full Grad Epoch:\t 75 \tLoss: 0.365058\t lambda: 8.254e-06\t lr: 0.317\n",
            "==Train Epoch:\t\t\t 78 \tLoss: 0.363589\t lambda: 8.699e-06\t lr: 0.376\n",
            "--\t\t\tphi(X):\t 0.285 \tphi(Y): 0.230\n",
            "==Train Epoch:\t\t\t 81 \tLoss: 0.362616\t lambda: 9.145e-06\t lr: 0.350\n",
            "==Train Full Grad Epoch:\t 84 \tLoss: 0.362200\t lambda: 9.449e-06\t lr: 0.363\n",
            "--\t\t\tphi(X):\t 0.284 \tphi(Y): 0.231\n",
            "==Train Epoch:\t\t\t 87 \tLoss: 0.361099\t lambda: 9.895e-06\t lr: 0.383\n",
            "==Train Epoch:\t\t\t 90 \tLoss: 0.360268\t lambda: 1.034e-05\t lr: 0.388\n",
            "--\t\t\tphi(X):\t 0.283 \tphi(Y): 0.232\n",
            "==Train Epoch:\t\t\t 93 \tLoss: 0.359723\t lambda: 1.064e-05\t lr: 0.368\n",
            "==Train Full Grad Epoch:\t 96 \tLoss: 0.359707\t lambda: 1.095e-05\t lr: 0.317\n",
            "--\t\t\tphi(X):\t 0.282 \tphi(Y): 0.233\n",
            "==Train Epoch:\t\t\t 99 \tLoss: 0.358800\t lambda: 1.139e-05\t lr: 0.364\n",
            "\n",
            "Test set: Average loss: 0.3491\n",
            "\n",
            "==Train Epoch:\t\t\t 102 \tLoss: 0.358322\t lambda: 1.170e-05\t lr: 0.353\n",
            "--\t\t\tphi(X):\t 0.280 \tphi(Y): 0.233\n",
            "==Train Full Grad Epoch:\t 105 \tLoss: 0.358198\t lambda: 1.200e-05\t lr: 0.323\n",
            "==Train Epoch:\t\t\t 108 \tLoss: 0.357633\t lambda: 1.217e-05\t lr: 0.380\n",
            "--\t\t\tphi(X):\t 0.279 \tphi(Y): 0.234\n",
            "==Train Epoch:\t\t\t 111 \tLoss: 0.357203\t lambda: 1.247e-05\t lr: 0.349\n",
            "==Train Epoch:\t\t\t 114 \tLoss: 0.356435\t lambda: 1.292e-05\t lr: 0.333\n",
            "--\t\t\tphi(X):\t 0.277 \tphi(Y): 0.234\n",
            "==Train Epoch:\t\t\t 117 \tLoss: 0.355985\t lambda: 1.308e-05\t lr: 0.357\n",
            "==Train Epoch:\t\t\t 120 \tLoss: 0.355683\t lambda: 1.325e-05\t lr: 0.281\n",
            "--\t\t\tphi(X):\t 0.276 \tphi(Y): 0.234\n",
            "==Train Epoch:\t\t\t 123 \tLoss: 0.354598\t lambda: 1.369e-05\t lr: 0.346\n",
            "==Train Epoch:\t\t\t 126 \tLoss: 0.353670\t lambda: 1.400e-05\t lr: 0.345\n",
            "--\t\t\tphi(X):\t 0.273 \tphi(Y): 0.234\n",
            "==Train Full Grad Epoch:\t 129 \tLoss: 0.352631\t lambda: 1.431e-05\t lr: 0.335\n",
            "==Train Epoch:\t\t\t 132 \tLoss: 0.350359\t lambda: 1.476e-05\t lr: 0.338\n",
            "--\t\t\tphi(X):\t 0.269 \tphi(Y): 0.233\n",
            "==Train Full Grad Epoch:\t 135 \tLoss: 0.348325\t lambda: 1.506e-05\t lr: 0.324\n",
            "==Train Epoch:\t\t\t 138 \tLoss: 0.344791\t lambda: 1.551e-05\t lr: 0.337\n",
            "--\t\t\tphi(X):\t 0.265 \tphi(Y): 0.231\n",
            "==Train Epoch:\t\t\t 141 \tLoss: 0.339331\t lambda: 1.597e-05\t lr: 0.290\n",
            "==Train Epoch:\t\t\t 144 \tLoss: 0.334406\t lambda: 1.628e-05\t lr: 0.373\n",
            "--\t\t\tphi(X):\t 0.258 \tphi(Y): 0.228\n",
            "==Train Epoch:\t\t\t 147 \tLoss: 0.327478\t lambda: 1.659e-05\t lr: 0.347\n",
            "\n",
            "Test set: Average loss: 0.3065\n",
            "\n",
            "==Train Full Grad Epoch:\t 150 \tLoss: 0.316313\t lambda: 1.690e-05\t lr: 0.310\n",
            "--\t\t\tphi(X):\t 0.249 \tphi(Y): 0.224\n",
            "==Train Epoch:\t\t\t 153 \tLoss: 0.299264\t lambda: 1.736e-05\t lr: 0.303\n",
            "==Train Epoch:\t\t\t 156 \tLoss: 0.290119\t lambda: 1.753e-05\t lr: 0.290\n",
            "--\t\t\tphi(X):\t 0.237 \tphi(Y): 0.219\n",
            "==Train Epoch:\t\t\t 159 \tLoss: 0.279813\t lambda: 1.770e-05\t lr: 0.354\n",
            "==Train Full Grad Epoch:\t 162 \tLoss: 0.249941\t lambda: 1.802e-05\t lr: 0.340\n",
            "--\t\t\tphi(X):\t 0.224 \tphi(Y): 0.214\n",
            "==Train Epoch:\t\t\t 165 \tLoss: 0.226458\t lambda: 1.834e-05\t lr: 0.314\n",
            "==Train Epoch:\t\t\t 168 \tLoss: 0.182210\t lambda: 1.881e-05\t lr: 0.335\n",
            "--\t\t\tphi(X):\t 0.199 \tphi(Y): 0.203\n",
            "==Train Full Grad Epoch:\t 171 \tLoss: 0.174670\t lambda: 1.884e-05\t lr: 0.348\n",
            "==Train Full Grad Epoch:\t 174 \tLoss: 0.152233\t lambda: 1.917e-05\t lr: 0.321\n",
            "--\t\t\tphi(X):\t 0.189 \tphi(Y): 0.201\n",
            "==Train Epoch:\t\t\t 177 \tLoss: 0.129294\t lambda: 1.965e-05\t lr: 0.350\n",
            "==Train Full Grad Epoch:\t 180 \tLoss: 0.118104\t lambda: 1.983e-05\t lr: 0.253\n",
            "--\t\t\tphi(X):\t 0.172 \tphi(Y): 0.200\n",
            "==Train Epoch:\t\t\t 183 \tLoss: 0.104313\t lambda: 2.032e-05\t lr: 0.317\n",
            "==Train Epoch:\t\t\t 186 \tLoss: 0.095870\t lambda: 2.066e-05\t lr: 0.332\n",
            "--\t\t\tphi(X):\t 0.160 \tphi(Y): 0.202\n",
            "==Train Full Grad Epoch:\t 189 \tLoss: 0.090885\t lambda: 2.084e-05\t lr: 0.332\n",
            "==Train Full Grad Epoch:\t 192 \tLoss: 0.085325\t lambda: 2.117e-05\t lr: 0.316\n",
            "--\t\t\tphi(X):\t 0.155 \tphi(Y): 0.204\n",
            "==Train Epoch:\t\t\t 195 \tLoss: 0.081430\t lambda: 2.151e-05\t lr: 0.310\n",
            "==Train Epoch:\t\t\t 198 \tLoss: 0.077257\t lambda: 2.185e-05\t lr: 0.340\n",
            "--\t\t\tphi(X):\t 0.150 \tphi(Y): 0.209\n",
            "\n",
            "Test set: Average loss: 0.0648\n",
            "\n",
            "==Train Full Grad Epoch:\t 201 \tLoss: 0.072994\t lambda: 2.218e-05\t lr: 0.305\n",
            "==Train Epoch:\t\t\t 204 \tLoss: 0.068872\t lambda: 2.267e-05\t lr: 0.316\n",
            "--\t\t\tphi(X):\t 0.145 \tphi(Y): 0.216\n",
            "==Train Epoch:\t\t\t 207 \tLoss: 0.064610\t lambda: 2.317e-05\t lr: 0.306\n",
            "==Train Epoch:\t\t\t 210 \tLoss: 0.060774\t lambda: 2.366e-05\t lr: 0.339\n",
            "--\t\t\tphi(X):\t 0.140 \tphi(Y): 0.223\n",
            "==Train Epoch:\t\t\t 213 \tLoss: 0.058351\t lambda: 2.399e-05\t lr: 0.324\n",
            "==Train Epoch:\t\t\t 216 \tLoss: 0.055040\t lambda: 2.448e-05\t lr: 0.336\n",
            "--\t\t\tphi(X):\t 0.136 \tphi(Y): 0.229\n",
            "==Train Epoch:\t\t\t 219 \tLoss: 0.052917\t lambda: 2.482e-05\t lr: 0.291\n",
            "==Train Full Grad Epoch:\t 222 \tLoss: 0.050493\t lambda: 2.515e-05\t lr: 0.325\n",
            "--\t\t\tphi(X):\t 0.134 \tphi(Y): 0.233\n",
            "==Train Full Grad Epoch:\t 225 \tLoss: 0.048524\t lambda: 2.549e-05\t lr: 0.323\n",
            "==Train Epoch:\t\t\t 228 \tLoss: 0.046086\t lambda: 2.598e-05\t lr: 0.312\n",
            "--\t\t\tphi(X):\t 0.132 \tphi(Y): 0.238\n",
            "==Train Full Grad Epoch:\t 231 \tLoss: 0.043914\t lambda: 2.631e-05\t lr: 0.276\n",
            "==Train Epoch:\t\t\t 234 \tLoss: 0.043277\t lambda: 2.649e-05\t lr: 0.287\n",
            "--\t\t\tphi(X):\t 0.130 \tphi(Y): 0.241\n",
            "==Train Full Grad Epoch:\t 237 \tLoss: 0.041225\t lambda: 2.683e-05\t lr: 0.329\n",
            "==Train Epoch:\t\t\t 240 \tLoss: 0.039826\t lambda: 2.716e-05\t lr: 0.314\n",
            "--\t\t\tphi(X):\t 0.128 \tphi(Y): 0.245\n",
            "==Train Full Grad Epoch:\t 243 \tLoss: 0.038609\t lambda: 2.734e-05\t lr: 0.310\n",
            "==Train Epoch:\t\t\t 246 \tLoss: 0.036654\t lambda: 2.783e-05\t lr: 0.278\n",
            "--\t\t\tphi(X):\t 0.126 \tphi(Y): 0.248\n",
            "==Train Epoch:\t\t\t 249 \tLoss: 0.035439\t lambda: 2.816e-05\t lr: 0.280\n",
            "\n",
            "Test set: Average loss: 0.0254\n",
            "\n",
            "==Train Epoch:\t\t\t 252 \tLoss: 0.033798\t lambda: 2.865e-05\t lr: 0.318\n",
            "--\t\t\tphi(X):\t 0.124 \tphi(Y): 0.251\n",
            "==Train Full Grad Epoch:\t 255 \tLoss: 0.032586\t lambda: 2.898e-05\t lr: 0.288\n",
            "==Train Epoch:\t\t\t 258 \tLoss: 0.031380\t lambda: 2.947e-05\t lr: 0.282\n",
            "--\t\t\tphi(X):\t 0.121 \tphi(Y): 0.253\n",
            "==Train Full Grad Epoch:\t 261 \tLoss: 0.030730\t lambda: 2.965e-05\t lr: 0.341\n",
            "==Train Epoch:\t\t\t 264 \tLoss: 0.030435\t lambda: 2.983e-05\t lr: 0.305\n",
            "--\t\t\tphi(X):\t 0.120 \tphi(Y): 0.254\n",
            "==Train Epoch:\t\t\t 267 \tLoss: 0.029624\t lambda: 3.016e-05\t lr: 0.304\n",
            "==Train Epoch:\t\t\t 270 \tLoss: 0.028866\t lambda: 3.049e-05\t lr: 0.275\n",
            "--\t\t\tphi(X):\t 0.118 \tphi(Y): 0.255\n",
            "==Train Epoch:\t\t\t 273 \tLoss: 0.028475\t lambda: 3.067e-05\t lr: 0.305\n",
            "==Train Epoch:\t\t\t 276 \tLoss: 0.028111\t lambda: 3.085e-05\t lr: 0.294\n",
            "--\t\t\tphi(X):\t 0.117 \tphi(Y): 0.256\n",
            "==Train Epoch:\t\t\t 279 \tLoss: 0.027175\t lambda: 3.134e-05\t lr: 0.292\n",
            "==Train Epoch:\t\t\t 282 \tLoss: 0.026587\t lambda: 3.167e-05\t lr: 0.297\n",
            "--\t\t\tphi(X):\t 0.115 \tphi(Y): 0.257\n",
            "==Train Full Grad Epoch:\t 285 \tLoss: 0.025957\t lambda: 3.201e-05\t lr: 0.316\n",
            "==Train Epoch:\t\t\t 288 \tLoss: 0.025309\t lambda: 3.249e-05\t lr: 0.292\n",
            "--\t\t\tphi(X):\t 0.114 \tphi(Y): 0.258\n",
            "==Train Epoch:\t\t\t 291 \tLoss: 0.024657\t lambda: 3.298e-05\t lr: 0.272\n",
            "==Train Epoch:\t\t\t 294 \tLoss: 0.024429\t lambda: 3.316e-05\t lr: 0.305\n",
            "--\t\t\tphi(X):\t 0.112 \tphi(Y): 0.259\n",
            "==Train Epoch:\t\t\t 297 \tLoss: 0.023863\t lambda: 3.365e-05\t lr: 0.298\n",
            "\n",
            "Test set: Average loss: 0.0137\n",
            "\n",
            "==Train Epoch:\t\t\t 300 \tLoss: 0.023352\t lambda: 3.414e-05\t lr: 0.284\n",
            "--\t\t\tphi(X):\t 0.110 \tphi(Y): 0.260\n",
            "==Train Epoch:\t\t\t 303 \tLoss: 0.022891\t lambda: 3.463e-05\t lr: 0.286\n",
            "==Train Full Grad Epoch:\t 306 \tLoss: 0.022572\t lambda: 3.496e-05\t lr: 0.308\n",
            "--\t\t\tphi(X):\t 0.109 \tphi(Y): 0.260\n",
            "==Train Full Grad Epoch:\t 309 \tLoss: 0.022308\t lambda: 3.530e-05\t lr: 0.270\n",
            "==Train Epoch:\t\t\t 312 \tLoss: 0.021985\t lambda: 3.579e-05\t lr: 0.272\n",
            "--\t\t\tphi(X):\t 0.107 \tphi(Y): 0.261\n",
            "==Train Epoch:\t\t\t 315 \tLoss: 0.021755\t lambda: 3.612e-05\t lr: 0.285\n",
            "==Train Epoch:\t\t\t 318 \tLoss: 0.021543\t lambda: 3.646e-05\t lr: 0.300\n",
            "--\t\t\tphi(X):\t 0.106 \tphi(Y): 0.261\n",
            "==Train Epoch:\t\t\t 321 \tLoss: 0.021261\t lambda: 3.695e-05\t lr: 0.285\n",
            "==Train Epoch:\t\t\t 324 \tLoss: 0.021157\t lambda: 3.713e-05\t lr: 0.262\n",
            "--\t\t\tphi(X):\t 0.105 \tphi(Y): 0.261\n",
            "==Train Full Grad Epoch:\t 327 \tLoss: 0.021050\t lambda: 3.730e-05\t lr: 0.300\n",
            "==Train Full Grad Epoch:\t 330 \tLoss: 0.020960\t lambda: 3.748e-05\t lr: 0.301\n",
            "--\t\t\tphi(X):\t 0.105 \tphi(Y): 0.261\n",
            "==Train Epoch:\t\t\t 333 \tLoss: 0.020734\t lambda: 3.797e-05\t lr: 0.290\n",
            "==Train Epoch:\t\t\t 336 \tLoss: 0.020588\t lambda: 3.831e-05\t lr: 0.302\n",
            "--\t\t\tphi(X):\t 0.104 \tphi(Y): 0.261\n",
            "==Train Full Grad Epoch:\t 339 \tLoss: 0.020444\t lambda: 3.865e-05\t lr: 0.320\n",
            "==Train Epoch:\t\t\t 342 \tLoss: 0.020253\t lambda: 3.914e-05\t lr: 0.315\n",
            "--\t\t\tphi(X):\t 0.103 \tphi(Y): 0.261\n",
            "==Train Epoch:\t\t\t 345 \tLoss: 0.020078\t lambda: 3.963e-05\t lr: 0.271\n",
            "==Train Epoch:\t\t\t 348 \tLoss: 0.020014\t lambda: 3.981e-05\t lr: 0.286\n",
            "--\t\t\tphi(X):\t 0.102 \tphi(Y): 0.261\n",
            "\n",
            "Test set: Average loss: 0.0102\n",
            "\n",
            "==Train Full Grad Epoch:\t 351 \tLoss: 0.019963\t lambda: 3.999e-05\t lr: 0.269\n",
            "==Train Epoch:\t\t\t 354 \tLoss: 0.019846\t lambda: 4.032e-05\t lr: 0.305\n",
            "--\t\t\tphi(X):\t 0.101 \tphi(Y): 0.261\n",
            "==Train Epoch:\t\t\t 357 \tLoss: 0.019747\t lambda: 4.066e-05\t lr: 0.303\n",
            "==Train Full Grad Epoch:\t 360 \tLoss: 0.019657\t lambda: 4.099e-05\t lr: 0.270\n",
            "--\t\t\tphi(X):\t 0.100 \tphi(Y): 0.261\n",
            "==Train Epoch:\t\t\t 363 \tLoss: 0.019555\t lambda: 4.133e-05\t lr: 0.277\n",
            "==Train Full Grad Epoch:\t 366 \tLoss: 0.019513\t lambda: 4.151e-05\t lr: 0.281\n",
            "--\t\t\tphi(X):\t 0.100 \tphi(Y): 0.261\n",
            "==Train Full Grad Epoch:\t 369 \tLoss: 0.019501\t lambda: 4.154e-05\t lr: 0.308\n",
            "==Train Epoch:\t\t\t 372 \tLoss: 0.019412\t lambda: 4.187e-05\t lr: 0.292\n",
            "--\t\t\tphi(X):\t 0.099 \tphi(Y): 0.261\n",
            "==Train Epoch:\t\t\t 375 \tLoss: 0.019330\t lambda: 4.221e-05\t lr: 0.321\n",
            "==Train Epoch:\t\t\t 378 \tLoss: 0.019216\t lambda: 4.270e-05\t lr: 0.240\n",
            "--\t\t\tphi(X):\t 0.098 \tphi(Y): 0.260\n",
            "==Train Epoch:\t\t\t 381 \tLoss: 0.019172\t lambda: 4.288e-05\t lr: 0.293\n",
            "==Train Full Grad Epoch:\t 384 \tLoss: 0.019111\t lambda: 4.322e-05\t lr: 0.318\n",
            "--\t\t\tphi(X):\t 0.098 \tphi(Y): 0.260\n",
            "==Train Epoch:\t\t\t 387 \tLoss: 0.018993\t lambda: 4.371e-05\t lr: 0.275\n",
            "==Train Full Grad Epoch:\t 390 \tLoss: 0.018971\t lambda: 4.389e-05\t lr: 0.277\n",
            "--\t\t\tphi(X):\t 0.097 \tphi(Y): 0.260\n",
            "==Train Epoch:\t\t\t 393 \tLoss: 0.018891\t lambda: 4.423e-05\t lr: 0.258\n",
            "==Train Epoch:\t\t\t 396 \tLoss: 0.018855\t lambda: 4.441e-05\t lr: 0.283\n",
            "--\t\t\tphi(X):\t 0.097 \tphi(Y): 0.260\n",
            "==Train Full Grad Epoch:\t 399 \tLoss: 0.018831\t lambda: 4.459e-05\t lr: 0.289\n",
            "\n",
            "Test set: Average loss: 0.0091\n",
            "\n",
            "==Train Epoch:\t\t\t 402 \tLoss: 0.018787\t lambda: 4.477e-05\t lr: 0.310\n",
            "--\t\t\tphi(X):\t 0.096 \tphi(Y): 0.260\n",
            "==Train Full Grad Epoch:\t 405 \tLoss: 0.018790\t lambda: 4.479e-05\t lr: 0.306\n",
            "==Train Epoch:\t\t\t 408 \tLoss: 0.018692\t lambda: 4.529e-05\t lr: 0.324\n",
            "--\t\t\tphi(X):\t 0.096 \tphi(Y): 0.259\n",
            "==Train Epoch:\t\t\t 411 \tLoss: 0.018637\t lambda: 4.563e-05\t lr: 0.285\n",
            "==Train Epoch:\t\t\t 414 \tLoss: 0.018553\t lambda: 4.612e-05\t lr: 0.262\n",
            "--\t\t\tphi(X):\t 0.095 \tphi(Y): 0.259\n",
            "==Train Full Grad Epoch:\t 417 \tLoss: 0.018508\t lambda: 4.646e-05\t lr: 0.278\n",
            "==Train Full Grad Epoch:\t 420 \tLoss: 0.018477\t lambda: 4.664e-05\t lr: 0.273\n",
            "--\t\t\tphi(X):\t 0.095 \tphi(Y): 0.259\n",
            "==Train Full Grad Epoch:\t 423 \tLoss: 0.018467\t lambda: 4.666e-05\t lr: 0.301\n",
            "==Train Epoch:\t\t\t 426 \tLoss: 0.018378\t lambda: 4.716e-05\t lr: 0.303\n",
            "--\t\t\tphi(X):\t 0.094 \tphi(Y): 0.259\n",
            "==Train Epoch:\t\t\t 429 \tLoss: 0.018305\t lambda: 4.765e-05\t lr: 0.306\n",
            "==Train Epoch:\t\t\t 432 \tLoss: 0.018234\t lambda: 4.814e-05\t lr: 0.282\n",
            "--\t\t\tphi(X):\t 0.094 \tphi(Y): 0.258\n",
            "==Train Full Grad Epoch:\t 435 \tLoss: 0.018236\t lambda: 4.817e-05\t lr: 0.283\n",
            "==Train Epoch:\t\t\t 438 \tLoss: 0.018148\t lambda: 4.866e-05\t lr: 0.294\n",
            "--\t\t\tphi(X):\t 0.093 \tphi(Y): 0.258\n",
            "==Train Epoch:\t\t\t 441 \tLoss: 0.018076\t lambda: 4.916e-05\t lr: 0.269\n",
            "==Train Full Grad Epoch:\t 444 \tLoss: 0.018067\t lambda: 4.934e-05\t lr: 0.317\n",
            "--\t\t\tphi(X):\t 0.092 \tphi(Y): 0.258\n",
            "==Train Epoch:\t\t\t 447 \tLoss: 0.018002\t lambda: 4.968e-05\t lr: 0.329\n",
            "\n",
            "Test set: Average loss: 0.0082\n",
            "\n",
            "==Train Epoch:\t\t\t 450 \tLoss: 0.017934\t lambda: 5.017e-05\t lr: 0.285\n",
            "--\t\t\tphi(X):\t 0.092 \tphi(Y): 0.257\n",
            "==Train Full Grad Epoch:\t 453 \tLoss: 0.017910\t lambda: 5.051e-05\t lr: 0.305\n",
            "==Train Epoch:\t\t\t 456 \tLoss: 0.017821\t lambda: 5.101e-05\t lr: 0.259\n",
            "--\t\t\tphi(X):\t 0.091 \tphi(Y): 0.257\n",
            "==Train Epoch:\t\t\t 459 \tLoss: 0.017753\t lambda: 5.150e-05\t lr: 0.296\n",
            "==Train Epoch:\t\t\t 462 \tLoss: 0.017685\t lambda: 5.200e-05\t lr: 0.265\n",
            "--\t\t\tphi(X):\t 0.091 \tphi(Y): 0.256\n",
            "==Train Epoch:\t\t\t 465 \tLoss: 0.017638\t lambda: 5.234e-05\t lr: 0.302\n",
            "==Train Epoch:\t\t\t 468 \tLoss: 0.017573\t lambda: 5.283e-05\t lr: 0.296\n",
            "--\t\t\tphi(X):\t 0.090 \tphi(Y): 0.256\n",
            "==Train Epoch:\t\t\t 471 \tLoss: 0.017505\t lambda: 5.333e-05\t lr: 0.266\n",
            "==Train Full Grad Epoch:\t 474 \tLoss: 0.017501\t lambda: 5.351e-05\t lr: 0.280\n",
            "--\t\t\tphi(X):\t 0.090 \tphi(Y): 0.255\n",
            "==Train Full Grad Epoch:\t 477 \tLoss: 0.017475\t lambda: 5.369e-05\t lr: 0.304\n",
            "==Train Epoch:\t\t\t 480 \tLoss: 0.017431\t lambda: 5.388e-05\t lr: 0.302\n",
            "--\t\t\tphi(X):\t 0.089 \tphi(Y): 0.255\n",
            "==Train Full Grad Epoch:\t 483 \tLoss: 0.017424\t lambda: 5.406e-05\t lr: 0.298\n",
            "==Train Full Grad Epoch:\t 486 \tLoss: 0.017381\t lambda: 5.440e-05\t lr: 0.290\n",
            "--\t\t\tphi(X):\t 0.089 \tphi(Y): 0.255\n",
            "==Train Full Grad Epoch:\t 489 \tLoss: 0.017332\t lambda: 5.474e-05\t lr: 0.325\n",
            "==Train Epoch:\t\t\t 492 \tLoss: 0.017270\t lambda: 5.508e-05\t lr: 0.287\n",
            "--\t\t\tphi(X):\t 0.089 \tphi(Y): 0.254\n",
            "==Train Epoch:\t\t\t 495 \tLoss: 0.017239\t lambda: 5.526e-05\t lr: 0.314\n",
            "==Train Epoch:\t\t\t 498 \tLoss: 0.017197\t lambda: 5.560e-05\t lr: 0.284\n",
            "--\t\t\tphi(X):\t 0.088 \tphi(Y): 0.254\n",
            "\n",
            "Test set: Average loss: 0.0074\n",
            "\n",
            "==Train Epoch:\t\t\t 501 \tLoss: 0.017125\t lambda: 5.610e-05\t lr: 0.296\n",
            "==Train Epoch:\t\t\t 504 \tLoss: 0.017058\t lambda: 5.660e-05\t lr: 0.296\n",
            "--\t\t\tphi(X):\t 0.088 \tphi(Y): 0.253\n",
            "==Train Epoch:\t\t\t 507 \tLoss: 0.016982\t lambda: 5.709e-05\t lr: 0.273\n",
            "==Train Epoch:\t\t\t 510 \tLoss: 0.016940\t lambda: 5.743e-05\t lr: 0.310\n",
            "--\t\t\tphi(X):\t 0.087 \tphi(Y): 0.252\n",
            "==Train Full Grad Epoch:\t 513 \tLoss: 0.016930\t lambda: 5.762e-05\t lr: 0.303\n",
            "==Train Epoch:\t\t\t 516 \tLoss: 0.016860\t lambda: 5.796e-05\t lr: 0.273\n",
            "--\t\t\tphi(X):\t 0.087 \tphi(Y): 0.252\n",
            "==Train Epoch:\t\t\t 519 \tLoss: 0.016812\t lambda: 5.830e-05\t lr: 0.271\n",
            "==Train Epoch:\t\t\t 522 \tLoss: 0.016760\t lambda: 5.864e-05\t lr: 0.306\n",
            "--\t\t\tphi(X):\t 0.086 \tphi(Y): 0.251\n",
            "==Train Epoch:\t\t\t 525 \tLoss: 0.016703\t lambda: 5.898e-05\t lr: 0.293\n",
            "==Train Epoch:\t\t\t 528 \tLoss: 0.016638\t lambda: 5.948e-05\t lr: 0.284\n",
            "--\t\t\tphi(X):\t 0.086 \tphi(Y): 0.250\n",
            "==Train Epoch:\t\t\t 531 \tLoss: 0.016577\t lambda: 5.982e-05\t lr: 0.287\n",
            "==Train Epoch:\t\t\t 534 \tLoss: 0.016502\t lambda: 6.032e-05\t lr: 0.304\n",
            "--\t\t\tphi(X):\t 0.085 \tphi(Y): 0.248\n",
            "==Train Full Grad Epoch:\t 537 \tLoss: 0.016466\t lambda: 6.066e-05\t lr: 0.298\n",
            "==Train Epoch:\t\t\t 540 \tLoss: 0.016390\t lambda: 6.100e-05\t lr: 0.327\n",
            "--\t\t\tphi(X):\t 0.085 \tphi(Y): 0.247\n",
            "==Train Epoch:\t\t\t 543 \tLoss: 0.016338\t lambda: 6.135e-05\t lr: 0.287\n",
            "==Train Full Grad Epoch:\t 546 \tLoss: 0.016301\t lambda: 6.169e-05\t lr: 0.294\n",
            "--\t\t\tphi(X):\t 0.084 \tphi(Y): 0.246\n",
            "==Train Full Grad Epoch:\t 549 \tLoss: 0.016272\t lambda: 6.187e-05\t lr: 0.296\n",
            "\n",
            "Test set: Average loss: 0.0065\n",
            "\n",
            "==Train Epoch:\t\t\t 552 \tLoss: 0.016222\t lambda: 6.206e-05\t lr: 0.300\n",
            "--\t\t\tphi(X):\t 0.084 \tphi(Y): 0.245\n",
            "==Train Full Grad Epoch:\t 555 \tLoss: 0.016236\t lambda: 6.208e-05\t lr: 0.292\n",
            "==Train Epoch:\t\t\t 558 \tLoss: 0.016138\t lambda: 6.258e-05\t lr: 0.297\n",
            "--\t\t\tphi(X):\t 0.084 \tphi(Y): 0.244\n",
            "==Train Epoch:\t\t\t 561 \tLoss: 0.016056\t lambda: 6.308e-05\t lr: 0.281\n",
            "==Train Full Grad Epoch:\t 564 \tLoss: 0.016016\t lambda: 6.343e-05\t lr: 0.299\n",
            "--\t\t\tphi(X):\t 0.083 \tphi(Y): 0.243\n",
            "==Train Epoch:\t\t\t 567 \tLoss: 0.015911\t lambda: 6.393e-05\t lr: 0.315\n",
            "==Train Epoch:\t\t\t 570 \tLoss: 0.015860\t lambda: 6.427e-05\t lr: 0.286\n",
            "--\t\t\tphi(X):\t 0.083 \tphi(Y): 0.241\n",
            "==Train Epoch:\t\t\t 573 \tLoss: 0.015775\t lambda: 6.478e-05\t lr: 0.280\n",
            "==Train Full Grad Epoch:\t 576 \tLoss: 0.015730\t lambda: 6.512e-05\t lr: 0.295\n",
            "--\t\t\tphi(X):\t 0.082 \tphi(Y): 0.240\n",
            "==Train Epoch:\t\t\t 579 \tLoss: 0.015652\t lambda: 6.546e-05\t lr: 0.272\n",
            "==Train Epoch:\t\t\t 582 \tLoss: 0.015594\t lambda: 6.581e-05\t lr: 0.290\n",
            "--\t\t\tphi(X):\t 0.081 \tphi(Y): 0.239\n",
            "==Train Epoch:\t\t\t 585 \tLoss: 0.015507\t lambda: 6.631e-05\t lr: 0.299\n",
            "==Train Epoch:\t\t\t 588 \tLoss: 0.015422\t lambda: 6.682e-05\t lr: 0.301\n",
            "--\t\t\tphi(X):\t 0.081 \tphi(Y): 0.237\n",
            "==Train Epoch:\t\t\t 591 \tLoss: 0.015327\t lambda: 6.732e-05\t lr: 0.321\n",
            "==Train Full Grad Epoch:\t 594 \tLoss: 0.015283\t lambda: 6.767e-05\t lr: 0.292\n",
            "--\t\t\tphi(X):\t 0.080 \tphi(Y): 0.235\n",
            "==Train Epoch:\t\t\t 597 \tLoss: 0.015181\t lambda: 6.817e-05\t lr: 0.298\n",
            "\n",
            "Test set: Average loss: 0.0054\n",
            "\n",
            "==Train Epoch:\t\t\t 600 \tLoss: 0.015146\t lambda: 6.836e-05\t lr: 0.277\n",
            "--\t\t\tphi(X):\t 0.080 \tphi(Y): 0.234\n",
            "==Train Epoch:\t\t\t 603 \tLoss: 0.015056\t lambda: 6.886e-05\t lr: 0.296\n",
            "==Train Epoch:\t\t\t 606 \tLoss: 0.014997\t lambda: 6.921e-05\t lr: 0.275\n",
            "--\t\t\tphi(X):\t 0.079 \tphi(Y): 0.233\n",
            "==Train Epoch:\t\t\t 609 \tLoss: 0.014906\t lambda: 6.972e-05\t lr: 0.306\n",
            "==Train Epoch:\t\t\t 612 \tLoss: 0.014816\t lambda: 7.022e-05\t lr: 0.282\n",
            "--\t\t\tphi(X):\t 0.079 \tphi(Y): 0.231\n",
            "==Train Epoch:\t\t\t 615 \tLoss: 0.014756\t lambda: 7.057e-05\t lr: 0.251\n",
            "==Train Epoch:\t\t\t 618 \tLoss: 0.014669\t lambda: 7.108e-05\t lr: 0.323\n",
            "--\t\t\tphi(X):\t 0.078 \tphi(Y): 0.229\n",
            "==Train Full Grad Epoch:\t 621 \tLoss: 0.014673\t lambda: 7.110e-05\t lr: 0.290\n",
            "==Train Epoch:\t\t\t 624 \tLoss: 0.014601\t lambda: 7.145e-05\t lr: 0.286\n",
            "--\t\t\tphi(X):\t 0.078 \tphi(Y): 0.229\n",
            "==Train Epoch:\t\t\t 627 \tLoss: 0.014536\t lambda: 7.180e-05\t lr: 0.305\n",
            "==Train Epoch:\t\t\t 630 \tLoss: 0.014450\t lambda: 7.231e-05\t lr: 0.289\n",
            "--\t\t\tphi(X):\t 0.077 \tphi(Y): 0.227\n",
            "==Train Epoch:\t\t\t 633 \tLoss: 0.014358\t lambda: 7.281e-05\t lr: 0.268\n",
            "==Train Epoch:\t\t\t 636 \tLoss: 0.014298\t lambda: 7.316e-05\t lr: 0.303\n",
            "--\t\t\tphi(X):\t 0.077 \tphi(Y): 0.226\n",
            "==Train Epoch:\t\t\t 639 \tLoss: 0.014208\t lambda: 7.367e-05\t lr: 0.310\n",
            "==Train Full Grad Epoch:\t 642 \tLoss: 0.014164\t lambda: 7.402e-05\t lr: 0.291\n",
            "--\t\t\tphi(X):\t 0.076 \tphi(Y): 0.224\n",
            "==Train Epoch:\t\t\t 645 \tLoss: 0.014083\t lambda: 7.437e-05\t lr: 0.326\n",
            "==Train Epoch:\t\t\t 648 \tLoss: 0.013999\t lambda: 7.488e-05\t lr: 0.299\n",
            "--\t\t\tphi(X):\t 0.076 \tphi(Y): 0.223\n",
            "\n",
            "Test set: Average loss: 0.0042\n",
            "\n",
            "==Train Epoch:\t\t\t 651 \tLoss: 0.013934\t lambda: 7.523e-05\t lr: 0.287\n",
            "==Train Epoch:\t\t\t 654 \tLoss: 0.013874\t lambda: 7.558e-05\t lr: 0.323\n",
            "--\t\t\tphi(X):\t 0.075 \tphi(Y): 0.222\n",
            "==Train Epoch:\t\t\t 657 \tLoss: 0.013815\t lambda: 7.593e-05\t lr: 0.298\n",
            "==Train Full Grad Epoch:\t 660 \tLoss: 0.013797\t lambda: 7.611e-05\t lr: 0.262\n",
            "--\t\t\tphi(X):\t 0.075 \tphi(Y): 0.221\n",
            "==Train Epoch:\t\t\t 663 \tLoss: 0.013729\t lambda: 7.646e-05\t lr: 0.292\n",
            "==Train Full Grad Epoch:\t 666 \tLoss: 0.013685\t lambda: 7.681e-05\t lr: 0.281\n",
            "--\t\t\tphi(X):\t 0.075 \tphi(Y): 0.220\n",
            "==Train Full Grad Epoch:\t 669 \tLoss: 0.013625\t lambda: 7.716e-05\t lr: 0.291\n",
            "==Train Epoch:\t\t\t 672 \tLoss: 0.013555\t lambda: 7.751e-05\t lr: 0.268\n",
            "--\t\t\tphi(X):\t 0.074 \tphi(Y): 0.219\n",
            "==Train Epoch:\t\t\t 675 \tLoss: 0.013472\t lambda: 7.802e-05\t lr: 0.271\n",
            "==Train Full Grad Epoch:\t 678 \tLoss: 0.013424\t lambda: 7.837e-05\t lr: 0.315\n",
            "--\t\t\tphi(X):\t 0.074 \tphi(Y): 0.217\n",
            "==Train Epoch:\t\t\t 681 \tLoss: 0.013355\t lambda: 7.872e-05\t lr: 0.287\n",
            "==Train Epoch:\t\t\t 684 \tLoss: 0.013272\t lambda: 7.924e-05\t lr: 0.261\n",
            "--\t\t\tphi(X):\t 0.073 \tphi(Y): 0.216\n",
            "==Train Epoch:\t\t\t 687 \tLoss: 0.013241\t lambda: 7.943e-05\t lr: 0.281\n",
            "==Train Full Grad Epoch:\t 690 \tLoss: 0.013197\t lambda: 7.978e-05\t lr: 0.275\n",
            "--\t\t\tphi(X):\t 0.072 \tphi(Y): 0.215\n",
            "==Train Epoch:\t\t\t 693 \tLoss: 0.013103\t lambda: 8.029e-05\t lr: 0.304\n",
            "==Train Full Grad Epoch:\t 696 \tLoss: 0.013062\t lambda: 8.064e-05\t lr: 0.234\n",
            "--\t\t\tphi(X):\t 0.072 \tphi(Y): 0.214\n",
            "==Train Full Grad Epoch:\t 699 \tLoss: 0.013005\t lambda: 8.099e-05\t lr: 0.281\n",
            "\n",
            "Test set: Average loss: 0.0032\n",
            "\n",
            "==Train Full Grad Epoch:\t 702 \tLoss: 0.012953\t lambda: 8.135e-05\t lr: 0.282\n",
            "--\t\t\tphi(X):\t 0.071 \tphi(Y): 0.212\n",
            "==Train Epoch:\t\t\t 705 \tLoss: 0.012888\t lambda: 8.170e-05\t lr: 0.291\n",
            "==Train Epoch:\t\t\t 708 \tLoss: 0.012813\t lambda: 8.221e-05\t lr: 0.306\n",
            "--\t\t\tphi(X):\t 0.070 \tphi(Y): 0.211\n",
            "==Train Epoch:\t\t\t 711 \tLoss: 0.012759\t lambda: 8.257e-05\t lr: 0.300\n",
            "==Train Epoch:\t\t\t 714 \tLoss: 0.012710\t lambda: 8.292e-05\t lr: 0.290\n",
            "--\t\t\tphi(X):\t 0.070 \tphi(Y): 0.210\n",
            "==Train Full Grad Epoch:\t 717 \tLoss: 0.012717\t lambda: 8.294e-05\t lr: 0.304\n",
            "==Train Full Grad Epoch:\t 720 \tLoss: 0.012691\t lambda: 8.313e-05\t lr: 0.273\n",
            "--\t\t\tphi(X):\t 0.070 \tphi(Y): 0.210\n",
            "==Train Epoch:\t\t\t 723 \tLoss: 0.012607\t lambda: 8.365e-05\t lr: 0.300\n",
            "==Train Epoch:\t\t\t 726 \tLoss: 0.012581\t lambda: 8.384e-05\t lr: 0.287\n",
            "--\t\t\tphi(X):\t 0.069 \tphi(Y): 0.209\n",
            "==Train Epoch:\t\t\t 729 \tLoss: 0.012513\t lambda: 8.436e-05\t lr: 0.295\n",
            "==Train Epoch:\t\t\t 732 \tLoss: 0.012445\t lambda: 8.487e-05\t lr: 0.277\n",
            "--\t\t\tphi(X):\t 0.068 \tphi(Y): 0.207\n",
            "==Train Epoch:\t\t\t 735 \tLoss: 0.012377\t lambda: 8.539e-05\t lr: 0.251\n",
            "==Train Epoch:\t\t\t 738 \tLoss: 0.012316\t lambda: 8.591e-05\t lr: 0.259\n",
            "--\t\t\tphi(X):\t 0.067 \tphi(Y): 0.205\n",
            "==Train Epoch:\t\t\t 741 \tLoss: 0.012269\t lambda: 8.626e-05\t lr: 0.299\n",
            "==Train Full Grad Epoch:\t 744 \tLoss: 0.012264\t lambda: 8.645e-05\t lr: 0.258\n",
            "--\t\t\tphi(X):\t 0.067 \tphi(Y): 0.204\n",
            "==Train Epoch:\t\t\t 747 \tLoss: 0.012187\t lambda: 8.697e-05\t lr: 0.310\n",
            "\n",
            "Test set: Average loss: 0.0024\n",
            "\n",
            "==Train Epoch:\t\t\t 750 \tLoss: 0.012126\t lambda: 8.749e-05\t lr: 0.257\n",
            "--\t\t\tphi(X):\t 0.066 \tphi(Y): 0.203\n",
            "==Train Epoch:\t\t\t 753 \tLoss: 0.012069\t lambda: 8.801e-05\t lr: 0.318\n",
            "==Train Epoch:\t\t\t 756 \tLoss: 0.012013\t lambda: 8.853e-05\t lr: 0.274\n",
            "--\t\t\tphi(X):\t 0.065 \tphi(Y): 0.201\n",
            "==Train Epoch:\t\t\t 759 \tLoss: 0.011973\t lambda: 8.889e-05\t lr: 0.285\n",
            "==Train Full Grad Epoch:\t 762 \tLoss: 0.011985\t lambda: 8.891e-05\t lr: 0.288\n",
            "--\t\t\tphi(X):\t 0.065 \tphi(Y): 0.201\n",
            "==Train Epoch:\t\t\t 765 \tLoss: 0.011934\t lambda: 8.927e-05\t lr: 0.305\n",
            "==Train Epoch:\t\t\t 768 \tLoss: 0.011884\t lambda: 8.979e-05\t lr: 0.258\n",
            "--\t\t\tphi(X):\t 0.065 \tphi(Y): 0.199\n",
            "==Train Epoch:\t\t\t 771 \tLoss: 0.011829\t lambda: 9.031e-05\t lr: 0.286\n",
            "==Train Epoch:\t\t\t 774 \tLoss: 0.011794\t lambda: 9.067e-05\t lr: 0.282\n",
            "--\t\t\tphi(X):\t 0.064 \tphi(Y): 0.198\n",
            "==Train Full Grad Epoch:\t 777 \tLoss: 0.011791\t lambda: 9.086e-05\t lr: 0.294\n",
            "==Train Full Grad Epoch:\t 780 \tLoss: 0.011759\t lambda: 9.121e-05\t lr: 0.307\n",
            "--\t\t\tphi(X):\t 0.063 \tphi(Y): 0.197\n",
            "==Train Epoch:\t\t\t 783 \tLoss: 0.011693\t lambda: 9.174e-05\t lr: 0.292\n",
            "==Train Epoch:\t\t\t 786 \tLoss: 0.011644\t lambda: 9.226e-05\t lr: 0.283\n",
            "--\t\t\tphi(X):\t 0.062 \tphi(Y): 0.195\n",
            "==Train Epoch:\t\t\t 789 \tLoss: 0.011613\t lambda: 9.262e-05\t lr: 0.292\n",
            "==Train Epoch:\t\t\t 792 \tLoss: 0.011587\t lambda: 9.297e-05\t lr: 0.285\n",
            "--\t\t\tphi(X):\t 0.062 \tphi(Y): 0.194\n",
            "==Train Full Grad Epoch:\t 795 \tLoss: 0.011598\t lambda: 9.300e-05\t lr: 0.266\n",
            "==Train Epoch:\t\t\t 798 \tLoss: 0.011541\t lambda: 9.352e-05\t lr: 0.307\n",
            "--\t\t\tphi(X):\t 0.061 \tphi(Y): 0.193\n",
            "\n",
            "Test set: Average loss: 0.0017\n",
            "\n",
            "==Train Full Grad Epoch:\t 801 \tLoss: 0.011542\t lambda: 9.371e-05\t lr: 0.288\n",
            "==Train Epoch:\t\t\t 804 \tLoss: 0.011496\t lambda: 9.407e-05\t lr: 0.311\n",
            "--\t\t\tphi(X):\t 0.061 \tphi(Y): 0.192\n",
            "==Train Full Grad Epoch:\t 807 \tLoss: 0.011484\t lambda: 9.443e-05\t lr: 0.266\n",
            "==Train Epoch:\t\t\t 810 \tLoss: 0.011451\t lambda: 9.462e-05\t lr: 0.275\n",
            "--\t\t\tphi(X):\t 0.060 \tphi(Y): 0.191\n",
            "==Train Epoch:\t\t\t 813 \tLoss: 0.011413\t lambda: 9.515e-05\t lr: 0.293\n",
            "==Train Epoch:\t\t\t 816 \tLoss: 0.011388\t lambda: 9.551e-05\t lr: 0.279\n",
            "--\t\t\tphi(X):\t 0.060 \tphi(Y): 0.190\n",
            "==Train Epoch:\t\t\t 819 \tLoss: 0.011362\t lambda: 9.587e-05\t lr: 0.312\n",
            "==Train Epoch:\t\t\t 822 \tLoss: 0.011350\t lambda: 9.606e-05\t lr: 0.277\n",
            "--\t\t\tphi(X):\t 0.059 \tphi(Y): 0.189\n",
            "==Train Epoch:\t\t\t 825 \tLoss: 0.011324\t lambda: 9.642e-05\t lr: 0.290\n",
            "==Train Epoch:\t\t\t 828 \tLoss: 0.011290\t lambda: 9.694e-05\t lr: 0.266\n",
            "--\t\t\tphi(X):\t 0.058 \tphi(Y): 0.187\n",
            "==Train Epoch:\t\t\t 831 \tLoss: 0.011267\t lambda: 9.730e-05\t lr: 0.286\n",
            "==Train Epoch:\t\t\t 834 \tLoss: 0.011244\t lambda: 9.766e-05\t lr: 0.286\n",
            "--\t\t\tphi(X):\t 0.058 \tphi(Y): 0.186\n",
            "==Train Epoch:\t\t\t 837 \tLoss: 0.011223\t lambda: 9.802e-05\t lr: 0.283\n",
            "==Train Full Grad Epoch:\t 840 \tLoss: 0.011227\t lambda: 9.822e-05\t lr: 0.298\n",
            "--\t\t\tphi(X):\t 0.057 \tphi(Y): 0.185\n",
            "==Train Full Grad Epoch:\t 843 \tLoss: 0.011226\t lambda: 9.824e-05\t lr: 0.290\n",
            "==Train Epoch:\t\t\t 846 \tLoss: 0.011173\t lambda: 9.877e-05\t lr: 0.309\n",
            "--\t\t\tphi(X):\t 0.057 \tphi(Y): 0.184\n",
            "==Train Epoch:\t\t\t 849 \tLoss: 0.011144\t lambda: 9.930e-05\t lr: 0.326\n",
            "\n",
            "Test set: Average loss: 0.0014\n",
            "\n",
            "==Train Full Grad Epoch:\t 852 \tLoss: 0.011144\t lambda: 9.966e-05\t lr: 0.289\n",
            "--\t\t\tphi(X):\t 0.056 \tphi(Y): 0.183\n",
            "==Train Epoch:\t\t\t 855 \tLoss: 0.011096\t lambda: 1.002e-04\t lr: 0.300\n",
            "==Train Epoch:\t\t\t 858 \tLoss: 0.011074\t lambda: 1.005e-04\t lr: 0.270\n",
            "--\t\t\tphi(X):\t 0.056 \tphi(Y): 0.181\n",
            "==Train Full Grad Epoch:\t 861 \tLoss: 0.011072\t lambda: 1.009e-04\t lr: 0.264\n",
            "==Train Epoch:\t\t\t 864 \tLoss: 0.011024\t lambda: 1.014e-04\t lr: 0.313\n",
            "--\t\t\tphi(X):\t 0.055 \tphi(Y): 0.180\n",
            "==Train Epoch:\t\t\t 867 \tLoss: 0.010996\t lambda: 1.020e-04\t lr: 0.298\n",
            "==Train Full Grad Epoch:\t 870 \tLoss: 0.011003\t lambda: 1.022e-04\t lr: 0.306\n",
            "--\t\t\tphi(X):\t 0.055 \tphi(Y): 0.179\n",
            "==Train Epoch:\t\t\t 873 \tLoss: 0.010959\t lambda: 1.027e-04\t lr: 0.330\n",
            "==Train Epoch:\t\t\t 876 \tLoss: 0.010935\t lambda: 1.031e-04\t lr: 0.288\n",
            "--\t\t\tphi(X):\t 0.054 \tphi(Y): 0.177\n",
            "==Train Epoch:\t\t\t 879 \tLoss: 0.010914\t lambda: 1.036e-04\t lr: 0.296\n",
            "==Train Full Grad Epoch:\t 882 \tLoss: 0.010916\t lambda: 1.040e-04\t lr: 0.291\n",
            "--\t\t\tphi(X):\t 0.053 \tphi(Y): 0.176\n",
            "==Train Full Grad Epoch:\t 885 \tLoss: 0.010906\t lambda: 1.041e-04\t lr: 0.288\n",
            "==Train Epoch:\t\t\t 888 \tLoss: 0.010864\t lambda: 1.047e-04\t lr: 0.295\n",
            "--\t\t\tphi(X):\t 0.053 \tphi(Y): 0.175\n",
            "==Train Epoch:\t\t\t 891 \tLoss: 0.010839\t lambda: 1.052e-04\t lr: 0.308\n",
            "==Train Epoch:\t\t\t 894 \tLoss: 0.010827\t lambda: 1.056e-04\t lr: 0.304\n",
            "--\t\t\tphi(X):\t 0.052 \tphi(Y): 0.173\n",
            "==Train Epoch:\t\t\t 897 \tLoss: 0.010804\t lambda: 1.061e-04\t lr: 0.286\n",
            "\n",
            "Test set: Average loss: 0.0010\n",
            "\n",
            "==Train Epoch:\t\t\t 900 \tLoss: 0.010788\t lambda: 1.065e-04\t lr: 0.281\n",
            "--\t\t\tphi(X):\t 0.052 \tphi(Y): 0.171\n",
            "==Train Full Grad Epoch:\t 903 \tLoss: 0.010801\t lambda: 1.067e-04\t lr: 0.309\n",
            "==Train Full Grad Epoch:\t 906 \tLoss: 0.010784\t lambda: 1.070e-04\t lr: 0.298\n",
            "--\t\t\tphi(X):\t 0.051 \tphi(Y): 0.171\n",
            "==Train Epoch:\t\t\t 909 \tLoss: 0.010742\t lambda: 1.076e-04\t lr: 0.270\n",
            "==Train Epoch:\t\t\t 912 \tLoss: 0.010725\t lambda: 1.079e-04\t lr: 0.287\n",
            "--\t\t\tphi(X):\t 0.050 \tphi(Y): 0.169\n",
            "==Train Epoch:\t\t\t 915 \tLoss: 0.010706\t lambda: 1.085e-04\t lr: 0.282\n",
            "==Train Full Grad Epoch:\t 918 \tLoss: 0.010724\t lambda: 1.085e-04\t lr: 0.277\n",
            "--\t\t\tphi(X):\t 0.050 \tphi(Y): 0.168\n",
            "==Train Epoch:\t\t\t 921 \tLoss: 0.010686\t lambda: 1.090e-04\t lr: 0.293\n",
            "==Train Epoch:\t\t\t 924 \tLoss: 0.010663\t lambda: 1.096e-04\t lr: 0.268\n",
            "--\t\t\tphi(X):\t 0.049 \tphi(Y): 0.166\n",
            "==Train Epoch:\t\t\t 927 \tLoss: 0.010640\t lambda: 1.101e-04\t lr: 0.284\n",
            "==Train Epoch:\t\t\t 930 \tLoss: 0.010617\t lambda: 1.106e-04\t lr: 0.306\n",
            "--\t\t\tphi(X):\t 0.048 \tphi(Y): 0.165\n",
            "==Train Full Grad Epoch:\t 933 \tLoss: 0.010619\t lambda: 1.110e-04\t lr: 0.284\n",
            "==Train Epoch:\t\t\t 936 \tLoss: 0.010584\t lambda: 1.114e-04\t lr: 0.268\n",
            "--\t\t\tphi(X):\t 0.047 \tphi(Y): 0.164\n",
            "==Train Epoch:\t\t\t 939 \tLoss: 0.010563\t lambda: 1.119e-04\t lr: 0.311\n",
            "==Train Epoch:\t\t\t 942 \tLoss: 0.010548\t lambda: 1.123e-04\t lr: 0.309\n",
            "--\t\t\tphi(X):\t 0.046 \tphi(Y): 0.163\n",
            "==Train Full Grad Epoch:\t 945 \tLoss: 0.010565\t lambda: 1.123e-04\t lr: 0.262\n",
            "==Train Epoch:\t\t\t 948 \tLoss: 0.010531\t lambda: 1.127e-04\t lr: 0.284\n",
            "--\t\t\tphi(X):\t 0.046 \tphi(Y): 0.162\n",
            "\n",
            "Test set: Average loss: 0.0007\n",
            "\n",
            "==Train Epoch:\t\t\t 951 \tLoss: 0.010518\t lambda: 1.130e-04\t lr: 0.294\n",
            "==Train Epoch:\t\t\t 954 \tLoss: 0.010495\t lambda: 1.136e-04\t lr: 0.287\n",
            "--\t\t\tphi(X):\t 0.045 \tphi(Y): 0.161\n",
            "==Train Epoch:\t\t\t 957 \tLoss: 0.010482\t lambda: 1.139e-04\t lr: 0.277\n",
            "==Train Epoch:\t\t\t 960 \tLoss: 0.010461\t lambda: 1.145e-04\t lr: 0.280\n",
            "--\t\t\tphi(X):\t 0.044 \tphi(Y): 0.160\n",
            "==Train Epoch:\t\t\t 963 \tLoss: 0.010450\t lambda: 1.148e-04\t lr: 0.297\n",
            "==Train Full Grad Epoch:\t 966 \tLoss: 0.010461\t lambda: 1.150e-04\t lr: 0.297\n",
            "--\t\t\tphi(X):\t 0.043 \tphi(Y): 0.159\n",
            "==Train Epoch:\t\t\t 969 \tLoss: 0.010424\t lambda: 1.156e-04\t lr: 0.303\n",
            "==Train Epoch:\t\t\t 972 \tLoss: 0.010406\t lambda: 1.161e-04\t lr: 0.264\n",
            "--\t\t\tphi(X):\t 0.042 \tphi(Y): 0.158\n",
            "==Train Full Grad Epoch:\t 975 \tLoss: 0.010413\t lambda: 1.165e-04\t lr: 0.257\n",
            "==Train Full Grad Epoch:\t 978 \tLoss: 0.010408\t lambda: 1.167e-04\t lr: 0.295\n",
            "--\t\t\tphi(X):\t 0.041 \tphi(Y): 0.157\n",
            "==Train Epoch:\t\t\t 981 \tLoss: 0.010371\t lambda: 1.172e-04\t lr: 0.277\n",
            "==Train Full Grad Epoch:\t 984 \tLoss: 0.010381\t lambda: 1.176e-04\t lr: 0.276\n",
            "--\t\t\tphi(X):\t 0.040 \tphi(Y): 0.156\n",
            "==Train Epoch:\t\t\t 987 \tLoss: 0.010346\t lambda: 1.181e-04\t lr: 0.305\n",
            "==Train Epoch:\t\t\t 990 \tLoss: 0.010332\t lambda: 1.187e-04\t lr: 0.267\n",
            "--\t\t\tphi(X):\t 0.039 \tphi(Y): 0.155\n",
            "==Train Full Grad Epoch:\t 993 \tLoss: 0.010350\t lambda: 1.187e-04\t lr: 0.309\n",
            "==Train Epoch:\t\t\t 996 \tLoss: 0.010316\t lambda: 1.192e-04\t lr: 0.323\n",
            "--\t\t\tphi(X):\t 0.039 \tphi(Y): 0.154\n",
            "==Train Epoch:\t\t\t 999 \tLoss: 0.010304\t lambda: 1.196e-04\t lr: 0.276\n",
            "\n",
            "Test set: Average loss: 0.0005\n",
            "\n",
            "==Train Epoch:\t\t\t 1002 \tLoss: 0.010296\t lambda: 1.200e-04\t lr: 0.305\n",
            "--\t\t\tphi(X):\t 0.038 \tphi(Y): 0.153\n",
            "==Train Epoch:\t\t\t 1005 \tLoss: 0.010286\t lambda: 1.204e-04\t lr: 0.324\n",
            "==Train Epoch:\t\t\t 1008 \tLoss: 0.010270\t lambda: 1.209e-04\t lr: 0.314\n",
            "--\t\t\tphi(X):\t 0.037 \tphi(Y): 0.152\n",
            "==Train Epoch:\t\t\t 1011 \tLoss: 0.010263\t lambda: 1.213e-04\t lr: 0.285\n",
            "==Train Epoch:\t\t\t 1014 \tLoss: 0.010250\t lambda: 1.218e-04\t lr: 0.290\n",
            "--\t\t\tphi(X):\t 0.036 \tphi(Y): 0.151\n",
            "==Train Epoch:\t\t\t 1017 \tLoss: 0.010237\t lambda: 1.224e-04\t lr: 0.301\n",
            "==Train Epoch:\t\t\t 1020 \tLoss: 0.010226\t lambda: 1.229e-04\t lr: 0.305\n",
            "--\t\t\tphi(X):\t 0.035 \tphi(Y): 0.150\n",
            "==Train Epoch:\t\t\t 1023 \tLoss: 0.010216\t lambda: 1.233e-04\t lr: 0.290\n",
            "==Train Epoch:\t\t\t 1026 \tLoss: 0.010213\t lambda: 1.235e-04\t lr: 0.318\n",
            "--\t\t\tphi(X):\t 0.034 \tphi(Y): 0.149\n",
            "==Train Epoch:\t\t\t 1029 \tLoss: 0.010204\t lambda: 1.240e-04\t lr: 0.302\n",
            "==Train Full Grad Epoch:\t 1032 \tLoss: 0.010217\t lambda: 1.242e-04\t lr: 0.314\n",
            "--\t\t\tphi(X):\t 0.034 \tphi(Y): 0.148\n",
            "==Train Epoch:\t\t\t 1035 \tLoss: 0.010194\t lambda: 1.244e-04\t lr: 0.299\n",
            "==Train Epoch:\t\t\t 1038 \tLoss: 0.010186\t lambda: 1.248e-04\t lr: 0.304\n",
            "--\t\t\tphi(X):\t 0.033 \tphi(Y): 0.148\n",
            "==Train Epoch:\t\t\t 1041 \tLoss: 0.010179\t lambda: 1.252e-04\t lr: 0.263\n",
            "==Train Epoch:\t\t\t 1044 \tLoss: 0.010171\t lambda: 1.257e-04\t lr: 0.305\n",
            "--\t\t\tphi(X):\t 0.032 \tphi(Y): 0.147\n",
            "==Train Epoch:\t\t\t 1047 \tLoss: 0.010161\t lambda: 1.263e-04\t lr: 0.286\n",
            "\n",
            "Test set: Average loss: 0.0003\n",
            "\n",
            "==Train Epoch:\t\t\t 1050 \tLoss: 0.010151\t lambda: 1.268e-04\t lr: 0.283\n",
            "--\t\t\tphi(X):\t 0.031 \tphi(Y): 0.146\n",
            "==Train Epoch:\t\t\t 1053 \tLoss: 0.010144\t lambda: 1.272e-04\t lr: 0.266\n",
            "==Train Epoch:\t\t\t 1056 \tLoss: 0.010136\t lambda: 1.277e-04\t lr: 0.342\n",
            "--\t\t\tphi(X):\t 0.030 \tphi(Y): 0.145\n",
            "==Train Epoch:\t\t\t 1059 \tLoss: 0.010129\t lambda: 1.283e-04\t lr: 0.289\n",
            "==Train Full Grad Epoch:\t 1062 \tLoss: 0.010144\t lambda: 1.285e-04\t lr: 0.310\n",
            "--\t\t\tphi(X):\t 0.029 \tphi(Y): 0.144\n",
            "==Train Epoch:\t\t\t 1065 \tLoss: 0.010118\t lambda: 1.290e-04\t lr: 0.290\n",
            "==Train Epoch:\t\t\t 1068 \tLoss: 0.010113\t lambda: 1.294e-04\t lr: 0.272\n",
            "--\t\t\tphi(X):\t 0.028 \tphi(Y): 0.143\n",
            "==Train Full Grad Epoch:\t 1071 \tLoss: 0.010126\t lambda: 1.296e-04\t lr: 0.292\n",
            "==Train Epoch:\t\t\t 1074 \tLoss: 0.010101\t lambda: 1.302e-04\t lr: 0.305\n",
            "--\t\t\tphi(X):\t 0.028 \tphi(Y): 0.142\n",
            "==Train Epoch:\t\t\t 1077 \tLoss: 0.010094\t lambda: 1.305e-04\t lr: 0.296\n",
            "==Train Epoch:\t\t\t 1080 \tLoss: 0.010092\t lambda: 1.309e-04\t lr: 0.281\n",
            "--\t\t\tphi(X):\t 0.027 \tphi(Y): 0.141\n",
            "==Train Epoch:\t\t\t 1083 \tLoss: 0.010085\t lambda: 1.313e-04\t lr: 0.299\n",
            "==Train Epoch:\t\t\t 1086 \tLoss: 0.010086\t lambda: 1.315e-04\t lr: 0.280\n",
            "--\t\t\tphi(X):\t 0.027 \tphi(Y): 0.140\n",
            "==Train Epoch:\t\t\t 1089 \tLoss: 0.010078\t lambda: 1.320e-04\t lr: 0.320\n",
            "==Train Epoch:\t\t\t 1092 \tLoss: 0.010073\t lambda: 1.324e-04\t lr: 0.317\n",
            "--\t\t\tphi(X):\t 0.026 \tphi(Y): 0.139\n",
            "==Train Epoch:\t\t\t 1095 \tLoss: 0.010072\t lambda: 1.328e-04\t lr: 0.266\n",
            "==Train Epoch:\t\t\t 1098 \tLoss: 0.010067\t lambda: 1.330e-04\t lr: 0.288\n",
            "--\t\t\tphi(X):\t 0.025 \tphi(Y): 0.139\n",
            "\n",
            "Test set: Average loss: 0.0002\n",
            "\n",
            "==Train Epoch:\t\t\t 1101 \tLoss: 0.010063\t lambda: 1.334e-04\t lr: 0.289\n",
            "==Train Full Grad Epoch:\t 1104 \tLoss: 0.010078\t lambda: 1.336e-04\t lr: 0.294\n",
            "--\t\t\tphi(X):\t 0.025 \tphi(Y): 0.138\n",
            "==Train Epoch:\t\t\t 1107 \tLoss: 0.010057\t lambda: 1.339e-04\t lr: 0.265\n",
            "==Train Full Grad Epoch:\t 1110 \tLoss: 0.010071\t lambda: 1.343e-04\t lr: 0.284\n",
            "--\t\t\tphi(X):\t 0.024 \tphi(Y): 0.137\n",
            "==Train Epoch:\t\t\t 1113 \tLoss: 0.010049\t lambda: 1.347e-04\t lr: 0.276\n",
            "==Train Epoch:\t\t\t 1116 \tLoss: 0.010046\t lambda: 1.351e-04\t lr: 0.302\n",
            "--\t\t\tphi(X):\t 0.023 \tphi(Y): 0.136\n",
            "==Train Epoch:\t\t\t 1119 \tLoss: 0.010042\t lambda: 1.355e-04\t lr: 0.292\n",
            "==Train Full Grad Epoch:\t 1122 \tLoss: 0.010056\t lambda: 1.358e-04\t lr: 0.290\n",
            "--\t\t\tphi(X):\t 0.023 \tphi(Y): 0.135\n",
            "==Train Epoch:\t\t\t 1125 \tLoss: 0.010036\t lambda: 1.362e-04\t lr: 0.299\n",
            "==Train Epoch:\t\t\t 1128 \tLoss: 0.010029\t lambda: 1.368e-04\t lr: 0.281\n",
            "--\t\t\tphi(X):\t 0.022 \tphi(Y): 0.134\n",
            "==Train Full Grad Epoch:\t 1131 \tLoss: 0.010046\t lambda: 1.371e-04\t lr: 0.269\n",
            "==Train Full Grad Epoch:\t 1134 \tLoss: 0.010042\t lambda: 1.373e-04\t lr: 0.277\n",
            "--\t\t\tphi(X):\t 0.021 \tphi(Y): 0.134\n",
            "==Train Epoch:\t\t\t 1137 \tLoss: 0.010020\t lambda: 1.379e-04\t lr: 0.309\n",
            "==Train Epoch:\t\t\t 1140 \tLoss: 0.010020\t lambda: 1.383e-04\t lr: 0.285\n",
            "--\t\t\tphi(X):\t 0.021 \tphi(Y): 0.132\n",
            "==Train Epoch:\t\t\t 1143 \tLoss: 0.010014\t lambda: 1.388e-04\t lr: 0.276\n",
            "==Train Full Grad Epoch:\t 1146 \tLoss: 0.010029\t lambda: 1.392e-04\t lr: 0.285\n",
            "--\t\t\tphi(X):\t 0.020 \tphi(Y): 0.131\n",
            "==Train Epoch:\t\t\t 1149 \tLoss: 0.010006\t lambda: 1.398e-04\t lr: 0.285\n",
            "\n",
            "Test set: Average loss: 0.0002\n",
            "\n",
            "==Train Full Grad Epoch:\t 1152 \tLoss: 0.010026\t lambda: 1.398e-04\t lr: 0.284\n",
            "--\t\t\tphi(X):\t 0.019 \tphi(Y): 0.131\n",
            "==Train Full Grad Epoch:\t 1155 \tLoss: 0.010027\t lambda: 1.398e-04\t lr: 0.268\n",
            "==Train Full Grad Epoch:\t 1158 \tLoss: 0.010023\t lambda: 1.402e-04\t lr: 0.289\n",
            "--\t\t\tphi(X):\t 0.019 \tphi(Y): 0.130\n",
            "==Train Epoch:\t\t\t 1161 \tLoss: 0.010003\t lambda: 1.404e-04\t lr: 0.292\n",
            "==Train Epoch:\t\t\t 1164 \tLoss: 0.010000\t lambda: 1.410e-04\t lr: 0.280\n",
            "--\t\t\tphi(X):\t 0.018 \tphi(Y): 0.129\n",
            "==Train Epoch:\t\t\t 1167 \tLoss: 0.009994\t lambda: 1.415e-04\t lr: 0.302\n",
            "==Train Full Grad Epoch:\t 1170 \tLoss: 0.010014\t lambda: 1.415e-04\t lr: 0.292\n",
            "--\t\t\tphi(X):\t 0.018 \tphi(Y): 0.128\n",
            "==Train Full Grad Epoch:\t 1173 \tLoss: 0.010014\t lambda: 1.417e-04\t lr: 0.311\n",
            "==Train Epoch:\t\t\t 1176 \tLoss: 0.009994\t lambda: 1.419e-04\t lr: 0.301\n",
            "--\t\t\tphi(X):\t 0.018 \tphi(Y): 0.128\n",
            "==Train Epoch:\t\t\t 1179 \tLoss: 0.009991\t lambda: 1.423e-04\t lr: 0.270\n",
            "==Train Epoch:\t\t\t 1182 \tLoss: 0.009989\t lambda: 1.427e-04\t lr: 0.295\n",
            "--\t\t\tphi(X):\t 0.017 \tphi(Y): 0.127\n",
            "==Train Epoch:\t\t\t 1185 \tLoss: 0.009988\t lambda: 1.431e-04\t lr: 0.274\n",
            "==Train Full Grad Epoch:\t 1188 \tLoss: 0.010005\t lambda: 1.435e-04\t lr: 0.269\n",
            "--\t\t\tphi(X):\t 0.017 \tphi(Y): 0.126\n",
            "==Train Epoch:\t\t\t 1191 \tLoss: 0.009984\t lambda: 1.440e-04\t lr: 0.285\n",
            "==Train Full Grad Epoch:\t 1194 \tLoss: 0.010000\t lambda: 1.442e-04\t lr: 0.304\n",
            "--\t\t\tphi(X):\t 0.016 \tphi(Y): 0.125\n",
            "==Train Epoch:\t\t\t 1197 \tLoss: 0.009981\t lambda: 1.446e-04\t lr: 0.290\n",
            "\n",
            "Test set: Average loss: 0.0001\n",
            "\n",
            "==Train Epoch:\t\t\t 1200 \tLoss: 0.009976\t lambda: 1.452e-04\t lr: 0.304\n",
            "--\t\t\tphi(X):\t 0.016 \tphi(Y): 0.124\n",
            "==Train Full Grad Epoch:\t 1203 \tLoss: 0.009997\t lambda: 1.454e-04\t lr: 0.272\n",
            "==Train Epoch:\t\t\t 1206 \tLoss: 0.009978\t lambda: 1.456e-04\t lr: 0.290\n",
            "--\t\t\tphi(X):\t 0.015 \tphi(Y): 0.123\n",
            "==Train Epoch:\t\t\t 1209 \tLoss: 0.009975\t lambda: 1.461e-04\t lr: 0.296\n",
            "==Train Full Grad Epoch:\t 1212 \tLoss: 0.009993\t lambda: 1.465e-04\t lr: 0.263\n",
            "--\t\t\tphi(X):\t 0.015 \tphi(Y): 0.122\n",
            "==Train Epoch:\t\t\t 1215 \tLoss: 0.009972\t lambda: 1.469e-04\t lr: 0.308\n",
            "==Train Epoch:\t\t\t 1218 \tLoss: 0.009972\t lambda: 1.475e-04\t lr: 0.282\n",
            "--\t\t\tphi(X):\t 0.014 \tphi(Y): 0.121\n",
            "==Train Epoch:\t\t\t 1221 \tLoss: 0.009971\t lambda: 1.480e-04\t lr: 0.257\n",
            "==Train Full Grad Epoch:\t 1224 \tLoss: 0.009988\t lambda: 1.484e-04\t lr: 0.261\n",
            "--\t\t\tphi(X):\t 0.013 \tphi(Y): 0.120\n",
            "==Train Full Grad Epoch:\t 1227 \tLoss: 0.009988\t lambda: 1.488e-04\t lr: 0.315\n",
            "==Train Epoch:\t\t\t 1230 \tLoss: 0.009969\t lambda: 1.493e-04\t lr: 0.278\n",
            "--\t\t\tphi(X):\t 0.013 \tphi(Y): 0.119\n",
            "==Train Epoch:\t\t\t 1233 \tLoss: 0.009967\t lambda: 1.497e-04\t lr: 0.305\n",
            "==Train Epoch:\t\t\t 1236 \tLoss: 0.009966\t lambda: 1.501e-04\t lr: 0.304\n",
            "--\t\t\tphi(X):\t 0.012 \tphi(Y): 0.118\n",
            "==Train Epoch:\t\t\t 1239 \tLoss: 0.009963\t lambda: 1.507e-04\t lr: 0.276\n",
            "==Train Epoch:\t\t\t 1242 \tLoss: 0.009965\t lambda: 1.512e-04\t lr: 0.262\n",
            "--\t\t\tphi(X):\t 0.012 \tphi(Y): 0.116\n",
            "==Train Full Grad Epoch:\t 1245 \tLoss: 0.009983\t lambda: 1.514e-04\t lr: 0.300\n",
            "==Train Full Grad Epoch:\t 1248 \tLoss: 0.009983\t lambda: 1.518e-04\t lr: 0.288\n",
            "--\t\t\tphi(X):\t 0.011 \tphi(Y): 0.115\n",
            "\n",
            "Test set: Average loss: 0.0001\n",
            "\n",
            "==Train Full Grad Epoch:\t 1251 \tLoss: 0.009983\t lambda: 1.522e-04\t lr: 0.313\n",
            "==Train Epoch:\t\t\t 1254 \tLoss: 0.009963\t lambda: 1.526e-04\t lr: 0.272\n",
            "--\t\t\tphi(X):\t 0.011 \tphi(Y): 0.115\n",
            "==Train Epoch:\t\t\t 1257 \tLoss: 0.009962\t lambda: 1.532e-04\t lr: 0.277\n",
            "==Train Full Grad Epoch:\t 1260 \tLoss: 0.009980\t lambda: 1.535e-04\t lr: 0.262\n",
            "--\t\t\tphi(X):\t 0.010 \tphi(Y): 0.113\n",
            "==Train Epoch:\t\t\t 1263 \tLoss: 0.009961\t lambda: 1.539e-04\t lr: 0.309\n",
            "==Train Epoch:\t\t\t 1266 \tLoss: 0.009961\t lambda: 1.545e-04\t lr: 0.308\n",
            "--\t\t\tphi(X):\t 0.010 \tphi(Y): 0.112\n",
            "==Train Full Grad Epoch:\t 1269 \tLoss: 0.009980\t lambda: 1.547e-04\t lr: 0.276\n",
            "==Train Epoch:\t\t\t 1272 \tLoss: 0.009962\t lambda: 1.551e-04\t lr: 0.273\n",
            "--\t\t\tphi(X):\t 0.009 \tphi(Y): 0.111\n",
            "==Train Epoch:\t\t\t 1275 \tLoss: 0.009960\t lambda: 1.555e-04\t lr: 0.305\n",
            "==Train Epoch:\t\t\t 1278 \tLoss: 0.009959\t lambda: 1.560e-04\t lr: 0.312\n",
            "--\t\t\tphi(X):\t 0.009 \tphi(Y): 0.110\n",
            "==Train Epoch:\t\t\t 1281 \tLoss: 0.009961\t lambda: 1.562e-04\t lr: 0.318\n",
            "==Train Epoch:\t\t\t 1284 \tLoss: 0.009960\t lambda: 1.566e-04\t lr: 0.302\n",
            "--\t\t\tphi(X):\t 0.008 \tphi(Y): 0.109\n",
            "==Train Full Grad Epoch:\t 1287 \tLoss: 0.009977\t lambda: 1.568e-04\t lr: 0.272\n",
            "==Train Epoch:\t\t\t 1290 \tLoss: 0.009960\t lambda: 1.574e-04\t lr: 0.292\n",
            "--\t\t\tphi(X):\t 0.008 \tphi(Y): 0.109\n",
            "==Train Epoch:\t\t\t 1293 \tLoss: 0.009958\t lambda: 1.578e-04\t lr: 0.298\n",
            "==Train Full Grad Epoch:\t 1296 \tLoss: 0.009977\t lambda: 1.580e-04\t lr: 0.283\n",
            "--\t\t\tphi(X):\t 0.007 \tphi(Y): 0.108\n",
            "==Train Epoch:\t\t\t 1299 \tLoss: 0.009957\t lambda: 1.586e-04\t lr: 0.273\n",
            "\n",
            "Test set: Average loss: 0.0001\n",
            "\n",
            "==Train Epoch:\t\t\t 1302 \tLoss: 0.009954\t lambda: 1.589e-04\t lr: 0.294\n",
            "--\t\t\tphi(X):\t 0.007 \tphi(Y): 0.107\n",
            "==Train Epoch:\t\t\t 1305 \tLoss: 0.009952\t lambda: 1.595e-04\t lr: 0.291\n",
            "==Train Epoch:\t\t\t 1308 \tLoss: 0.009948\t lambda: 1.601e-04\t lr: 0.293\n",
            "--\t\t\tphi(X):\t 0.006 \tphi(Y): 0.106\n",
            "==Train Epoch:\t\t\t 1311 \tLoss: 0.009947\t lambda: 1.605e-04\t lr: 0.287\n",
            "==Train Epoch:\t\t\t 1314 \tLoss: 0.009943\t lambda: 1.610e-04\t lr: 0.304\n",
            "--\t\t\tphi(X):\t 0.005 \tphi(Y): 0.105\n",
            "==Train Epoch:\t\t\t 1317 \tLoss: 0.009942\t lambda: 1.616e-04\t lr: 0.272\n",
            "==Train Full Grad Epoch:\t 1320 \tLoss: 0.009956\t lambda: 1.620e-04\t lr: 0.276\n",
            "--\t\t\tphi(X):\t 0.005 \tphi(Y): 0.104\n",
            "==Train Epoch:\t\t\t 1323 \tLoss: 0.009939\t lambda: 1.624e-04\t lr: 0.248\n",
            "==Train Epoch:\t\t\t 1326 \tLoss: 0.009938\t lambda: 1.628e-04\t lr: 0.277\n",
            "--\t\t\tphi(X):\t 0.004 \tphi(Y): 0.104\n",
            "==Train Full Grad Epoch:\t 1329 \tLoss: 0.009957\t lambda: 1.628e-04\t lr: 0.287\n",
            "==Train Full Grad Epoch:\t 1332 \tLoss: 0.009955\t lambda: 1.632e-04\t lr: 0.282\n",
            "--\t\t\tphi(X):\t 0.004 \tphi(Y): 0.103\n",
            "==Train Epoch:\t\t\t 1335 \tLoss: 0.009934\t lambda: 1.637e-04\t lr: 0.295\n",
            "==Train Full Grad Epoch:\t 1338 \tLoss: 0.009953\t lambda: 1.639e-04\t lr: 0.301\n",
            "--\t\t\tphi(X):\t 0.003 \tphi(Y): 0.103\n",
            "==Train Full Grad Epoch:\t 1341 \tLoss: 0.009954\t lambda: 1.642e-04\t lr: 0.288\n",
            "==Train Full Grad Epoch:\t 1344 \tLoss: 0.009955\t lambda: 1.642e-04\t lr: 0.292\n",
            "--\t\t\tphi(X):\t 0.003 \tphi(Y): 0.103\n",
            "==Train Full Grad Epoch:\t 1347 \tLoss: 0.009956\t lambda: 1.642e-04\t lr: 0.308\n",
            "\n",
            "Test set: Average loss: 0.0001\n",
            "\n",
            "==Train Full Grad Epoch:\t 1350 \tLoss: 0.009954\t lambda: 1.646e-04\t lr: 0.283\n",
            "--\t\t\tphi(X):\t 0.003 \tphi(Y): 0.103\n",
            "==Train Full Grad Epoch:\t 1353 \tLoss: 0.009952\t lambda: 1.650e-04\t lr: 0.312\n",
            "==Train Full Grad Epoch:\t 1356 \tLoss: 0.009954\t lambda: 1.654e-04\t lr: 0.300\n",
            "--\t\t\tphi(X):\t 0.003 \tphi(Y): 0.102\n",
            "==Train Full Grad Epoch:\t 1359 \tLoss: 0.009953\t lambda: 1.656e-04\t lr: 0.305\n",
            "==Train Epoch:\t\t\t 1362 \tLoss: 0.009935\t lambda: 1.662e-04\t lr: 0.312\n",
            "--\t\t\tphi(X):\t 0.002 \tphi(Y): 0.102\n",
            "==Train Epoch:\t\t\t 1365 \tLoss: 0.009935\t lambda: 1.664e-04\t lr: 0.306\n",
            "==Train Full Grad Epoch:\t 1368 \tLoss: 0.009954\t lambda: 1.668e-04\t lr: 0.276\n",
            "--\t\t\tphi(X):\t 0.002 \tphi(Y): 0.102\n",
            "==Train Epoch:\t\t\t 1371 \tLoss: 0.009936\t lambda: 1.673e-04\t lr: 0.295\n",
            "==Train Epoch:\t\t\t 1374 \tLoss: 0.009937\t lambda: 1.675e-04\t lr: 0.301\n",
            "--\t\t\tphi(X):\t 0.001 \tphi(Y): 0.102\n",
            "==Train Epoch:\t\t\t 1377 \tLoss: 0.009937\t lambda: 1.681e-04\t lr: 0.298\n",
            "==Train Epoch:\t\t\t 1380 \tLoss: 0.009938\t lambda: 1.687e-04\t lr: 0.270\n",
            "--\t\t\tphi(X):\t 0.001 \tphi(Y): 0.101\n",
            "==Train Epoch:\t\t\t 1383 \tLoss: 0.009939\t lambda: 1.691e-04\t lr: 0.295\n",
            "==Train Epoch:\t\t\t 1386 \tLoss: 0.009939\t lambda: 1.694e-04\t lr: 0.260\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.101\n",
            "==Train Epoch:\t\t\t 1389 \tLoss: 0.009939\t lambda: 1.700e-04\t lr: 0.295\n",
            "==Train Epoch:\t\t\t 1392 \tLoss: 0.009940\t lambda: 1.706e-04\t lr: 0.294\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.101\n",
            "==Train Epoch:\t\t\t 1395 \tLoss: 0.009940\t lambda: 1.712e-04\t lr: 0.312\n",
            "==Train Full Grad Epoch:\t 1398 \tLoss: 0.009960\t lambda: 1.714e-04\t lr: 0.268\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.101\n",
            "\n",
            "Test set: Average loss: 0.0000\n",
            "\n",
            "==Train Epoch:\t\t\t 1401 \tLoss: 0.009940\t lambda: 1.718e-04\t lr: 0.277\n",
            "==Train Epoch:\t\t\t 1404 \tLoss: 0.009942\t lambda: 1.721e-04\t lr: 0.311\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.101\n",
            "==Train Full Grad Epoch:\t 1407 \tLoss: 0.009960\t lambda: 1.725e-04\t lr: 0.281\n",
            "==Train Full Grad Epoch:\t 1410 \tLoss: 0.009961\t lambda: 1.729e-04\t lr: 0.289\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.101\n",
            "==Train Epoch:\t\t\t 1413 \tLoss: 0.009941\t lambda: 1.735e-04\t lr: 0.284\n",
            "==Train Full Grad Epoch:\t 1416 \tLoss: 0.009961\t lambda: 1.737e-04\t lr: 0.315\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.101\n",
            "==Train Epoch:\t\t\t 1419 \tLoss: 0.009940\t lambda: 1.743e-04\t lr: 0.291\n",
            "==Train Full Grad Epoch:\t 1422 \tLoss: 0.009959\t lambda: 1.747e-04\t lr: 0.275\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.101\n",
            "==Train Epoch:\t\t\t 1425 \tLoss: 0.009941\t lambda: 1.752e-04\t lr: 0.304\n",
            "==Train Epoch:\t\t\t 1428 \tLoss: 0.009943\t lambda: 1.758e-04\t lr: 0.312\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.101\n",
            "==Train Epoch:\t\t\t 1431 \tLoss: 0.009942\t lambda: 1.764e-04\t lr: 0.290\n",
            "==Train Epoch:\t\t\t 1434 \tLoss: 0.009942\t lambda: 1.768e-04\t lr: 0.273\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.100\n",
            "==Train Epoch:\t\t\t 1437 \tLoss: 0.009941\t lambda: 1.771e-04\t lr: 0.294\n",
            "==Train Epoch:\t\t\t 1440 \tLoss: 0.009941\t lambda: 1.777e-04\t lr: 0.278\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.100\n",
            "==Train Full Grad Epoch:\t 1443 \tLoss: 0.009962\t lambda: 1.779e-04\t lr: 0.310\n",
            "==Train Epoch:\t\t\t 1446 \tLoss: 0.009942\t lambda: 1.785e-04\t lr: 0.242\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.100\n",
            "==Train Epoch:\t\t\t 1449 \tLoss: 0.009941\t lambda: 1.791e-04\t lr: 0.315\n",
            "\n",
            "Test set: Average loss: 0.0000\n",
            "\n",
            "==Train Epoch:\t\t\t 1452 \tLoss: 0.009942\t lambda: 1.796e-04\t lr: 0.267\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.100\n",
            "==Train Full Grad Epoch:\t 1455 \tLoss: 0.009962\t lambda: 1.797e-04\t lr: 0.283\n",
            "==Train Epoch:\t\t\t 1458 \tLoss: 0.009942\t lambda: 1.799e-04\t lr: 0.267\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.100\n",
            "==Train Epoch:\t\t\t 1461 \tLoss: 0.009942\t lambda: 1.803e-04\t lr: 0.254\n",
            "==Train Epoch:\t\t\t 1464 \tLoss: 0.009943\t lambda: 1.808e-04\t lr: 0.308\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.100\n",
            "==Train Epoch:\t\t\t 1467 \tLoss: 0.009942\t lambda: 1.814e-04\t lr: 0.282\n",
            "==Train Epoch:\t\t\t 1470 \tLoss: 0.009944\t lambda: 1.816e-04\t lr: 0.309\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.100\n",
            "==Train Epoch:\t\t\t 1473 \tLoss: 0.009942\t lambda: 1.820e-04\t lr: 0.294\n",
            "==Train Epoch:\t\t\t 1476 \tLoss: 0.009942\t lambda: 1.826e-04\t lr: 0.311\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.100\n",
            "==Train Epoch:\t\t\t 1479 \tLoss: 0.009941\t lambda: 1.828e-04\t lr: 0.309\n",
            "==Train Full Grad Epoch:\t 1482 \tLoss: 0.009962\t lambda: 1.830e-04\t lr: 0.310\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.100\n",
            "==Train Full Grad Epoch:\t 1485 \tLoss: 0.009963\t lambda: 1.832e-04\t lr: 0.293\n",
            "==Train Full Grad Epoch:\t 1488 \tLoss: 0.009962\t lambda: 1.836e-04\t lr: 0.305\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.100\n",
            "==Train Full Grad Epoch:\t 1491 \tLoss: 0.009961\t lambda: 1.840e-04\t lr: 0.293\n",
            "==Train Epoch:\t\t\t 1494 \tLoss: 0.009942\t lambda: 1.844e-04\t lr: 0.335\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.100\n",
            "==Train Epoch:\t\t\t 1497 \tLoss: 0.009942\t lambda: 1.848e-04\t lr: 0.268\n",
            "\n",
            "Test set: Average loss: 0.0000\n",
            "\n",
            "==Train Epoch:\t\t\t 1500 \tLoss: 0.009944\t lambda: 1.851e-04\t lr: 0.314\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.100\n",
            "==Train Epoch:\t\t\t 1503 \tLoss: 0.009940\t lambda: 1.855e-04\t lr: 0.307\n",
            "==Train Epoch:\t\t\t 1506 \tLoss: 0.009941\t lambda: 1.861e-04\t lr: 0.305\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.100\n",
            "==Train Epoch:\t\t\t 1509 \tLoss: 0.009943\t lambda: 1.867e-04\t lr: 0.281\n",
            "==Train Full Grad Epoch:\t 1512 \tLoss: 0.009962\t lambda: 1.867e-04\t lr: 0.271\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.100\n",
            "==Train Epoch:\t\t\t 1515 \tLoss: 0.009942\t lambda: 1.873e-04\t lr: 0.298\n",
            "==Train Epoch:\t\t\t 1518 \tLoss: 0.009944\t lambda: 1.878e-04\t lr: 0.277\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.099\n",
            "==Train Epoch:\t\t\t 1521 \tLoss: 0.009943\t lambda: 1.882e-04\t lr: 0.298\n",
            "==Train Epoch:\t\t\t 1524 \tLoss: 0.009943\t lambda: 1.886e-04\t lr: 0.302\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.099\n",
            "==Train Full Grad Epoch:\t 1527 \tLoss: 0.009962\t lambda: 1.888e-04\t lr: 0.296\n",
            "==Train Epoch:\t\t\t 1530 \tLoss: 0.009944\t lambda: 1.892e-04\t lr: 0.279\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.099\n",
            "==Train Full Grad Epoch:\t 1533 \tLoss: 0.009962\t lambda: 1.896e-04\t lr: 0.273\n",
            "==Train Epoch:\t\t\t 1536 \tLoss: 0.009943\t lambda: 1.900e-04\t lr: 0.306\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.099\n",
            "==Train Epoch:\t\t\t 1539 \tLoss: 0.009942\t lambda: 1.906e-04\t lr: 0.312\n",
            "==Train Full Grad Epoch:\t 1542 \tLoss: 0.009962\t lambda: 1.910e-04\t lr: 0.265\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.099\n",
            "==Train Full Grad Epoch:\t 1545 \tLoss: 0.009962\t lambda: 1.912e-04\t lr: 0.277\n",
            "==Train Epoch:\t\t\t 1548 \tLoss: 0.009944\t lambda: 1.917e-04\t lr: 0.267\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.099\n",
            "\n",
            "Test set: Average loss: 0.0000\n",
            "\n",
            "==Train Full Grad Epoch:\t 1551 \tLoss: 0.009963\t lambda: 1.920e-04\t lr: 0.283\n",
            "==Train Epoch:\t\t\t 1554 \tLoss: 0.009943\t lambda: 1.925e-04\t lr: 0.282\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.099\n",
            "==Train Epoch:\t\t\t 1557 \tLoss: 0.009943\t lambda: 1.931e-04\t lr: 0.311\n",
            "==Train Full Grad Epoch:\t 1560 \tLoss: 0.009964\t lambda: 1.931e-04\t lr: 0.271\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.099\n",
            "==Train Full Grad Epoch:\t 1563 \tLoss: 0.009962\t lambda: 1.935e-04\t lr: 0.300\n",
            "==Train Epoch:\t\t\t 1566 \tLoss: 0.009944\t lambda: 1.941e-04\t lr: 0.300\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.099\n",
            "==Train Full Grad Epoch:\t 1569 \tLoss: 0.009962\t lambda: 1.945e-04\t lr: 0.292\n",
            "==Train Full Grad Epoch:\t 1572 \tLoss: 0.009963\t lambda: 1.947e-04\t lr: 0.293\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.099\n",
            "==Train Epoch:\t\t\t 1575 \tLoss: 0.009943\t lambda: 1.951e-04\t lr: 0.258\n",
            "==Train Epoch:\t\t\t 1578 \tLoss: 0.009945\t lambda: 1.955e-04\t lr: 0.283\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.099\n",
            "==Train Full Grad Epoch:\t 1581 \tLoss: 0.009962\t lambda: 1.958e-04\t lr: 0.254\n",
            "==Train Full Grad Epoch:\t 1584 \tLoss: 0.009963\t lambda: 1.961e-04\t lr: 0.292\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.099\n",
            "==Train Epoch:\t\t\t 1587 \tLoss: 0.009945\t lambda: 1.966e-04\t lr: 0.260\n",
            "==Train Epoch:\t\t\t 1590 \tLoss: 0.009943\t lambda: 1.970e-04\t lr: 0.296\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.099\n",
            "==Train Epoch:\t\t\t 1593 \tLoss: 0.009945\t lambda: 1.976e-04\t lr: 0.285\n",
            "==Train Epoch:\t\t\t 1596 \tLoss: 0.009943\t lambda: 1.980e-04\t lr: 0.299\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.098\n",
            "==Train Epoch:\t\t\t 1599 \tLoss: 0.009944\t lambda: 1.985e-04\t lr: 0.292\n",
            "\n",
            "Test set: Average loss: 0.0000\n",
            "\n",
            "==Train Epoch:\t\t\t 1602 \tLoss: 0.009945\t lambda: 1.988e-04\t lr: 0.280\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.098\n",
            "==Train Epoch:\t\t\t 1605 \tLoss: 0.009943\t lambda: 1.993e-04\t lr: 0.294\n",
            "==Train Epoch:\t\t\t 1608 \tLoss: 0.009944\t lambda: 1.999e-04\t lr: 0.290\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.098\n",
            "==Train Epoch:\t\t\t 1611 \tLoss: 0.009946\t lambda: 2.003e-04\t lr: 0.281\n",
            "==Train Epoch:\t\t\t 1614 \tLoss: 0.009945\t lambda: 2.007e-04\t lr: 0.273\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.098\n",
            "==Train Epoch:\t\t\t 1617 \tLoss: 0.009945\t lambda: 2.009e-04\t lr: 0.297\n",
            "==Train Epoch:\t\t\t 1620 \tLoss: 0.009944\t lambda: 2.015e-04\t lr: 0.297\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.098\n",
            "==Train Full Grad Epoch:\t 1623 \tLoss: 0.009964\t lambda: 2.017e-04\t lr: 0.264\n",
            "==Train Epoch:\t\t\t 1626 \tLoss: 0.009947\t lambda: 2.022e-04\t lr: 0.288\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.098\n",
            "==Train Epoch:\t\t\t 1629 \tLoss: 0.009947\t lambda: 2.026e-04\t lr: 0.289\n",
            "==Train Full Grad Epoch:\t 1632 \tLoss: 0.009963\t lambda: 2.028e-04\t lr: 0.298\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.098\n",
            "==Train Epoch:\t\t\t 1635 \tLoss: 0.009947\t lambda: 2.034e-04\t lr: 0.271\n",
            "==Train Full Grad Epoch:\t 1638 \tLoss: 0.009965\t lambda: 2.038e-04\t lr: 0.278\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.098\n",
            "==Train Epoch:\t\t\t 1641 \tLoss: 0.009946\t lambda: 2.044e-04\t lr: 0.258\n",
            "==Train Full Grad Epoch:\t 1644 \tLoss: 0.009964\t lambda: 2.046e-04\t lr: 0.266\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.098\n",
            "==Train Epoch:\t\t\t 1647 \tLoss: 0.009946\t lambda: 2.051e-04\t lr: 0.277\n",
            "\n",
            "Test set: Average loss: 0.0000\n",
            "\n",
            "==Train Full Grad Epoch:\t 1650 \tLoss: 0.009963\t lambda: 2.055e-04\t lr: 0.276\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.098\n",
            "==Train Epoch:\t\t\t 1653 \tLoss: 0.009945\t lambda: 2.059e-04\t lr: 0.322\n",
            "==Train Epoch:\t\t\t 1656 \tLoss: 0.009945\t lambda: 2.063e-04\t lr: 0.274\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.098\n",
            "==Train Epoch:\t\t\t 1659 \tLoss: 0.009945\t lambda: 2.069e-04\t lr: 0.292\n",
            "==Train Epoch:\t\t\t 1662 \tLoss: 0.009947\t lambda: 2.073e-04\t lr: 0.271\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.098\n",
            "==Train Full Grad Epoch:\t 1665 \tLoss: 0.009964\t lambda: 2.075e-04\t lr: 0.311\n",
            "==Train Full Grad Epoch:\t 1668 \tLoss: 0.009965\t lambda: 2.077e-04\t lr: 0.314\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.097\n",
            "==Train Full Grad Epoch:\t 1671 \tLoss: 0.009964\t lambda: 2.081e-04\t lr: 0.268\n",
            "==Train Epoch:\t\t\t 1674 \tLoss: 0.009947\t lambda: 2.085e-04\t lr: 0.291\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.097\n",
            "==Train Epoch:\t\t\t 1677 \tLoss: 0.009947\t lambda: 2.090e-04\t lr: 0.282\n",
            "==Train Full Grad Epoch:\t 1680 \tLoss: 0.009966\t lambda: 2.093e-04\t lr: 0.291\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.097\n",
            "==Train Epoch:\t\t\t 1683 \tLoss: 0.009947\t lambda: 2.098e-04\t lr: 0.269\n",
            "==Train Full Grad Epoch:\t 1686 \tLoss: 0.009964\t lambda: 2.102e-04\t lr: 0.347\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.097\n",
            "==Train Epoch:\t\t\t 1689 \tLoss: 0.009946\t lambda: 2.108e-04\t lr: 0.300\n",
            "==Train Epoch:\t\t\t 1692 \tLoss: 0.009947\t lambda: 2.114e-04\t lr: 0.287\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.097\n",
            "==Train Epoch:\t\t\t 1695 \tLoss: 0.009947\t lambda: 2.117e-04\t lr: 0.305\n",
            "==Train Epoch:\t\t\t 1698 \tLoss: 0.009948\t lambda: 2.121e-04\t lr: 0.263\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.097\n",
            "\n",
            "Test set: Average loss: 0.0000\n",
            "\n",
            "==Train Epoch:\t\t\t 1701 \tLoss: 0.009947\t lambda: 2.127e-04\t lr: 0.300\n",
            "==Train Full Grad Epoch:\t 1704 \tLoss: 0.009964\t lambda: 2.131e-04\t lr: 0.278\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.097\n",
            "==Train Epoch:\t\t\t 1707 \tLoss: 0.009949\t lambda: 2.133e-04\t lr: 0.301\n",
            "==Train Epoch:\t\t\t 1710 \tLoss: 0.009947\t lambda: 2.135e-04\t lr: 0.288\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.097\n",
            "==Train Epoch:\t\t\t 1713 \tLoss: 0.009946\t lambda: 2.139e-04\t lr: 0.300\n",
            "==Train Epoch:\t\t\t 1716 \tLoss: 0.009947\t lambda: 2.143e-04\t lr: 0.301\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.097\n",
            "==Train Full Grad Epoch:\t 1719 \tLoss: 0.009965\t lambda: 2.145e-04\t lr: 0.292\n",
            "==Train Epoch:\t\t\t 1722 \tLoss: 0.009946\t lambda: 2.151e-04\t lr: 0.313\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.097\n",
            "==Train Full Grad Epoch:\t 1725 \tLoss: 0.009965\t lambda: 2.155e-04\t lr: 0.262\n",
            "==Train Epoch:\t\t\t 1728 \tLoss: 0.009948\t lambda: 2.159e-04\t lr: 0.266\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.097\n",
            "==Train Epoch:\t\t\t 1731 \tLoss: 0.009949\t lambda: 2.164e-04\t lr: 0.267\n",
            "==Train Epoch:\t\t\t 1734 \tLoss: 0.009948\t lambda: 2.168e-04\t lr: 0.269\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.096\n",
            "==Train Epoch:\t\t\t 1737 \tLoss: 0.009947\t lambda: 2.174e-04\t lr: 0.290\n",
            "==Train Epoch:\t\t\t 1740 \tLoss: 0.009947\t lambda: 2.178e-04\t lr: 0.280\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.096\n",
            "==Train Epoch:\t\t\t 1743 \tLoss: 0.009948\t lambda: 2.184e-04\t lr: 0.297\n",
            "==Train Full Grad Epoch:\t 1746 \tLoss: 0.009965\t lambda: 2.186e-04\t lr: 0.290\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.096\n",
            "==Train Epoch:\t\t\t 1749 \tLoss: 0.009948\t lambda: 2.190e-04\t lr: 0.302\n",
            "\n",
            "Test set: Average loss: 0.0000\n",
            "\n",
            "==Train Epoch:\t\t\t 1752 \tLoss: 0.009950\t lambda: 2.193e-04\t lr: 0.308\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.096\n",
            "==Train Epoch:\t\t\t 1755 \tLoss: 0.009949\t lambda: 2.196e-04\t lr: 0.277\n",
            "==Train Full Grad Epoch:\t 1758 \tLoss: 0.009966\t lambda: 2.198e-04\t lr: 0.284\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.096\n",
            "==Train Epoch:\t\t\t 1761 \tLoss: 0.009947\t lambda: 2.203e-04\t lr: 0.320\n",
            "==Train Epoch:\t\t\t 1764 \tLoss: 0.009948\t lambda: 2.207e-04\t lr: 0.285\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.096\n",
            "==Train Full Grad Epoch:\t 1767 \tLoss: 0.009965\t lambda: 2.211e-04\t lr: 0.286\n",
            "==Train Full Grad Epoch:\t 1770 \tLoss: 0.009966\t lambda: 2.215e-04\t lr: 0.290\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.096\n",
            "==Train Epoch:\t\t\t 1773 \tLoss: 0.009948\t lambda: 2.221e-04\t lr: 0.283\n",
            "==Train Epoch:\t\t\t 1776 \tLoss: 0.009948\t lambda: 2.226e-04\t lr: 0.294\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.096\n",
            "==Train Epoch:\t\t\t 1779 \tLoss: 0.009947\t lambda: 2.230e-04\t lr: 0.289\n",
            "==Train Full Grad Epoch:\t 1782 \tLoss: 0.009966\t lambda: 2.232e-04\t lr: 0.316\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.096\n",
            "==Train Epoch:\t\t\t 1785 \tLoss: 0.009949\t lambda: 2.238e-04\t lr: 0.290\n",
            "==Train Full Grad Epoch:\t 1788 \tLoss: 0.009966\t lambda: 2.242e-04\t lr: 0.294\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.096\n",
            "==Train Full Grad Epoch:\t 1791 \tLoss: 0.009968\t lambda: 2.242e-04\t lr: 0.309\n",
            "==Train Epoch:\t\t\t 1794 \tLoss: 0.009951\t lambda: 2.246e-04\t lr: 0.287\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.096\n",
            "==Train Full Grad Epoch:\t 1797 \tLoss: 0.009966\t lambda: 2.248e-04\t lr: 0.279\n",
            "\n",
            "Test set: Average loss: 0.0000\n",
            "\n",
            "==Train Epoch:\t\t\t 1800 \tLoss: 0.009949\t lambda: 2.254e-04\t lr: 0.289\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.095\n",
            "==Train Epoch:\t\t\t 1803 \tLoss: 0.009949\t lambda: 2.258e-04\t lr: 0.287\n",
            "==Train Epoch:\t\t\t 1806 \tLoss: 0.009950\t lambda: 2.262e-04\t lr: 0.272\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.095\n",
            "==Train Epoch:\t\t\t 1809 \tLoss: 0.009949\t lambda: 2.268e-04\t lr: 0.310\n",
            "==Train Epoch:\t\t\t 1812 \tLoss: 0.009949\t lambda: 2.273e-04\t lr: 0.274\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.095\n",
            "==Train Epoch:\t\t\t 1815 \tLoss: 0.009948\t lambda: 2.279e-04\t lr: 0.302\n",
            "==Train Full Grad Epoch:\t 1818 \tLoss: 0.009967\t lambda: 2.281e-04\t lr: 0.292\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.095\n",
            "==Train Epoch:\t\t\t 1821 \tLoss: 0.009948\t lambda: 2.287e-04\t lr: 0.302\n",
            "==Train Epoch:\t\t\t 1824 \tLoss: 0.009949\t lambda: 2.291e-04\t lr: 0.314\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.095\n",
            "==Train Full Grad Epoch:\t 1827 \tLoss: 0.009966\t lambda: 2.295e-04\t lr: 0.302\n",
            "==Train Epoch:\t\t\t 1830 \tLoss: 0.009950\t lambda: 2.299e-04\t lr: 0.302\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.095\n",
            "==Train Epoch:\t\t\t 1833 \tLoss: 0.009950\t lambda: 2.304e-04\t lr: 0.270\n",
            "==Train Full Grad Epoch:\t 1836 \tLoss: 0.009965\t lambda: 2.306e-04\t lr: 0.271\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.095\n",
            "==Train Epoch:\t\t\t 1839 \tLoss: 0.009951\t lambda: 2.312e-04\t lr: 0.297\n",
            "==Train Epoch:\t\t\t 1842 \tLoss: 0.009949\t lambda: 2.314e-04\t lr: 0.303\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.095\n",
            "==Train Full Grad Epoch:\t 1845 \tLoss: 0.009966\t lambda: 2.316e-04\t lr: 0.296\n",
            "==Train Full Grad Epoch:\t 1848 \tLoss: 0.009966\t lambda: 2.320e-04\t lr: 0.286\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.095\n",
            "\n",
            "Test set: Average loss: 0.0000\n",
            "\n",
            "==Train Epoch:\t\t\t 1851 \tLoss: 0.009949\t lambda: 2.326e-04\t lr: 0.292\n",
            "==Train Full Grad Epoch:\t 1854 \tLoss: 0.009968\t lambda: 2.328e-04\t lr: 0.258\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.094\n",
            "==Train Full Grad Epoch:\t 1857 \tLoss: 0.009967\t lambda: 2.332e-04\t lr: 0.298\n",
            "==Train Full Grad Epoch:\t 1860 \tLoss: 0.009968\t lambda: 2.334e-04\t lr: 0.314\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.094\n",
            "==Train Epoch:\t\t\t 1863 \tLoss: 0.009950\t lambda: 2.338e-04\t lr: 0.282\n",
            "==Train Epoch:\t\t\t 1866 \tLoss: 0.009949\t lambda: 2.344e-04\t lr: 0.320\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.094\n",
            "==Train Epoch:\t\t\t 1869 \tLoss: 0.009951\t lambda: 2.348e-04\t lr: 0.277\n",
            "==Train Full Grad Epoch:\t 1872 \tLoss: 0.009966\t lambda: 2.351e-04\t lr: 0.289\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.094\n",
            "==Train Epoch:\t\t\t 1875 \tLoss: 0.009951\t lambda: 2.354e-04\t lr: 0.296\n",
            "==Train Epoch:\t\t\t 1878 \tLoss: 0.009951\t lambda: 2.356e-04\t lr: 0.267\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.094\n",
            "==Train Epoch:\t\t\t 1881 \tLoss: 0.009951\t lambda: 2.358e-04\t lr: 0.298\n",
            "==Train Epoch:\t\t\t 1884 \tLoss: 0.009950\t lambda: 2.362e-04\t lr: 0.321\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.094\n",
            "==Train Full Grad Epoch:\t 1887 \tLoss: 0.009967\t lambda: 2.364e-04\t lr: 0.270\n",
            "==Train Epoch:\t\t\t 1890 \tLoss: 0.009952\t lambda: 2.366e-04\t lr: 0.278\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.094\n",
            "==Train Epoch:\t\t\t 1893 \tLoss: 0.009950\t lambda: 2.368e-04\t lr: 0.251\n",
            "==Train Epoch:\t\t\t 1896 \tLoss: 0.009951\t lambda: 2.374e-04\t lr: 0.323\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.094\n",
            "==Train Full Grad Epoch:\t 1899 \tLoss: 0.009967\t lambda: 2.378e-04\t lr: 0.328\n",
            "\n",
            "Test set: Average loss: 0.0000\n",
            "\n",
            "==Train Epoch:\t\t\t 1902 \tLoss: 0.009950\t lambda: 2.381e-04\t lr: 0.265\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.094\n",
            "==Train Epoch:\t\t\t 1905 \tLoss: 0.009950\t lambda: 2.385e-04\t lr: 0.286\n",
            "==Train Epoch:\t\t\t 1908 \tLoss: 0.009951\t lambda: 2.391e-04\t lr: 0.251\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.094\n",
            "==Train Full Grad Epoch:\t 1911 \tLoss: 0.009966\t lambda: 2.395e-04\t lr: 0.292\n",
            "==Train Full Grad Epoch:\t 1914 \tLoss: 0.009967\t lambda: 2.397e-04\t lr: 0.271\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.094\n",
            "==Train Epoch:\t\t\t 1917 \tLoss: 0.009951\t lambda: 2.403e-04\t lr: 0.289\n",
            "==Train Epoch:\t\t\t 1920 \tLoss: 0.009950\t lambda: 2.408e-04\t lr: 0.295\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.093\n",
            "==Train Full Grad Epoch:\t 1923 \tLoss: 0.009967\t lambda: 2.412e-04\t lr: 0.290\n",
            "==Train Epoch:\t\t\t 1926 \tLoss: 0.009951\t lambda: 2.418e-04\t lr: 0.298\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.093\n",
            "==Train Epoch:\t\t\t 1929 \tLoss: 0.009951\t lambda: 2.422e-04\t lr: 0.272\n",
            "==Train Epoch:\t\t\t 1932 \tLoss: 0.009952\t lambda: 2.428e-04\t lr: 0.286\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.093\n",
            "==Train Epoch:\t\t\t 1935 \tLoss: 0.009953\t lambda: 2.432e-04\t lr: 0.275\n",
            "==Train Epoch:\t\t\t 1938 \tLoss: 0.009952\t lambda: 2.436e-04\t lr: 0.264\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.093\n",
            "==Train Full Grad Epoch:\t 1941 \tLoss: 0.009967\t lambda: 2.439e-04\t lr: 0.318\n",
            "==Train Epoch:\t\t\t 1944 \tLoss: 0.009951\t lambda: 2.445e-04\t lr: 0.322\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.093\n",
            "==Train Epoch:\t\t\t 1947 \tLoss: 0.009952\t lambda: 2.451e-04\t lr: 0.300\n",
            "\n",
            "Test set: Average loss: 0.0000\n",
            "\n",
            "==Train Epoch:\t\t\t 1950 \tLoss: 0.009952\t lambda: 2.455e-04\t lr: 0.304\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.093\n",
            "==Train Full Grad Epoch:\t 1953 \tLoss: 0.009968\t lambda: 2.459e-04\t lr: 0.290\n",
            "==Train Full Grad Epoch:\t 1956 \tLoss: 0.009969\t lambda: 2.461e-04\t lr: 0.295\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.093\n",
            "==Train Epoch:\t\t\t 1959 \tLoss: 0.009953\t lambda: 2.465e-04\t lr: 0.301\n",
            "==Train Epoch:\t\t\t 1962 \tLoss: 0.009952\t lambda: 2.469e-04\t lr: 0.283\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.093\n",
            "==Train Full Grad Epoch:\t 1965 \tLoss: 0.009968\t lambda: 2.473e-04\t lr: 0.295\n",
            "==Train Epoch:\t\t\t 1968 \tLoss: 0.009953\t lambda: 2.478e-04\t lr: 0.303\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.092\n",
            "==Train Epoch:\t\t\t 1971 \tLoss: 0.009952\t lambda: 2.482e-04\t lr: 0.261\n",
            "==Train Full Grad Epoch:\t 1974 \tLoss: 0.009968\t lambda: 2.484e-04\t lr: 0.303\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.092\n",
            "==Train Epoch:\t\t\t 1977 \tLoss: 0.009951\t lambda: 2.490e-04\t lr: 0.288\n",
            "==Train Epoch:\t\t\t 1980 \tLoss: 0.009954\t lambda: 2.494e-04\t lr: 0.283\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.092\n",
            "==Train Epoch:\t\t\t 1983 \tLoss: 0.009951\t lambda: 2.498e-04\t lr: 0.320\n",
            "==Train Full Grad Epoch:\t 1986 \tLoss: 0.009968\t lambda: 2.500e-04\t lr: 0.295\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.092\n",
            "==Train Epoch:\t\t\t 1989 \tLoss: 0.009953\t lambda: 2.504e-04\t lr: 0.285\n",
            "==Train Epoch:\t\t\t 1992 \tLoss: 0.009953\t lambda: 2.508e-04\t lr: 0.308\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.092\n",
            "==Train Epoch:\t\t\t 1995 \tLoss: 0.009953\t lambda: 2.512e-04\t lr: 0.277\n",
            "==Train Full Grad Epoch:\t 1998 \tLoss: 0.009968\t lambda: 2.516e-04\t lr: 0.275\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.092\n",
            "\n",
            "Test set: Average loss: 0.0000\n",
            "\n",
            "==Train Epoch:\t\t\t 2001 \tLoss: 0.009952\t lambda: 2.521e-04\t lr: 0.318\n",
            "==Train Epoch:\t\t\t 2004 \tLoss: 0.009953\t lambda: 2.523e-04\t lr: 0.298\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.092\n",
            "==Train Epoch:\t\t\t 2007 \tLoss: 0.009954\t lambda: 2.527e-04\t lr: 0.284\n",
            "==Train Epoch:\t\t\t 2010 \tLoss: 0.009952\t lambda: 2.533e-04\t lr: 0.266\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.092\n",
            "==Train Epoch:\t\t\t 2013 \tLoss: 0.009955\t lambda: 2.537e-04\t lr: 0.294\n",
            "==Train Epoch:\t\t\t 2016 \tLoss: 0.009953\t lambda: 2.543e-04\t lr: 0.256\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.092\n",
            "==Train Epoch:\t\t\t 2019 \tLoss: 0.009953\t lambda: 2.547e-04\t lr: 0.318\n",
            "==Train Epoch:\t\t\t 2022 \tLoss: 0.009954\t lambda: 2.552e-04\t lr: 0.313\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.091\n",
            "==Train Epoch:\t\t\t 2025 \tLoss: 0.009954\t lambda: 2.558e-04\t lr: 0.284\n",
            "==Train Full Grad Epoch:\t 2028 \tLoss: 0.009969\t lambda: 2.562e-04\t lr: 0.288\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.091\n",
            "==Train Epoch:\t\t\t 2031 \tLoss: 0.009954\t lambda: 2.566e-04\t lr: 0.296\n",
            "==Train Epoch:\t\t\t 2034 \tLoss: 0.009954\t lambda: 2.572e-04\t lr: 0.304\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.091\n",
            "==Train Epoch:\t\t\t 2037 \tLoss: 0.009954\t lambda: 2.576e-04\t lr: 0.294\n",
            "==Train Epoch:\t\t\t 2040 \tLoss: 0.009955\t lambda: 2.578e-04\t lr: 0.292\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.091\n",
            "==Train Full Grad Epoch:\t 2043 \tLoss: 0.009970\t lambda: 2.578e-04\t lr: 0.297\n",
            "==Train Epoch:\t\t\t 2046 \tLoss: 0.009954\t lambda: 2.584e-04\t lr: 0.289\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.091\n",
            "==Train Full Grad Epoch:\t 2049 \tLoss: 0.009970\t lambda: 2.586e-04\t lr: 0.290\n",
            "\n",
            "Test set: Average loss: 0.0000\n",
            "\n",
            "==Train Epoch:\t\t\t 2052 \tLoss: 0.009955\t lambda: 2.591e-04\t lr: 0.264\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.091\n",
            "==Train Epoch:\t\t\t 2055 \tLoss: 0.009955\t lambda: 2.595e-04\t lr: 0.298\n",
            "==Train Epoch:\t\t\t 2058 \tLoss: 0.009954\t lambda: 2.599e-04\t lr: 0.296\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.091\n",
            "==Train Epoch:\t\t\t 2061 \tLoss: 0.009955\t lambda: 2.605e-04\t lr: 0.300\n",
            "==Train Epoch:\t\t\t 2064 \tLoss: 0.009955\t lambda: 2.607e-04\t lr: 0.290\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.091\n",
            "==Train Full Grad Epoch:\t 2067 \tLoss: 0.009969\t lambda: 2.609e-04\t lr: 0.280\n",
            "==Train Full Grad Epoch:\t 2070 \tLoss: 0.009969\t lambda: 2.611e-04\t lr: 0.283\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.090\n",
            "==Train Epoch:\t\t\t 2073 \tLoss: 0.009955\t lambda: 2.615e-04\t lr: 0.292\n",
            "==Train Epoch:\t\t\t 2076 \tLoss: 0.009955\t lambda: 2.621e-04\t lr: 0.304\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.090\n",
            "==Train Full Grad Epoch:\t 2079 \tLoss: 0.009970\t lambda: 2.623e-04\t lr: 0.292\n",
            "==Train Epoch:\t\t\t 2082 \tLoss: 0.009954\t lambda: 2.629e-04\t lr: 0.297\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.090\n",
            "==Train Epoch:\t\t\t 2085 \tLoss: 0.009955\t lambda: 2.635e-04\t lr: 0.304\n",
            "==Train Epoch:\t\t\t 2088 \tLoss: 0.009955\t lambda: 2.640e-04\t lr: 0.307\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.090\n",
            "==Train Epoch:\t\t\t 2091 \tLoss: 0.009955\t lambda: 2.646e-04\t lr: 0.270\n",
            "==Train Epoch:\t\t\t 2094 \tLoss: 0.009955\t lambda: 2.652e-04\t lr: 0.269\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.090\n",
            "==Train Epoch:\t\t\t 2097 \tLoss: 0.009955\t lambda: 2.654e-04\t lr: 0.293\n",
            "\n",
            "Test set: Average loss: 0.0000\n",
            "\n",
            "==Train Full Grad Epoch:\t 2100 \tLoss: 0.009969\t lambda: 2.658e-04\t lr: 0.287\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.090\n",
            "==Train Full Grad Epoch:\t 2103 \tLoss: 0.009969\t lambda: 2.660e-04\t lr: 0.281\n",
            "==Train Full Grad Epoch:\t 2106 \tLoss: 0.009970\t lambda: 2.662e-04\t lr: 0.294\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.090\n",
            "==Train Epoch:\t\t\t 2109 \tLoss: 0.009956\t lambda: 2.664e-04\t lr: 0.290\n",
            "==Train Epoch:\t\t\t 2112 \tLoss: 0.009956\t lambda: 2.668e-04\t lr: 0.274\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.090\n",
            "==Train Epoch:\t\t\t 2115 \tLoss: 0.009957\t lambda: 2.672e-04\t lr: 0.292\n",
            "==Train Epoch:\t\t\t 2118 \tLoss: 0.009954\t lambda: 2.676e-04\t lr: 0.288\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.089\n",
            "==Train Epoch:\t\t\t 2121 \tLoss: 0.009956\t lambda: 2.682e-04\t lr: 0.307\n",
            "==Train Epoch:\t\t\t 2124 \tLoss: 0.009956\t lambda: 2.685e-04\t lr: 0.302\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.089\n",
            "==Train Epoch:\t\t\t 2127 \tLoss: 0.009956\t lambda: 2.691e-04\t lr: 0.317\n",
            "==Train Epoch:\t\t\t 2130 \tLoss: 0.009956\t lambda: 2.697e-04\t lr: 0.288\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.089\n",
            "==Train Epoch:\t\t\t 2133 \tLoss: 0.009956\t lambda: 2.699e-04\t lr: 0.307\n",
            "==Train Epoch:\t\t\t 2136 \tLoss: 0.009955\t lambda: 2.703e-04\t lr: 0.317\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.089\n",
            "==Train Epoch:\t\t\t 2139 \tLoss: 0.009955\t lambda: 2.709e-04\t lr: 0.280\n",
            "==Train Full Grad Epoch:\t 2142 \tLoss: 0.009970\t lambda: 2.711e-04\t lr: 0.293\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.089\n",
            "==Train Epoch:\t\t\t 2145 \tLoss: 0.009957\t lambda: 2.715e-04\t lr: 0.309\n",
            "==Train Full Grad Epoch:\t 2148 \tLoss: 0.009970\t lambda: 2.717e-04\t lr: 0.303\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.089\n",
            "\n",
            "Test set: Average loss: 0.0000\n",
            "\n",
            "==Train Full Grad Epoch:\t 2151 \tLoss: 0.009970\t lambda: 2.719e-04\t lr: 0.335\n",
            "==Train Full Grad Epoch:\t 2154 \tLoss: 0.009970\t lambda: 2.723e-04\t lr: 0.296\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.089\n",
            "==Train Epoch:\t\t\t 2157 \tLoss: 0.009956\t lambda: 2.727e-04\t lr: 0.324\n",
            "==Train Full Grad Epoch:\t 2160 \tLoss: 0.009969\t lambda: 2.731e-04\t lr: 0.299\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.088\n",
            "==Train Full Grad Epoch:\t 2163 \tLoss: 0.009971\t lambda: 2.733e-04\t lr: 0.273\n",
            "==Train Epoch:\t\t\t 2166 \tLoss: 0.009956\t lambda: 2.737e-04\t lr: 0.284\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.088\n",
            "==Train Epoch:\t\t\t 2169 \tLoss: 0.009957\t lambda: 2.742e-04\t lr: 0.264\n",
            "==Train Full Grad Epoch:\t 2172 \tLoss: 0.009971\t lambda: 2.744e-04\t lr: 0.287\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.088\n",
            "==Train Epoch:\t\t\t 2175 \tLoss: 0.009957\t lambda: 2.750e-04\t lr: 0.289\n",
            "==Train Epoch:\t\t\t 2178 \tLoss: 0.009957\t lambda: 2.754e-04\t lr: 0.307\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.088\n",
            "==Train Epoch:\t\t\t 2181 \tLoss: 0.009956\t lambda: 2.758e-04\t lr: 0.295\n",
            "==Train Epoch:\t\t\t 2184 \tLoss: 0.009956\t lambda: 2.764e-04\t lr: 0.317\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.088\n",
            "==Train Epoch:\t\t\t 2187 \tLoss: 0.009957\t lambda: 2.768e-04\t lr: 0.291\n",
            "==Train Epoch:\t\t\t 2190 \tLoss: 0.009957\t lambda: 2.772e-04\t lr: 0.287\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.088\n",
            "==Train Full Grad Epoch:\t 2193 \tLoss: 0.009970\t lambda: 2.774e-04\t lr: 0.295\n",
            "==Train Epoch:\t\t\t 2196 \tLoss: 0.009957\t lambda: 2.779e-04\t lr: 0.308\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.088\n",
            "==Train Full Grad Epoch:\t 2199 \tLoss: 0.009970\t lambda: 2.782e-04\t lr: 0.258\n",
            "\n",
            "Test set: Average loss: 0.0000\n",
            "\n",
            "==Train Full Grad Epoch:\t 2202 \tLoss: 0.009971\t lambda: 2.785e-04\t lr: 0.300\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.087\n",
            "==Train Epoch:\t\t\t 2205 \tLoss: 0.009958\t lambda: 2.791e-04\t lr: 0.314\n",
            "==Train Full Grad Epoch:\t 2208 \tLoss: 0.009970\t lambda: 2.793e-04\t lr: 0.286\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.087\n",
            "==Train Full Grad Epoch:\t 2211 \tLoss: 0.009970\t lambda: 2.795e-04\t lr: 0.300\n",
            "==Train Epoch:\t\t\t 2214 \tLoss: 0.009957\t lambda: 2.801e-04\t lr: 0.304\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.087\n",
            "==Train Full Grad Epoch:\t 2217 \tLoss: 0.009969\t lambda: 2.805e-04\t lr: 0.270\n",
            "==Train Full Grad Epoch:\t 2220 \tLoss: 0.009969\t lambda: 2.809e-04\t lr: 0.308\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.087\n",
            "==Train Epoch:\t\t\t 2223 \tLoss: 0.009957\t lambda: 2.813e-04\t lr: 0.274\n",
            "==Train Full Grad Epoch:\t 2226 \tLoss: 0.009971\t lambda: 2.815e-04\t lr: 0.302\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.087\n",
            "==Train Epoch:\t\t\t 2229 \tLoss: 0.009957\t lambda: 2.821e-04\t lr: 0.301\n",
            "==Train Epoch:\t\t\t 2232 \tLoss: 0.009957\t lambda: 2.825e-04\t lr: 0.293\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.087\n",
            "==Train Epoch:\t\t\t 2235 \tLoss: 0.009957\t lambda: 2.830e-04\t lr: 0.290\n",
            "==Train Epoch:\t\t\t 2238 \tLoss: 0.009959\t lambda: 2.834e-04\t lr: 0.310\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.086\n",
            "==Train Epoch:\t\t\t 2241 \tLoss: 0.009958\t lambda: 2.840e-04\t lr: 0.297\n",
            "==Train Epoch:\t\t\t 2244 \tLoss: 0.009957\t lambda: 2.844e-04\t lr: 0.273\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.086\n",
            "==Train Full Grad Epoch:\t 2247 \tLoss: 0.009971\t lambda: 2.846e-04\t lr: 0.301\n",
            "\n",
            "Test set: Average loss: 0.0000\n",
            "\n",
            "==Train Epoch:\t\t\t 2250 \tLoss: 0.009958\t lambda: 2.852e-04\t lr: 0.313\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.086\n",
            "==Train Epoch:\t\t\t 2253 \tLoss: 0.009960\t lambda: 2.858e-04\t lr: 0.290\n",
            "==Train Full Grad Epoch:\t 2256 \tLoss: 0.009970\t lambda: 2.862e-04\t lr: 0.264\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.086\n",
            "==Train Full Grad Epoch:\t 2259 \tLoss: 0.009970\t lambda: 2.865e-04\t lr: 0.317\n",
            "==Train Epoch:\t\t\t 2262 \tLoss: 0.009958\t lambda: 2.871e-04\t lr: 0.299\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.086\n",
            "==Train Epoch:\t\t\t 2265 \tLoss: 0.009958\t lambda: 2.875e-04\t lr: 0.315\n",
            "==Train Epoch:\t\t\t 2268 \tLoss: 0.009958\t lambda: 2.881e-04\t lr: 0.297\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.086\n",
            "==Train Epoch:\t\t\t 2271 \tLoss: 0.009958\t lambda: 2.885e-04\t lr: 0.308\n",
            "==Train Epoch:\t\t\t 2274 \tLoss: 0.009959\t lambda: 2.889e-04\t lr: 0.309\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.085\n",
            "==Train Full Grad Epoch:\t 2277 \tLoss: 0.009971\t lambda: 2.891e-04\t lr: 0.297\n",
            "==Train Epoch:\t\t\t 2280 \tLoss: 0.009958\t lambda: 2.895e-04\t lr: 0.313\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.085\n",
            "==Train Full Grad Epoch:\t 2283 \tLoss: 0.009973\t lambda: 2.895e-04\t lr: 0.309\n",
            "==Train Full Grad Epoch:\t 2286 \tLoss: 0.009971\t lambda: 2.897e-04\t lr: 0.308\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.085\n",
            "==Train Epoch:\t\t\t 2289 \tLoss: 0.009959\t lambda: 2.901e-04\t lr: 0.321\n",
            "==Train Epoch:\t\t\t 2292 \tLoss: 0.009959\t lambda: 2.905e-04\t lr: 0.327\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.085\n",
            "==Train Epoch:\t\t\t 2295 \tLoss: 0.009960\t lambda: 2.907e-04\t lr: 0.275\n",
            "==Train Epoch:\t\t\t 2298 \tLoss: 0.009959\t lambda: 2.911e-04\t lr: 0.303\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.085\n",
            "\n",
            "Test set: Average loss: 0.0000\n",
            "\n",
            "==Train Full Grad Epoch:\t 2301 \tLoss: 0.009972\t lambda: 2.913e-04\t lr: 0.295\n",
            "==Train Epoch:\t\t\t 2304 \tLoss: 0.009960\t lambda: 2.919e-04\t lr: 0.301\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.085\n",
            "==Train Epoch:\t\t\t 2307 \tLoss: 0.009959\t lambda: 2.925e-04\t lr: 0.323\n",
            "==Train Epoch:\t\t\t 2310 \tLoss: 0.009959\t lambda: 2.929e-04\t lr: 0.292\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.085\n",
            "==Train Epoch:\t\t\t 2313 \tLoss: 0.009960\t lambda: 2.934e-04\t lr: 0.283\n",
            "==Train Epoch:\t\t\t 2316 \tLoss: 0.009958\t lambda: 2.940e-04\t lr: 0.293\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.084\n",
            "==Train Full Grad Epoch:\t 2319 \tLoss: 0.009971\t lambda: 2.944e-04\t lr: 0.316\n",
            "==Train Epoch:\t\t\t 2322 \tLoss: 0.009960\t lambda: 2.948e-04\t lr: 0.288\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.084\n",
            "==Train Full Grad Epoch:\t 2325 \tLoss: 0.009972\t lambda: 2.952e-04\t lr: 0.259\n",
            "==Train Epoch:\t\t\t 2328 \tLoss: 0.009959\t lambda: 2.958e-04\t lr: 0.303\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.084\n",
            "==Train Epoch:\t\t\t 2331 \tLoss: 0.009960\t lambda: 2.962e-04\t lr: 0.288\n",
            "==Train Epoch:\t\t\t 2334 \tLoss: 0.009959\t lambda: 2.967e-04\t lr: 0.297\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.084\n",
            "==Train Epoch:\t\t\t 2337 \tLoss: 0.009960\t lambda: 2.971e-04\t lr: 0.282\n",
            "==Train Full Grad Epoch:\t 2340 \tLoss: 0.009972\t lambda: 2.973e-04\t lr: 0.300\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.084\n",
            "==Train Epoch:\t\t\t 2343 \tLoss: 0.009961\t lambda: 2.977e-04\t lr: 0.290\n",
            "==Train Epoch:\t\t\t 2346 \tLoss: 0.009960\t lambda: 2.983e-04\t lr: 0.318\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.083\n",
            "==Train Full Grad Epoch:\t 2349 \tLoss: 0.009973\t lambda: 2.987e-04\t lr: 0.301\n",
            "\n",
            "Test set: Average loss: 0.0000\n",
            "\n",
            "==Train Epoch:\t\t\t 2352 \tLoss: 0.009961\t lambda: 2.993e-04\t lr: 0.290\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.083\n",
            "==Train Full Grad Epoch:\t 2355 \tLoss: 0.009971\t lambda: 2.997e-04\t lr: 0.298\n",
            "==Train Epoch:\t\t\t 2358 \tLoss: 0.009961\t lambda: 3.002e-04\t lr: 0.298\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.083\n",
            "==Train Epoch:\t\t\t 2361 \tLoss: 0.009960\t lambda: 3.008e-04\t lr: 0.296\n",
            "==Train Full Grad Epoch:\t 2364 \tLoss: 0.009973\t lambda: 3.010e-04\t lr: 0.285\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.083\n",
            "==Train Epoch:\t\t\t 2367 \tLoss: 0.009960\t lambda: 3.016e-04\t lr: 0.288\n",
            "==Train Epoch:\t\t\t 2370 \tLoss: 0.009961\t lambda: 3.020e-04\t lr: 0.275\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.082\n",
            "==Train Full Grad Epoch:\t 2373 \tLoss: 0.009972\t lambda: 3.022e-04\t lr: 0.265\n",
            "==Train Epoch:\t\t\t 2376 \tLoss: 0.009961\t lambda: 3.028e-04\t lr: 0.263\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.082\n",
            "==Train Epoch:\t\t\t 2379 \tLoss: 0.009961\t lambda: 3.032e-04\t lr: 0.312\n",
            "==Train Epoch:\t\t\t 2382 \tLoss: 0.009961\t lambda: 3.037e-04\t lr: 0.301\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.082\n",
            "==Train Full Grad Epoch:\t 2385 \tLoss: 0.009973\t lambda: 3.041e-04\t lr: 0.276\n",
            "==Train Full Grad Epoch:\t 2388 \tLoss: 0.009972\t lambda: 3.045e-04\t lr: 0.277\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.082\n",
            "==Train Epoch:\t\t\t 2391 \tLoss: 0.009960\t lambda: 3.047e-04\t lr: 0.276\n",
            "==Train Epoch:\t\t\t 2394 \tLoss: 0.009961\t lambda: 3.050e-04\t lr: 0.291\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.082\n",
            "==Train Epoch:\t\t\t 2397 \tLoss: 0.009961\t lambda: 3.055e-04\t lr: 0.288\n",
            "\n",
            "Test set: Average loss: 0.0000\n",
            "\n",
            "==Train Epoch:\t\t\t 2400 \tLoss: 0.009961\t lambda: 3.061e-04\t lr: 0.278\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.081\n",
            "==Train Epoch:\t\t\t 2403 \tLoss: 0.009961\t lambda: 3.065e-04\t lr: 0.304\n",
            "==Train Epoch:\t\t\t 2406 \tLoss: 0.009962\t lambda: 3.069e-04\t lr: 0.289\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.081\n",
            "==Train Full Grad Epoch:\t 2409 \tLoss: 0.009973\t lambda: 3.073e-04\t lr: 0.294\n",
            "==Train Epoch:\t\t\t 2412 \tLoss: 0.009961\t lambda: 3.079e-04\t lr: 0.303\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.081\n",
            "==Train Epoch:\t\t\t 2415 \tLoss: 0.009962\t lambda: 3.083e-04\t lr: 0.295\n",
            "==Train Epoch:\t\t\t 2418 \tLoss: 0.009962\t lambda: 3.088e-04\t lr: 0.312\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.081\n",
            "==Train Epoch:\t\t\t 2421 \tLoss: 0.009961\t lambda: 3.094e-04\t lr: 0.295\n",
            "==Train Epoch:\t\t\t 2424 \tLoss: 0.009960\t lambda: 3.100e-04\t lr: 0.308\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.081\n",
            "==Train Epoch:\t\t\t 2427 \tLoss: 0.009962\t lambda: 3.104e-04\t lr: 0.302\n",
            "==Train Epoch:\t\t\t 2430 \tLoss: 0.009962\t lambda: 3.108e-04\t lr: 0.299\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.080\n",
            "==Train Epoch:\t\t\t 2433 \tLoss: 0.009961\t lambda: 3.113e-04\t lr: 0.298\n",
            "==Train Full Grad Epoch:\t 2436 \tLoss: 0.009973\t lambda: 3.117e-04\t lr: 0.300\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.080\n",
            "==Train Epoch:\t\t\t 2439 \tLoss: 0.009962\t lambda: 3.123e-04\t lr: 0.304\n",
            "==Train Epoch:\t\t\t 2442 \tLoss: 0.009961\t lambda: 3.125e-04\t lr: 0.267\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.080\n",
            "==Train Epoch:\t\t\t 2445 \tLoss: 0.009963\t lambda: 3.129e-04\t lr: 0.287\n",
            "==Train Epoch:\t\t\t 2448 \tLoss: 0.009963\t lambda: 3.133e-04\t lr: 0.311\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.080\n",
            "\n",
            "Test set: Average loss: 0.0000\n",
            "\n",
            "==Train Epoch:\t\t\t 2451 \tLoss: 0.009962\t lambda: 3.137e-04\t lr: 0.282\n",
            "==Train Full Grad Epoch:\t 2454 \tLoss: 0.009974\t lambda: 3.141e-04\t lr: 0.306\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.079\n",
            "==Train Full Grad Epoch:\t 2457 \tLoss: 0.009975\t lambda: 3.143e-04\t lr: 0.303\n",
            "==Train Epoch:\t\t\t 2460 \tLoss: 0.009963\t lambda: 3.147e-04\t lr: 0.314\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.079\n",
            "==Train Full Grad Epoch:\t 2463 \tLoss: 0.009974\t lambda: 3.151e-04\t lr: 0.314\n",
            "==Train Epoch:\t\t\t 2466 \tLoss: 0.009962\t lambda: 3.155e-04\t lr: 0.326\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.079\n",
            "==Train Epoch:\t\t\t 2469 \tLoss: 0.009962\t lambda: 3.161e-04\t lr: 0.313\n",
            "==Train Epoch:\t\t\t 2472 \tLoss: 0.009962\t lambda: 3.166e-04\t lr: 0.284\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.079\n",
            "==Train Epoch:\t\t\t 2475 \tLoss: 0.009963\t lambda: 3.170e-04\t lr: 0.293\n",
            "==Train Epoch:\t\t\t 2478 \tLoss: 0.009964\t lambda: 3.176e-04\t lr: 0.301\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.078\n",
            "==Train Full Grad Epoch:\t 2481 \tLoss: 0.009974\t lambda: 3.180e-04\t lr: 0.294\n",
            "==Train Full Grad Epoch:\t 2484 \tLoss: 0.009974\t lambda: 3.184e-04\t lr: 0.270\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.078\n",
            "==Train Full Grad Epoch:\t 2487 \tLoss: 0.009974\t lambda: 3.188e-04\t lr: 0.288\n",
            "==Train Epoch:\t\t\t 2490 \tLoss: 0.009963\t lambda: 3.192e-04\t lr: 0.289\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.078\n",
            "==Train Epoch:\t\t\t 2493 \tLoss: 0.009964\t lambda: 3.198e-04\t lr: 0.295\n",
            "==Train Epoch:\t\t\t 2496 \tLoss: 0.009963\t lambda: 3.203e-04\t lr: 0.285\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.078\n",
            "==Train Epoch:\t\t\t 2499 \tLoss: 0.009964\t lambda: 3.209e-04\t lr: 0.277\n",
            "\n",
            "Test set: Average loss: 0.0000\n",
            "\n",
            "==Train Epoch:\t\t\t 2502 \tLoss: 0.009964\t lambda: 3.211e-04\t lr: 0.278\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.077\n",
            "==Train Epoch:\t\t\t 2505 \tLoss: 0.009963\t lambda: 3.215e-04\t lr: 0.297\n",
            "==Train Epoch:\t\t\t 2508 \tLoss: 0.009964\t lambda: 3.219e-04\t lr: 0.308\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.077\n",
            "==Train Epoch:\t\t\t 2511 \tLoss: 0.009963\t lambda: 3.225e-04\t lr: 0.301\n",
            "==Train Epoch:\t\t\t 2514 \tLoss: 0.009963\t lambda: 3.229e-04\t lr: 0.270\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.077\n",
            "==Train Epoch:\t\t\t 2517 \tLoss: 0.009963\t lambda: 3.233e-04\t lr: 0.311\n",
            "==Train Full Grad Epoch:\t 2520 \tLoss: 0.009975\t lambda: 3.237e-04\t lr: 0.279\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.077\n",
            "==Train Full Grad Epoch:\t 2523 \tLoss: 0.009974\t lambda: 3.241e-04\t lr: 0.276\n",
            "==Train Epoch:\t\t\t 2526 \tLoss: 0.009963\t lambda: 3.246e-04\t lr: 0.294\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.076\n",
            "==Train Full Grad Epoch:\t 2529 \tLoss: 0.009976\t lambda: 3.249e-04\t lr: 0.327\n",
            "==Train Epoch:\t\t\t 2532 \tLoss: 0.009964\t lambda: 3.253e-04\t lr: 0.315\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.076\n",
            "==Train Epoch:\t\t\t 2535 \tLoss: 0.009961\t lambda: 3.255e-04\t lr: 0.286\n",
            "==Train Epoch:\t\t\t 2538 \tLoss: 0.009962\t lambda: 3.260e-04\t lr: 0.280\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.076\n",
            "==Train Epoch:\t\t\t 2541 \tLoss: 0.009964\t lambda: 3.263e-04\t lr: 0.314\n",
            "==Train Full Grad Epoch:\t 2544 \tLoss: 0.009974\t lambda: 3.266e-04\t lr: 0.289\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.076\n",
            "==Train Epoch:\t\t\t 2547 \tLoss: 0.009965\t lambda: 3.269e-04\t lr: 0.340\n",
            "\n",
            "Test set: Average loss: 0.0000\n",
            "\n",
            "==Train Full Grad Epoch:\t 2550 \tLoss: 0.009976\t lambda: 3.269e-04\t lr: 0.305\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.076\n",
            "==Train Epoch:\t\t\t 2553 \tLoss: 0.009964\t lambda: 3.273e-04\t lr: 0.307\n",
            "==Train Epoch:\t\t\t 2556 \tLoss: 0.009964\t lambda: 3.277e-04\t lr: 0.305\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.076\n",
            "==Train Full Grad Epoch:\t 2559 \tLoss: 0.009975\t lambda: 3.281e-04\t lr: 0.317\n",
            "==Train Full Grad Epoch:\t 2562 \tLoss: 0.009975\t lambda: 3.285e-04\t lr: 0.276\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.075\n",
            "==Train Epoch:\t\t\t 2565 \tLoss: 0.009965\t lambda: 3.287e-04\t lr: 0.307\n",
            "==Train Epoch:\t\t\t 2568 \tLoss: 0.009963\t lambda: 3.291e-04\t lr: 0.311\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.075\n",
            "==Train Epoch:\t\t\t 2571 \tLoss: 0.009965\t lambda: 3.297e-04\t lr: 0.315\n",
            "==Train Epoch:\t\t\t 2574 \tLoss: 0.009965\t lambda: 3.302e-04\t lr: 0.284\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.075\n",
            "==Train Epoch:\t\t\t 2577 \tLoss: 0.009964\t lambda: 3.308e-04\t lr: 0.316\n",
            "==Train Full Grad Epoch:\t 2580 \tLoss: 0.009976\t lambda: 3.310e-04\t lr: 0.319\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.074\n",
            "==Train Full Grad Epoch:\t 2583 \tLoss: 0.009974\t lambda: 3.312e-04\t lr: 0.273\n",
            "==Train Epoch:\t\t\t 2586 \tLoss: 0.009965\t lambda: 3.316e-04\t lr: 0.284\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.074\n",
            "==Train Epoch:\t\t\t 2589 \tLoss: 0.009965\t lambda: 3.322e-04\t lr: 0.289\n",
            "==Train Epoch:\t\t\t 2592 \tLoss: 0.009965\t lambda: 3.326e-04\t lr: 0.324\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.074\n",
            "==Train Epoch:\t\t\t 2595 \tLoss: 0.009966\t lambda: 3.332e-04\t lr: 0.276\n",
            "==Train Epoch:\t\t\t 2598 \tLoss: 0.009965\t lambda: 3.336e-04\t lr: 0.295\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.074\n",
            "\n",
            "Test set: Average loss: 0.0000\n",
            "\n",
            "==Train Full Grad Epoch:\t 2601 \tLoss: 0.009976\t lambda: 3.336e-04\t lr: 0.263\n",
            "==Train Epoch:\t\t\t 2604 \tLoss: 0.009965\t lambda: 3.340e-04\t lr: 0.287\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.073\n",
            "==Train Full Grad Epoch:\t 2607 \tLoss: 0.009976\t lambda: 3.342e-04\t lr: 0.289\n",
            "==Train Full Grad Epoch:\t 2610 \tLoss: 0.009974\t lambda: 3.344e-04\t lr: 0.279\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.073\n",
            "==Train Epoch:\t\t\t 2613 \tLoss: 0.009966\t lambda: 3.348e-04\t lr: 0.285\n",
            "==Train Epoch:\t\t\t 2616 \tLoss: 0.009965\t lambda: 3.354e-04\t lr: 0.297\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.073\n",
            "==Train Epoch:\t\t\t 2619 \tLoss: 0.009965\t lambda: 3.358e-04\t lr: 0.292\n",
            "==Train Epoch:\t\t\t 2622 \tLoss: 0.009965\t lambda: 3.364e-04\t lr: 0.325\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.073\n",
            "==Train Epoch:\t\t\t 2625 \tLoss: 0.009966\t lambda: 3.369e-04\t lr: 0.317\n",
            "==Train Epoch:\t\t\t 2628 \tLoss: 0.009964\t lambda: 3.373e-04\t lr: 0.290\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.072\n",
            "==Train Full Grad Epoch:\t 2631 \tLoss: 0.009976\t lambda: 3.375e-04\t lr: 0.285\n",
            "==Train Epoch:\t\t\t 2634 \tLoss: 0.009966\t lambda: 3.379e-04\t lr: 0.293\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.072\n",
            "==Train Epoch:\t\t\t 2637 \tLoss: 0.009965\t lambda: 3.383e-04\t lr: 0.320\n",
            "==Train Epoch:\t\t\t 2640 \tLoss: 0.009966\t lambda: 3.389e-04\t lr: 0.300\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.072\n",
            "==Train Epoch:\t\t\t 2643 \tLoss: 0.009966\t lambda: 3.395e-04\t lr: 0.287\n",
            "==Train Epoch:\t\t\t 2646 \tLoss: 0.009966\t lambda: 3.399e-04\t lr: 0.292\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.071\n",
            "==Train Epoch:\t\t\t 2649 \tLoss: 0.009966\t lambda: 3.403e-04\t lr: 0.322\n",
            "\n",
            "Test set: Average loss: 0.0000\n",
            "\n",
            "==Train Epoch:\t\t\t 2652 \tLoss: 0.009966\t lambda: 3.407e-04\t lr: 0.306\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.071\n",
            "==Train Epoch:\t\t\t 2655 \tLoss: 0.009966\t lambda: 3.413e-04\t lr: 0.299\n",
            "==Train Epoch:\t\t\t 2658 \tLoss: 0.009966\t lambda: 3.415e-04\t lr: 0.298\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.071\n",
            "==Train Epoch:\t\t\t 2661 \tLoss: 0.009967\t lambda: 3.419e-04\t lr: 0.300\n",
            "==Train Epoch:\t\t\t 2664 \tLoss: 0.009966\t lambda: 3.424e-04\t lr: 0.273\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.071\n",
            "==Train Epoch:\t\t\t 2667 \tLoss: 0.009967\t lambda: 3.430e-04\t lr: 0.297\n",
            "==Train Epoch:\t\t\t 2670 \tLoss: 0.009966\t lambda: 3.436e-04\t lr: 0.292\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.070\n",
            "==Train Full Grad Epoch:\t 2673 \tLoss: 0.009977\t lambda: 3.436e-04\t lr: 0.298\n",
            "==Train Epoch:\t\t\t 2676 \tLoss: 0.009967\t lambda: 3.440e-04\t lr: 0.285\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.070\n",
            "==Train Epoch:\t\t\t 2679 \tLoss: 0.009967\t lambda: 3.444e-04\t lr: 0.315\n",
            "==Train Epoch:\t\t\t 2682 \tLoss: 0.009967\t lambda: 3.448e-04\t lr: 0.267\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.070\n",
            "==Train Full Grad Epoch:\t 2685 \tLoss: 0.009976\t lambda: 3.452e-04\t lr: 0.289\n",
            "==Train Epoch:\t\t\t 2688 \tLoss: 0.009967\t lambda: 3.454e-04\t lr: 0.306\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.069\n",
            "==Train Epoch:\t\t\t 2691 \tLoss: 0.009967\t lambda: 3.458e-04\t lr: 0.286\n",
            "==Train Epoch:\t\t\t 2694 \tLoss: 0.009967\t lambda: 3.464e-04\t lr: 0.271\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.069\n",
            "==Train Full Grad Epoch:\t 2697 \tLoss: 0.009977\t lambda: 3.466e-04\t lr: 0.281\n",
            "\n",
            "Test set: Average loss: 0.0000\n",
            "\n",
            "==Train Epoch:\t\t\t 2700 \tLoss: 0.009967\t lambda: 3.472e-04\t lr: 0.298\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.069\n",
            "==Train Epoch:\t\t\t 2703 \tLoss: 0.009966\t lambda: 3.478e-04\t lr: 0.286\n",
            "==Train Epoch:\t\t\t 2706 \tLoss: 0.009966\t lambda: 3.480e-04\t lr: 0.291\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.068\n",
            "==Train Epoch:\t\t\t 2709 \tLoss: 0.009968\t lambda: 3.484e-04\t lr: 0.345\n",
            "==Train Full Grad Epoch:\t 2712 \tLoss: 0.009977\t lambda: 3.486e-04\t lr: 0.320\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.068\n",
            "==Train Epoch:\t\t\t 2715 \tLoss: 0.009967\t lambda: 3.488e-04\t lr: 0.278\n",
            "==Train Epoch:\t\t\t 2718 \tLoss: 0.009967\t lambda: 3.494e-04\t lr: 0.293\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.068\n",
            "==Train Epoch:\t\t\t 2721 \tLoss: 0.009967\t lambda: 3.498e-04\t lr: 0.294\n",
            "==Train Epoch:\t\t\t 2724 \tLoss: 0.009967\t lambda: 3.504e-04\t lr: 0.279\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.067\n",
            "==Train Epoch:\t\t\t 2727 \tLoss: 0.009968\t lambda: 3.508e-04\t lr: 0.280\n",
            "==Train Epoch:\t\t\t 2730 \tLoss: 0.009968\t lambda: 3.513e-04\t lr: 0.270\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.067\n",
            "==Train Epoch:\t\t\t 2733 \tLoss: 0.009967\t lambda: 3.519e-04\t lr: 0.272\n",
            "==Train Epoch:\t\t\t 2736 \tLoss: 0.009968\t lambda: 3.523e-04\t lr: 0.301\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.067\n",
            "==Train Epoch:\t\t\t 2739 \tLoss: 0.009966\t lambda: 3.527e-04\t lr: 0.309\n",
            "==Train Full Grad Epoch:\t 2742 \tLoss: 0.009978\t lambda: 3.527e-04\t lr: 0.331\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.066\n",
            "==Train Full Grad Epoch:\t 2745 \tLoss: 0.009978\t lambda: 3.530e-04\t lr: 0.311\n",
            "==Train Full Grad Epoch:\t 2748 \tLoss: 0.009977\t lambda: 3.534e-04\t lr: 0.279\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.066\n",
            "\n",
            "Test set: Average loss: 0.0000\n",
            "\n",
            "==Train Full Grad Epoch:\t 2751 \tLoss: 0.009978\t lambda: 3.537e-04\t lr: 0.309\n",
            "==Train Full Grad Epoch:\t 2754 \tLoss: 0.009977\t lambda: 3.540e-04\t lr: 0.318\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.066\n",
            "==Train Epoch:\t\t\t 2757 \tLoss: 0.009967\t lambda: 3.545e-04\t lr: 0.298\n",
            "==Train Full Grad Epoch:\t 2760 \tLoss: 0.009978\t lambda: 3.548e-04\t lr: 0.287\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.066\n",
            "==Train Epoch:\t\t\t 2763 \tLoss: 0.009968\t lambda: 3.550e-04\t lr: 0.274\n",
            "==Train Epoch:\t\t\t 2766 \tLoss: 0.009968\t lambda: 3.555e-04\t lr: 0.300\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.065\n",
            "==Train Full Grad Epoch:\t 2769 \tLoss: 0.009978\t lambda: 3.556e-04\t lr: 0.305\n",
            "==Train Epoch:\t\t\t 2772 \tLoss: 0.009968\t lambda: 3.560e-04\t lr: 0.297\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.065\n",
            "==Train Full Grad Epoch:\t 2775 \tLoss: 0.009977\t lambda: 3.564e-04\t lr: 0.305\n",
            "==Train Full Grad Epoch:\t 2778 \tLoss: 0.009978\t lambda: 3.568e-04\t lr: 0.295\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.065\n",
            "==Train Epoch:\t\t\t 2781 \tLoss: 0.009969\t lambda: 3.573e-04\t lr: 0.285\n",
            "==Train Epoch:\t\t\t 2784 \tLoss: 0.009968\t lambda: 3.576e-04\t lr: 0.307\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.064\n",
            "==Train Epoch:\t\t\t 2787 \tLoss: 0.009968\t lambda: 3.580e-04\t lr: 0.276\n",
            "==Train Epoch:\t\t\t 2790 \tLoss: 0.009967\t lambda: 3.585e-04\t lr: 0.306\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.064\n",
            "==Train Epoch:\t\t\t 2793 \tLoss: 0.009969\t lambda: 3.589e-04\t lr: 0.277\n",
            "==Train Epoch:\t\t\t 2796 \tLoss: 0.009967\t lambda: 3.593e-04\t lr: 0.283\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.064\n",
            "==Train Epoch:\t\t\t 2799 \tLoss: 0.009969\t lambda: 3.597e-04\t lr: 0.325\n",
            "\n",
            "Test set: Average loss: 0.0000\n",
            "\n",
            "==Train Epoch:\t\t\t 2802 \tLoss: 0.009969\t lambda: 3.601e-04\t lr: 0.321\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.063\n",
            "==Train Full Grad Epoch:\t 2805 \tLoss: 0.009977\t lambda: 3.605e-04\t lr: 0.302\n",
            "==Train Epoch:\t\t\t 2808 \tLoss: 0.009968\t lambda: 3.611e-04\t lr: 0.303\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.063\n",
            "==Train Full Grad Epoch:\t 2811 \tLoss: 0.009978\t lambda: 3.613e-04\t lr: 0.305\n",
            "==Train Epoch:\t\t\t 2814 \tLoss: 0.009968\t lambda: 3.619e-04\t lr: 0.315\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.062\n",
            "==Train Epoch:\t\t\t 2817 \tLoss: 0.009969\t lambda: 3.623e-04\t lr: 0.312\n",
            "==Train Epoch:\t\t\t 2820 \tLoss: 0.009970\t lambda: 3.629e-04\t lr: 0.307\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.062\n",
            "==Train Full Grad Epoch:\t 2823 \tLoss: 0.009978\t lambda: 3.633e-04\t lr: 0.311\n",
            "==Train Epoch:\t\t\t 2826 \tLoss: 0.009969\t lambda: 3.639e-04\t lr: 0.275\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.062\n",
            "==Train Full Grad Epoch:\t 2829 \tLoss: 0.009977\t lambda: 3.641e-04\t lr: 0.318\n",
            "==Train Epoch:\t\t\t 2832 \tLoss: 0.009969\t lambda: 3.646e-04\t lr: 0.326\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.061\n",
            "==Train Epoch:\t\t\t 2835 \tLoss: 0.009968\t lambda: 3.650e-04\t lr: 0.321\n",
            "==Train Full Grad Epoch:\t 2838 \tLoss: 0.009977\t lambda: 3.654e-04\t lr: 0.299\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.061\n",
            "==Train Epoch:\t\t\t 2841 \tLoss: 0.009969\t lambda: 3.660e-04\t lr: 0.327\n",
            "==Train Full Grad Epoch:\t 2844 \tLoss: 0.009978\t lambda: 3.661e-04\t lr: 0.329\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.060\n",
            "==Train Epoch:\t\t\t 2847 \tLoss: 0.009970\t lambda: 3.665e-04\t lr: 0.275\n",
            "\n",
            "Test set: Average loss: 0.0000\n",
            "\n",
            "==Train Full Grad Epoch:\t 2850 \tLoss: 0.009979\t lambda: 3.667e-04\t lr: 0.278\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.060\n",
            "==Train Epoch:\t\t\t 2853 \tLoss: 0.009970\t lambda: 3.671e-04\t lr: 0.298\n",
            "==Train Epoch:\t\t\t 2856 \tLoss: 0.009969\t lambda: 3.675e-04\t lr: 0.291\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.060\n",
            "==Train Epoch:\t\t\t 2859 \tLoss: 0.009970\t lambda: 3.679e-04\t lr: 0.297\n",
            "==Train Epoch:\t\t\t 2862 \tLoss: 0.009968\t lambda: 3.681e-04\t lr: 0.292\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.059\n",
            "==Train Full Grad Epoch:\t 2865 \tLoss: 0.009978\t lambda: 3.683e-04\t lr: 0.288\n",
            "==Train Full Grad Epoch:\t 2868 \tLoss: 0.009978\t lambda: 3.687e-04\t lr: 0.294\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.059\n",
            "==Train Full Grad Epoch:\t 2871 \tLoss: 0.009978\t lambda: 3.691e-04\t lr: 0.290\n",
            "==Train Full Grad Epoch:\t 2874 \tLoss: 0.009977\t lambda: 3.695e-04\t lr: 0.294\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.059\n",
            "==Train Full Grad Epoch:\t 2877 \tLoss: 0.009978\t lambda: 3.699e-04\t lr: 0.239\n",
            "==Train Epoch:\t\t\t 2880 \tLoss: 0.009970\t lambda: 3.705e-04\t lr: 0.313\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.058\n",
            "==Train Epoch:\t\t\t 2883 \tLoss: 0.009969\t lambda: 3.710e-04\t lr: 0.280\n",
            "==Train Epoch:\t\t\t 2886 \tLoss: 0.009970\t lambda: 3.714e-04\t lr: 0.274\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.058\n",
            "==Train Epoch:\t\t\t 2889 \tLoss: 0.009970\t lambda: 3.720e-04\t lr: 0.278\n",
            "==Train Epoch:\t\t\t 2892 \tLoss: 0.009970\t lambda: 3.724e-04\t lr: 0.299\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.057\n",
            "==Train Epoch:\t\t\t 2895 \tLoss: 0.009970\t lambda: 3.728e-04\t lr: 0.285\n",
            "==Train Full Grad Epoch:\t 2898 \tLoss: 0.009979\t lambda: 3.730e-04\t lr: 0.306\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.057\n",
            "\n",
            "Test set: Average loss: 0.0000\n",
            "\n",
            "==Train Epoch:\t\t\t 2901 \tLoss: 0.009969\t lambda: 3.736e-04\t lr: 0.291\n",
            "==Train Full Grad Epoch:\t 2904 \tLoss: 0.009978\t lambda: 3.740e-04\t lr: 0.298\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.056\n",
            "==Train Epoch:\t\t\t 2907 \tLoss: 0.009971\t lambda: 3.744e-04\t lr: 0.276\n",
            "==Train Epoch:\t\t\t 2910 \tLoss: 0.009970\t lambda: 3.746e-04\t lr: 0.284\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.056\n",
            "==Train Epoch:\t\t\t 2913 \tLoss: 0.009969\t lambda: 3.752e-04\t lr: 0.288\n",
            "==Train Epoch:\t\t\t 2916 \tLoss: 0.009970\t lambda: 3.756e-04\t lr: 0.300\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.056\n",
            "==Train Full Grad Epoch:\t 2919 \tLoss: 0.009978\t lambda: 3.760e-04\t lr: 0.298\n",
            "==Train Epoch:\t\t\t 2922 \tLoss: 0.009970\t lambda: 3.766e-04\t lr: 0.294\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.055\n",
            "==Train Epoch:\t\t\t 2925 \tLoss: 0.009971\t lambda: 3.772e-04\t lr: 0.302\n",
            "==Train Epoch:\t\t\t 2928 \tLoss: 0.009969\t lambda: 3.778e-04\t lr: 0.303\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.054\n",
            "==Train Epoch:\t\t\t 2931 \tLoss: 0.009970\t lambda: 3.783e-04\t lr: 0.288\n",
            "==Train Epoch:\t\t\t 2934 \tLoss: 0.009970\t lambda: 3.786e-04\t lr: 0.308\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.054\n",
            "==Train Epoch:\t\t\t 2937 \tLoss: 0.009971\t lambda: 3.790e-04\t lr: 0.280\n",
            "==Train Epoch:\t\t\t 2940 \tLoss: 0.009969\t lambda: 3.795e-04\t lr: 0.279\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.053\n",
            "==Train Epoch:\t\t\t 2943 \tLoss: 0.009970\t lambda: 3.801e-04\t lr: 0.302\n",
            "==Train Epoch:\t\t\t 2946 \tLoss: 0.009969\t lambda: 3.805e-04\t lr: 0.305\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.053\n",
            "==Train Epoch:\t\t\t 2949 \tLoss: 0.009972\t lambda: 3.811e-04\t lr: 0.311\n",
            "\n",
            "Test set: Average loss: 0.0000\n",
            "\n",
            "==Train Epoch:\t\t\t 2952 \tLoss: 0.009971\t lambda: 3.817e-04\t lr: 0.345\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.052\n",
            "==Train Full Grad Epoch:\t 2955 \tLoss: 0.009979\t lambda: 3.821e-04\t lr: 0.282\n",
            "==Train Epoch:\t\t\t 2958 \tLoss: 0.009969\t lambda: 3.825e-04\t lr: 0.300\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.052\n",
            "==Train Epoch:\t\t\t 2961 \tLoss: 0.009970\t lambda: 3.829e-04\t lr: 0.334\n",
            "==Train Epoch:\t\t\t 2964 \tLoss: 0.009970\t lambda: 3.835e-04\t lr: 0.322\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.051\n",
            "==Train Full Grad Epoch:\t 2967 \tLoss: 0.009978\t lambda: 3.839e-04\t lr: 0.282\n",
            "==Train Full Grad Epoch:\t 2970 \tLoss: 0.009979\t lambda: 3.843e-04\t lr: 0.308\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.050\n",
            "==Train Full Grad Epoch:\t 2973 \tLoss: 0.009978\t lambda: 3.845e-04\t lr: 0.317\n",
            "==Train Epoch:\t\t\t 2976 \tLoss: 0.009970\t lambda: 3.851e-04\t lr: 0.310\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.050\n",
            "==Train Epoch:\t\t\t 2979 \tLoss: 0.009971\t lambda: 3.855e-04\t lr: 0.302\n",
            "==Train Epoch:\t\t\t 2982 \tLoss: 0.009970\t lambda: 3.861e-04\t lr: 0.293\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.049\n",
            "==Train Epoch:\t\t\t 2985 \tLoss: 0.009970\t lambda: 3.865e-04\t lr: 0.329\n",
            "==Train Epoch:\t\t\t 2988 \tLoss: 0.009971\t lambda: 3.869e-04\t lr: 0.308\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.049\n",
            "==Train Full Grad Epoch:\t 2991 \tLoss: 0.009979\t lambda: 3.873e-04\t lr: 0.271\n",
            "==Train Full Grad Epoch:\t 2994 \tLoss: 0.009978\t lambda: 3.877e-04\t lr: 0.270\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.048\n",
            "==Train Epoch:\t\t\t 2997 \tLoss: 0.009971\t lambda: 3.882e-04\t lr: 0.298\n",
            "\n",
            "Test set: Average loss: 0.0000\n",
            "\n",
            "==Train Epoch:\t\t\t 3000 \tLoss: 0.009971\t lambda: 3.886e-04\t lr: 0.289\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.048\n",
            "==Train Epoch:\t\t\t 3003 \tLoss: 0.009970\t lambda: 3.890e-04\t lr: 0.293\n",
            "==Train Full Grad Epoch:\t 3006 \tLoss: 0.009979\t lambda: 3.891e-04\t lr: 0.320\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.047\n",
            "==Train Epoch:\t\t\t 3009 \tLoss: 0.009970\t lambda: 3.893e-04\t lr: 0.258\n",
            "==Train Epoch:\t\t\t 3012 \tLoss: 0.009971\t lambda: 3.897e-04\t lr: 0.304\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.047\n",
            "==Train Epoch:\t\t\t 3015 \tLoss: 0.009971\t lambda: 3.899e-04\t lr: 0.329\n",
            "==Train Epoch:\t\t\t 3018 \tLoss: 0.009970\t lambda: 3.903e-04\t lr: 0.271\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.046\n",
            "==Train Epoch:\t\t\t 3021 \tLoss: 0.009971\t lambda: 3.909e-04\t lr: 0.312\n",
            "==Train Epoch:\t\t\t 3024 \tLoss: 0.009970\t lambda: 3.911e-04\t lr: 0.273\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.046\n",
            "==Train Epoch:\t\t\t 3027 \tLoss: 0.009971\t lambda: 3.917e-04\t lr: 0.288\n",
            "==Train Epoch:\t\t\t 3030 \tLoss: 0.009971\t lambda: 3.921e-04\t lr: 0.300\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.045\n",
            "==Train Epoch:\t\t\t 3033 \tLoss: 0.009970\t lambda: 3.925e-04\t lr: 0.273\n",
            "==Train Full Grad Epoch:\t 3036 \tLoss: 0.009979\t lambda: 3.927e-04\t lr: 0.309\n",
            "--\t\t\tphi(X):\t 0.000 \tphi(Y): 0.045\n",
            "==Train Epoch:\t\t\t 3039 \tLoss: 0.009970\t lambda: 3.933e-04\t lr: 0.329\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPUYNQ_3fzGA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y[0:10,:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sa8WVQpKeqQC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.Y.fact.weight[0:10,:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06HoX-V6l5zP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.Y.fact.weight.sum(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKc2Ic9Il-WN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y.sum(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Y_IRtgZkW88",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "C"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHFNjYCugO2r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.C.fact.weight"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRoY9NhakfAl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X[0:10,:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYs9hu03kZyp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.X.fact.weight[0:10,:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5HuTuumXbir",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optParam = optimizerX.param_groups[0]\n",
        "print((optParam['params'][0].grad !=0).sum())\n",
        "gradX = optParam['params'][0].grad\n",
        "batch_i = gradX.sum(1) != 0\n",
        "gradX[batch_i,:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InSGk0-Xoihy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch = next(iter(train_loader))[0]\n",
        "#batch = batch.to(dev)\n",
        "batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfuQ14tDSAiZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(np.unique(batch[:,1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94Ig7_hMWR2v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.round(torch.matmul(torch.matmul(model.Y.fact.weight,model.C.fact.weight),torch.transpose(model.X.fact.weight,0,1))*100)/100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huV92OMHsqKP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y@C@X.T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ba9mJzqb7Mm_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "D"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}